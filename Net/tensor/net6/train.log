DEBUG:root:[ Iteration 0 ] Training loss: 0.173617
DEBUG:root:[ Iteration 0 ] Test loss: 0.167575
DEBUG:root:[ Iteration 3 ] Training loss: 0.171808
DEBUG:root:[ Iteration 6 ] Training loss: 0.157945
DEBUG:root:[ Iteration 9 ] Training loss: 0.155951
DEBUG:root:[ Iteration 12 ] Training loss: 0.178033
DEBUG:root:[ Iteration 15 ] Training loss: 0.161198
DEBUG:root:[ Iteration 18 ] Training loss: 0.169006
DEBUG:root:[ Iteration 20 ] Test loss: 0.164082
DEBUG:root:[ Iteration 21 ] Training loss: 0.151584
DEBUG:root:[ Iteration 24 ] Training loss: 0.162811
DEBUG:root:[ Iteration 27 ] Training loss: 0.16577
DEBUG:root:[ Iteration 30 ] Training loss: 0.169345
DEBUG:root:[ Iteration 33 ] Training loss: 0.162159
DEBUG:root:[ Iteration 36 ] Training loss: 0.161171
DEBUG:root:[ Iteration 39 ] Training loss: 0.160031
DEBUG:root:[ Iteration 40 ] Test loss: 0.160344
DEBUG:root:[ Iteration 42 ] Training loss: 0.158838
DEBUG:root:[ Iteration 45 ] Training loss: 0.174164
DEBUG:root:[ Iteration 48 ] Training loss: 0.156974
DEBUG:root:[ Iteration 51 ] Training loss: 0.17087
DEBUG:root:[ Iteration 54 ] Training loss: 0.162857
DEBUG:root:[ Iteration 57 ] Training loss: 0.156162
DEBUG:root:[ Iteration 60 ] Training loss: 0.148666
DEBUG:root:[ Iteration 60 ] Test loss: 0.159288
DEBUG:root:[ Iteration 63 ] Training loss: 0.161749
DEBUG:root:[ Iteration 66 ] Training loss: 0.150905
DEBUG:root:[ Iteration 69 ] Training loss: 0.170554
DEBUG:root:[ Iteration 72 ] Training loss: 0.154744
DEBUG:root:[ Iteration 75 ] Training loss: 0.147863
DEBUG:root:[ Iteration 78 ] Training loss: 0.162291
DEBUG:root:[ Iteration 80 ] Test loss: 0.159073
DEBUG:root:[ Iteration 81 ] Training loss: 0.162764
DEBUG:root:[ Iteration 84 ] Training loss: 0.147911
DEBUG:root:[ Iteration 87 ] Training loss: 0.139189
DEBUG:root:[ Iteration 90 ] Training loss: 0.168601
DEBUG:root:[ Iteration 93 ] Training loss: 0.151303
DEBUG:root:[ Iteration 96 ] Training loss: 0.153591
DEBUG:root:[ Iteration 99 ] Training loss: 0.159805
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h05m53s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.186004
DEBUG:root:[ Iteration 0 ] Test loss: 0.187346
DEBUG:root:[ Iteration 3 ] Training loss: 0.177744
DEBUG:root:[ Iteration 6 ] Training loss: 0.19548
DEBUG:root:[ Iteration 9 ] Training loss: 0.186236
DEBUG:root:[ Iteration 12 ] Training loss: 0.186154
DEBUG:root:[ Iteration 15 ] Training loss: 0.18407
DEBUG:root:[ Iteration 18 ] Training loss: 0.187182
DEBUG:root:[ Iteration 20 ] Test loss: 0.181616
DEBUG:root:[ Iteration 21 ] Training loss: 0.183707
DEBUG:root:[ Iteration 24 ] Training loss: 0.180565
DEBUG:root:[ Iteration 27 ] Training loss: 0.184745
DEBUG:root:[ Iteration 30 ] Training loss: 0.1812
DEBUG:root:[ Iteration 33 ] Training loss: 0.186996
DEBUG:root:[ Iteration 36 ] Training loss: 0.174902
DEBUG:root:[ Iteration 39 ] Training loss: 0.177699
DEBUG:root:[ Iteration 40 ] Test loss: 0.177504
DEBUG:root:[ Iteration 42 ] Training loss: 0.180098
DEBUG:root:[ Iteration 45 ] Training loss: 0.170354
DEBUG:root:[ Iteration 48 ] Training loss: 0.181157
DEBUG:root:[ Iteration 51 ] Training loss: 0.179292
DEBUG:root:[ Iteration 54 ] Training loss: 0.177847
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h07m35s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.166048
DEBUG:root:[ Iteration 0 ] Test loss: 0.167573
DEBUG:root:[ Iteration 3 ] Training loss: 0.165956
DEBUG:root:[ Iteration 6 ] Training loss: 0.165796
DEBUG:root:[ Iteration 9 ] Training loss: 0.165613
DEBUG:root:[ Iteration 12 ] Training loss: 0.165421
DEBUG:root:[ Iteration 15 ] Training loss: 0.165229
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h08m31s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.174003
DEBUG:root:[ Iteration 0 ] Test loss: 0.174534
DEBUG:root:[ Iteration 3 ] Training loss: 0.170861
DEBUG:root:[ Iteration 6 ] Training loss: 0.166844
DEBUG:root:[ Iteration 9 ] Training loss: 0.163321
DEBUG:root:[ Iteration 12 ] Training loss: 0.160155
DEBUG:root:[ Iteration 15 ] Training loss: 0.157431
DEBUG:root:[ Iteration 18 ] Training loss: 0.155183
DEBUG:root:[ Iteration 20 ] Test loss: 0.161487
DEBUG:root:[ Iteration 21 ] Training loss: 0.153413
DEBUG:root:[ Iteration 24 ] Training loss: 0.15212
DEBUG:root:[ Iteration 27 ] Training loss: 0.151241
DEBUG:root:[ Iteration 30 ] Training loss: 0.150681
DEBUG:root:[ Iteration 33 ] Training loss: 0.150343
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h09m23s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.173845
DEBUG:root:[ Iteration 0 ] Test loss: 0.177812
DEBUG:root:[ Iteration 3 ] Training loss: 0.172451
DEBUG:root:[ Iteration 6 ] Training loss: 0.169804
DEBUG:root:[ Iteration 9 ] Training loss: 0.16671
DEBUG:root:[ Iteration 12 ] Training loss: 0.162978
DEBUG:root:[ Iteration 15 ] Training loss: 0.158517
DEBUG:root:[ Iteration 18 ] Training loss: 0.154642
DEBUG:root:[ Iteration 20 ] Test loss: 0.162808
DEBUG:root:[ Iteration 21 ] Training loss: 0.1523
DEBUG:root:[ Iteration 24 ] Training loss: 0.151724
DEBUG:root:[ Iteration 27 ] Training loss: 0.151624
DEBUG:root:[ Iteration 30 ] Training loss: 0.151282
DEBUG:root:[ Iteration 33 ] Training loss: 0.150821
DEBUG:root:[ Iteration 36 ] Training loss: 0.150503
DEBUG:root:[ Iteration 39 ] Training loss: 0.150303
DEBUG:root:[ Iteration 40 ] Test loss: 0.159248
DEBUG:root:[ Iteration 42 ] Training loss: 0.15012
DEBUG:root:[ Iteration 45 ] Training loss: 0.14995
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h11m28s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.20273
DEBUG:root:[ Iteration 0 ] Test loss: 0.203702
DEBUG:root:[ Iteration 3 ] Training loss: 0.173312
DEBUG:root:[ Iteration 6 ] Training loss: 0.16989
DEBUG:root:[ Iteration 9 ] Training loss: 0.165847
DEBUG:root:[ Iteration 12 ] Training loss: 0.162718
DEBUG:root:[ Iteration 15 ] Training loss: 0.162961
DEBUG:root:[ Iteration 18 ] Training loss: 0.163639
DEBUG:root:[ Iteration 20 ] Test loss: 0.161406
DEBUG:root:[ Iteration 21 ] Training loss: 0.163617
DEBUG:root:[ Iteration 24 ] Training loss: 0.163235
DEBUG:root:[ Iteration 27 ] Training loss: 0.162896
DEBUG:root:[ Iteration 30 ] Training loss: 0.162651
DEBUG:root:[ Iteration 33 ] Training loss: 0.162534
DEBUG:root:[ Iteration 36 ] Training loss: 0.16252
DEBUG:root:[ Iteration 39 ] Training loss: 0.162555
DEBUG:root:[ Iteration 40 ] Test loss: 0.16043
DEBUG:root:[ Iteration 42 ] Training loss: 0.162579
DEBUG:root:[ Iteration 45 ] Training loss: 0.162577
DEBUG:root:[ Iteration 48 ] Training loss: 0.162563
DEBUG:root:[ Iteration 51 ] Training loss: 0.162546
DEBUG:root:[ Iteration 54 ] Training loss: 0.16253
DEBUG:root:[ Iteration 57 ] Training loss: 0.162519
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h14m06s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.326788
DEBUG:root:[ Iteration 0 ] Test loss: 0.353377
DEBUG:root:[ Iteration 3 ] Training loss: 0.141189
DEBUG:root:[ Iteration 6 ] Training loss: 0.162673
DEBUG:root:[ Iteration 9 ] Training loss: 0.161024
DEBUG:root:[ Iteration 12 ] Training loss: 0.152084
DEBUG:root:[ Iteration 15 ] Training loss: 0.149768
DEBUG:root:[ Iteration 18 ] Training loss: 0.150066
DEBUG:root:[ Iteration 20 ] Test loss: 0.159789
DEBUG:root:[ Iteration 21 ] Training loss: 0.149943
DEBUG:root:[ Iteration 24 ] Training loss: 0.149249
DEBUG:root:[ Iteration 27 ] Training loss: 0.148334
DEBUG:root:[ Iteration 30 ] Training loss: 0.147519
DEBUG:root:[ Iteration 33 ] Training loss: 0.146924
DEBUG:root:[ Iteration 36 ] Training loss: 0.146423
DEBUG:root:[ Iteration 39 ] Training loss: 0.14591
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h20m10s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.172972
DEBUG:root:[ Iteration 0 ] Test loss: 0.166191
DEBUG:root:[ Iteration 3 ] Training loss: 0.157606
DEBUG:root:[ Iteration 6 ] Training loss: 0.152339
DEBUG:root:[ Iteration 9 ] Training loss: 0.142891
DEBUG:root:[ Iteration 12 ] Training loss: 0.128625
DEBUG:root:[ Iteration 15 ] Training loss: 0.105197
DEBUG:root:[ Iteration 18 ] Training loss: 0.0751605
DEBUG:root:[ Iteration 20 ] Test loss: 0.166481
DEBUG:root:[ Iteration 21 ] Training loss: 0.0455712
DEBUG:root:[ Iteration 24 ] Training loss: 0.0396058
DEBUG:root:[ Iteration 27 ] Training loss: 0.121889
DEBUG:root:[ Iteration 30 ] Training loss: 0.186032
DEBUG:root:[ Iteration 33 ] Training loss: 0.13746
DEBUG:root:[ Iteration 36 ] Training loss: 0.136323
DEBUG:root:[ Iteration 39 ] Training loss: 0.130602
DEBUG:root:[ Iteration 40 ] Test loss: 0.296368
DEBUG:root:[ Iteration 42 ] Training loss: 0.131622
DEBUG:root:[ Iteration 45 ] Training loss: 0.129476
DEBUG:root:[ Iteration 48 ] Training loss: 0.127099
DEBUG:root:[ Iteration 51 ] Training loss: 0.126882
DEBUG:root:[ Iteration 54 ] Training loss: 0.126722
DEBUG:root:[ Iteration 57 ] Training loss: 0.126376
DEBUG:root:[ Iteration 60 ] Training loss: 0.126209
DEBUG:root:[ Iteration 60 ] Test loss: 0.302619
DEBUG:root:[ Iteration 63 ] Training loss: 0.126118
DEBUG:root:[ Iteration 66 ] Training loss: 0.125928
DEBUG:root:[ Iteration 69 ] Training loss: 0.125747
DEBUG:root:[ Iteration 72 ] Training loss: 0.12568
DEBUG:root:[ Iteration 75 ] Training loss: 0.125582
DEBUG:root:[ Iteration 78 ] Training loss: 0.125477
DEBUG:root:[ Iteration 80 ] Test loss: 0.306011
DEBUG:root:[ Iteration 81 ] Training loss: 0.125452
DEBUG:root:[ Iteration 84 ] Training loss: 0.125405
DEBUG:root:[ Iteration 87 ] Training loss: 0.125366
DEBUG:root:[ Iteration 90 ] Training loss: 0.125342
DEBUG:root:[ Iteration 93 ] Training loss: 0.12531
DEBUG:root:[ Iteration 96 ] Training loss: 0.125295
DEBUG:root:[ Iteration 99 ] Training loss: 0.125281
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h20m35s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.180192
DEBUG:root:[ Iteration 0 ] Test loss: 0.172456
DEBUG:root:[ Iteration 3 ] Training loss: 0.163634
DEBUG:root:[ Iteration 6 ] Training loss: 0.157531
DEBUG:root:[ Iteration 9 ] Training loss: 0.155372
DEBUG:root:[ Iteration 12 ] Training loss: 0.153731
DEBUG:root:[ Iteration 15 ] Training loss: 0.15043
DEBUG:root:[ Iteration 18 ] Training loss: 0.146143
DEBUG:root:[ Iteration 20 ] Test loss: 0.17418
DEBUG:root:[ Iteration 21 ] Training loss: 0.143622
DEBUG:root:[ Iteration 24 ] Training loss: 0.142222
DEBUG:root:[ Iteration 27 ] Training loss: 0.138763
DEBUG:root:[ Iteration 30 ] Training loss: 0.136822
DEBUG:root:[ Iteration 33 ] Training loss: 0.13388
DEBUG:root:[ Iteration 36 ] Training loss: 0.1317
DEBUG:root:[ Iteration 39 ] Training loss: 0.129046
DEBUG:root:[ Iteration 40 ] Test loss: 0.17358
DEBUG:root:[ Iteration 42 ] Training loss: 0.126086
DEBUG:root:[ Iteration 45 ] Training loss: 0.12335
DEBUG:root:[ Iteration 48 ] Training loss: 0.120199
DEBUG:root:[ Iteration 51 ] Training loss: 0.11726
DEBUG:root:[ Iteration 54 ] Training loss: 0.113729
DEBUG:root:[ Iteration 57 ] Training loss: 0.11024
DEBUG:root:[ Iteration 60 ] Training loss: 0.106712
DEBUG:root:[ Iteration 60 ] Test loss: 0.170378
DEBUG:root:[ Iteration 63 ] Training loss: 0.102936
DEBUG:root:[ Iteration 66 ] Training loss: 0.0989267
DEBUG:root:[ Iteration 69 ] Training loss: 0.0949276
DEBUG:root:[ Iteration 72 ] Training loss: 0.0908091
DEBUG:root:[ Iteration 75 ] Training loss: 0.0866444
DEBUG:root:[ Iteration 78 ] Training loss: 0.0824178
DEBUG:root:[ Iteration 80 ] Test loss: 0.16885
DEBUG:root:[ Iteration 81 ] Training loss: 0.0781474
DEBUG:root:[ Iteration 84 ] Training loss: 0.0739075
DEBUG:root:[ Iteration 87 ] Training loss: 0.0696104
DEBUG:root:[ Iteration 90 ] Training loss: 0.0653892
DEBUG:root:[ Iteration 93 ] Training loss: 0.0612808
DEBUG:root:[ Iteration 96 ] Training loss: 0.057292
DEBUG:root:[ Iteration 99 ] Training loss: 0.0534528
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h21m35s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.188933
DEBUG:root:[ Iteration 0 ] Test loss: 0.183848
DEBUG:root:[ Iteration 3 ] Training loss: 0.189728
DEBUG:root:[ Iteration 6 ] Training loss: 0.18785
DEBUG:root:[ Iteration 9 ] Training loss: 0.186584
DEBUG:root:[ Iteration 12 ] Training loss: 0.19203
DEBUG:root:[ Iteration 15 ] Training loss: 0.181682
DEBUG:root:[ Iteration 18 ] Training loss: 0.183813
DEBUG:root:[ Iteration 20 ] Test loss: 0.177001
DEBUG:root:[ Iteration 21 ] Training loss: 0.184607
DEBUG:root:[ Iteration 24 ] Training loss: 0.174685
DEBUG:root:[ Iteration 27 ] Training loss: 0.174251
DEBUG:root:[ Iteration 30 ] Training loss: 0.170316
DEBUG:root:[ Iteration 33 ] Training loss: 0.171406
DEBUG:root:[ Iteration 36 ] Training loss: 0.172443
DEBUG:root:[ Iteration 39 ] Training loss: 0.168584
DEBUG:root:[ Iteration 40 ] Test loss: 0.169737
DEBUG:root:[ Iteration 42 ] Training loss: 0.166815
DEBUG:root:[ Iteration 45 ] Training loss: 0.162034
DEBUG:root:[ Iteration 48 ] Training loss: 0.165578
DEBUG:root:[ Iteration 51 ] Training loss: 0.163964
DEBUG:root:[ Iteration 54 ] Training loss: 0.163663
DEBUG:root:[ Iteration 57 ] Training loss: 0.161312
DEBUG:root:[ Iteration 60 ] Training loss: 0.158999
DEBUG:root:[ Iteration 60 ] Test loss: 0.164987
DEBUG:root:[ Iteration 63 ] Training loss: 0.164309
DEBUG:root:[ Iteration 66 ] Training loss: 0.163891
DEBUG:root:[ Iteration 69 ] Training loss: 0.157289
DEBUG:root:[ Iteration 72 ] Training loss: 0.164287
DEBUG:root:[ Iteration 75 ] Training loss: 0.16344
DEBUG:root:[ Iteration 78 ] Training loss: 0.160429
DEBUG:root:[ Iteration 80 ] Test loss: 0.162197
DEBUG:root:[ Iteration 81 ] Training loss: 0.163549
DEBUG:root:[ Iteration 84 ] Training loss: 0.155249
DEBUG:root:[ Iteration 87 ] Training loss: 0.151522
DEBUG:root:[ Iteration 90 ] Training loss: 0.164602
DEBUG:root:[ Iteration 93 ] Training loss: 0.159389
DEBUG:root:[ Iteration 96 ] Training loss: 0.168021
DEBUG:root:[ Iteration 99 ] Training loss: 0.151414
DEBUG:root:[ Iteration 100 ] Test loss: 0.160492
DEBUG:root:[ Iteration 102 ] Training loss: 0.158185
DEBUG:root:[ Iteration 105 ] Training loss: 0.155721
DEBUG:root:[ Iteration 108 ] Training loss: 0.154237
DEBUG:root:[ Iteration 111 ] Training loss: 0.164057
DEBUG:root:[ Iteration 114 ] Training loss: 0.160498
DEBUG:root:[ Iteration 117 ] Training loss: 0.157345
DEBUG:root:[ Iteration 120 ] Training loss: 0.1556
DEBUG:root:[ Iteration 120 ] Test loss: 0.159636
DEBUG:root:[ Iteration 123 ] Training loss: 0.155745
DEBUG:root:[ Iteration 126 ] Training loss: 0.144587
DEBUG:root:[ Iteration 129 ] Training loss: 0.156979
DEBUG:root:[ Iteration 132 ] Training loss: 0.160633
DEBUG:root:[ Iteration 135 ] Training loss: 0.163593
DEBUG:root:[ Iteration 138 ] Training loss: 0.161197
DEBUG:root:[ Iteration 140 ] Test loss: 0.159371
DEBUG:root:[ Iteration 141 ] Training loss: 0.142709
DEBUG:root:[ Iteration 144 ] Training loss: 0.153677
DEBUG:root:[ Iteration 147 ] Training loss: 0.157097
DEBUG:root:[ Iteration 150 ] Training loss: 0.139041
DEBUG:root:[ Iteration 153 ] Training loss: 0.148091
DEBUG:root:[ Iteration 156 ] Training loss: 0.151402
DEBUG:root:[ Iteration 159 ] Training loss: 0.144082
DEBUG:root:[ Iteration 160 ] Test loss: 0.159337
DEBUG:root:[ Iteration 162 ] Training loss: 0.149422
DEBUG:root:[ Iteration 165 ] Training loss: 0.150381
DEBUG:root:[ Iteration 168 ] Training loss: 0.145864
DEBUG:root:[ Iteration 171 ] Training loss: 0.147851
DEBUG:root:[ Iteration 174 ] Training loss: 0.152565
DEBUG:root:[ Iteration 177 ] Training loss: 0.151803
DEBUG:root:[ Iteration 180 ] Training loss: 0.150258
DEBUG:root:[ Iteration 180 ] Test loss: 0.159495
DEBUG:root:[ Iteration 183 ] Training loss: 0.156274
DEBUG:root:[ Iteration 186 ] Training loss: 0.13814
DEBUG:root:[ Iteration 189 ] Training loss: 0.137859
DEBUG:root:[ Iteration 192 ] Training loss: 0.151711
DEBUG:root:[ Iteration 195 ] Training loss: 0.149553
DEBUG:root:[ Iteration 198 ] Training loss: 0.150414
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h24m02s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.182971
DEBUG:root:[ Iteration 0 ] Test loss: 0.174861
DEBUG:root:[ Iteration 3 ] Training loss: 0.175062
DEBUG:root:[ Iteration 6 ] Training loss: 0.179685
DEBUG:root:[ Iteration 9 ] Training loss: 0.180599
DEBUG:root:[ Iteration 12 ] Training loss: 0.17348
DEBUG:root:[ Iteration 15 ] Training loss: 0.177123
DEBUG:root:[ Iteration 18 ] Training loss: 0.179218
DEBUG:root:[ Iteration 20 ] Test loss: 0.174759
DEBUG:root:[ Iteration 21 ] Training loss: 0.17181
DEBUG:root:[ Iteration 24 ] Training loss: 0.179764
DEBUG:root:[ Iteration 27 ] Training loss: 0.18061
DEBUG:root:[ Iteration 30 ] Training loss: 0.173365
DEBUG:root:[ Iteration 33 ] Training loss: 0.177708
DEBUG:root:[ Iteration 36 ] Training loss: 0.177085
DEBUG:root:[ Iteration 39 ] Training loss: 0.181577
DEBUG:root:[ Iteration 40 ] Test loss: 0.174654
DEBUG:root:[ Iteration 42 ] Training loss: 0.180019
DEBUG:root:[ Iteration 45 ] Training loss: 0.180061
DEBUG:root:[ Iteration 48 ] Training loss: 0.177679
DEBUG:root:[ Iteration 51 ] Training loss: 0.175541
DEBUG:root:[ Iteration 54 ] Training loss: 0.178727
DEBUG:root:[ Iteration 57 ] Training loss: 0.174317
DEBUG:root:[ Iteration 60 ] Training loss: 0.177031
DEBUG:root:[ Iteration 60 ] Test loss: 0.174548
DEBUG:root:[ Iteration 63 ] Training loss: 0.180226
DEBUG:root:[ Iteration 66 ] Training loss: 0.177941
DEBUG:root:[ Iteration 69 ] Training loss: 0.177629
DEBUG:root:[ Iteration 72 ] Training loss: 0.175723
DEBUG:root:[ Iteration 75 ] Training loss: 0.175163
DEBUG:root:[ Iteration 78 ] Training loss: 0.177069
DEBUG:root:[ Iteration 80 ] Test loss: 0.174443
DEBUG:root:[ Iteration 81 ] Training loss: 0.174822
DEBUG:root:[ Iteration 84 ] Training loss: 0.17646
DEBUG:root:[ Iteration 87 ] Training loss: 0.173905
DEBUG:root:[ Iteration 90 ] Training loss: 0.175773
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h28m03s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.171635
DEBUG:root:[ Iteration 0 ] Test loss: 0.174727
DEBUG:root:[ Iteration 3 ] Training loss: 0.171613
DEBUG:root:[ Iteration 6 ] Training loss: 0.171584
DEBUG:root:[ Iteration 9 ] Training loss: 0.171554
DEBUG:root:[ Iteration 12 ] Training loss: 0.171524
DEBUG:root:[ Iteration 15 ] Training loss: 0.171495
DEBUG:root:[ Iteration 18 ] Training loss: 0.171465
DEBUG:root:[ Iteration 20 ] Test loss: 0.174548
DEBUG:root:[ Iteration 21 ] Training loss: 0.171435
DEBUG:root:[ Iteration 24 ] Training loss: 0.171405
DEBUG:root:[ Iteration 27 ] Training loss: 0.171375
DEBUG:root:[ Iteration 30 ] Training loss: 0.171346
DEBUG:root:[ Iteration 33 ] Training loss: 0.171316
DEBUG:root:[ Iteration 36 ] Training loss: 0.171287
DEBUG:root:[ Iteration 39 ] Training loss: 0.171257
DEBUG:root:[ Iteration 40 ] Test loss: 0.174361
DEBUG:root:[ Iteration 42 ] Training loss: 0.171228
DEBUG:root:[ Iteration 45 ] Training loss: 0.171198
DEBUG:root:[ Iteration 48 ] Training loss: 0.171169
DEBUG:root:[ Iteration 51 ] Training loss: 0.17114
DEBUG:root:[ Iteration 54 ] Training loss: 0.171111
DEBUG:root:[ Iteration 57 ] Training loss: 0.171081
DEBUG:root:[ Iteration 60 ] Training loss: 0.171052
DEBUG:root:[ Iteration 60 ] Test loss: 0.174178
DEBUG:root:[ Iteration 63 ] Training loss: 0.171023
DEBUG:root:[ Iteration 66 ] Training loss: 0.170994
DEBUG:root:[ Iteration 69 ] Training loss: 0.170966
DEBUG:root:[ Iteration 72 ] Training loss: 0.170937
DEBUG:root:[ Iteration 75 ] Training loss: 0.170908
DEBUG:root:[ Iteration 78 ] Training loss: 0.170879
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h28m30s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.168384
DEBUG:root:[ Iteration 0 ] Test loss: 0.17407
DEBUG:root:[ Iteration 3 ] Training loss: 0.168252
DEBUG:root:[ Iteration 6 ] Training loss: 0.168073
DEBUG:root:[ Iteration 9 ] Training loss: 0.16789
DEBUG:root:[ Iteration 12 ] Training loss: 0.167707
DEBUG:root:[ Iteration 15 ] Training loss: 0.167526
DEBUG:root:[ Iteration 18 ] Training loss: 0.167347
DEBUG:root:[ Iteration 20 ] Test loss: 0.17311
DEBUG:root:[ Iteration 21 ] Training loss: 0.167169
DEBUG:root:[ Iteration 24 ] Training loss: 0.166992
DEBUG:root:[ Iteration 27 ] Training loss: 0.166817
DEBUG:root:[ Iteration 30 ] Training loss: 0.166643
DEBUG:root:[ Iteration 33 ] Training loss: 0.16647
DEBUG:root:[ Iteration 36 ] Training loss: 0.166299
DEBUG:root:[ Iteration 39 ] Training loss: 0.16613
DEBUG:root:[ Iteration 40 ] Test loss: 0.172157
DEBUG:root:[ Iteration 42 ] Training loss: 0.165961
DEBUG:root:[ Iteration 45 ] Training loss: 0.165794
DEBUG:root:[ Iteration 48 ] Training loss: 0.165627
DEBUG:root:[ Iteration 51 ] Training loss: 0.165462
DEBUG:root:[ Iteration 54 ] Training loss: 0.165299
DEBUG:root:[ Iteration 57 ] Training loss: 0.165136
DEBUG:root:[ Iteration 60 ] Training loss: 0.164974
DEBUG:root:[ Iteration 60 ] Test loss: 0.171255
DEBUG:root:[ Iteration 63 ] Training loss: 0.164813
DEBUG:root:[ Iteration 66 ] Training loss: 0.164653
DEBUG:root:[ Iteration 69 ] Training loss: 0.164494
DEBUG:root:[ Iteration 72 ] Training loss: 0.164336
DEBUG:root:[ Iteration 75 ] Training loss: 0.164179
DEBUG:root:[ Iteration 78 ] Training loss: 0.164022
DEBUG:root:[ Iteration 80 ] Test loss: 0.170396
DEBUG:root:[ Iteration 81 ] Training loss: 0.163867
DEBUG:root:[ Iteration 84 ] Training loss: 0.163712
DEBUG:root:[ Iteration 87 ] Training loss: 0.163558
DEBUG:root:[ Iteration 90 ] Training loss: 0.163404
DEBUG:root:[ Iteration 93 ] Training loss: 0.163251
DEBUG:root:[ Iteration 96 ] Training loss: 0.163098
DEBUG:root:[ Iteration 99 ] Training loss: 0.162946
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h29m37s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.177447
DEBUG:root:[ Iteration 0 ] Test loss: 0.175414
DEBUG:root:[ Iteration 3 ] Training loss: 0.177039
DEBUG:root:[ Iteration 6 ] Training loss: 0.176167
DEBUG:root:[ Iteration 9 ] Training loss: 0.174915
DEBUG:root:[ Iteration 12 ] Training loss: 0.173237
DEBUG:root:[ Iteration 15 ] Training loss: 0.170934
DEBUG:root:[ Iteration 18 ] Training loss: 0.167514
DEBUG:root:[ Iteration 20 ] Test loss: 0.16782
DEBUG:root:[ Iteration 21 ] Training loss: 0.161985
DEBUG:root:[ Iteration 24 ] Training loss: 0.152997
DEBUG:root:[ Iteration 27 ] Training loss: 0.141399
DEBUG:root:[ Iteration 30 ] Training loss: 0.135879
DEBUG:root:[ Iteration 33 ] Training loss: 0.139377
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h32m24s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.179828
DEBUG:root:[ Iteration 0 ] Test loss: 0.173749
DEBUG:root:[ Iteration 3 ] Training loss: 0.179812
DEBUG:root:[ Iteration 6 ] Training loss: 0.179776
DEBUG:root:[ Iteration 9 ] Training loss: 0.179726
DEBUG:root:[ Iteration 12 ] Training loss: 0.179668
DEBUG:root:[ Iteration 15 ] Training loss: 0.179604
DEBUG:root:[ Iteration 18 ] Training loss: 0.179537
DEBUG:root:[ Iteration 20 ] Test loss: 0.172843
DEBUG:root:[ Iteration 21 ] Training loss: 0.179469
DEBUG:root:[ Iteration 24 ] Training loss: 0.179401
DEBUG:root:[ Iteration 27 ] Training loss: 0.179333
DEBUG:root:[ Iteration 30 ] Training loss: 0.179267
DEBUG:root:[ Iteration 33 ] Training loss: 0.179203
DEBUG:root:[ Iteration 36 ] Training loss: 0.179141
DEBUG:root:[ Iteration 39 ] Training loss: 0.179081
DEBUG:root:[ Iteration 40 ] Test loss: 0.171593
DEBUG:root:[ Iteration 42 ] Training loss: 0.179023
DEBUG:root:[ Iteration 45 ] Training loss: 0.178968
DEBUG:root:[ Iteration 48 ] Training loss: 0.178916
DEBUG:root:[ Iteration 51 ] Training loss: 0.178865
DEBUG:root:[ Iteration 54 ] Training loss: 0.178817
DEBUG:root:[ Iteration 57 ] Training loss: 0.178771
DEBUG:root:[ Iteration 60 ] Training loss: 0.178728
DEBUG:root:[ Iteration 60 ] Test loss: 0.170515
DEBUG:root:[ Iteration 63 ] Training loss: 0.178686
DEBUG:root:[ Iteration 66 ] Training loss: 0.178646
DEBUG:root:[ Iteration 69 ] Training loss: 0.178608
DEBUG:root:[ Iteration 72 ] Training loss: 0.178572
DEBUG:root:[ Iteration 75 ] Training loss: 0.178538
DEBUG:root:[ Iteration 78 ] Training loss: 0.178505
DEBUG:root:[ Iteration 80 ] Test loss: 0.169635
DEBUG:root:[ Iteration 81 ] Training loss: 0.178474
DEBUG:root:[ Iteration 84 ] Training loss: 0.178444
DEBUG:root:[ Iteration 87 ] Training loss: 0.178416
DEBUG:root:[ Iteration 90 ] Training loss: 0.178389
DEBUG:root:[ Iteration 93 ] Training loss: 0.178363
DEBUG:root:[ Iteration 96 ] Training loss: 0.178339
DEBUG:root:[ Iteration 99 ] Training loss: 0.178316
DEBUG:root:[ Iteration 100 ] Test loss: 0.168917
DEBUG:root:[ Iteration 102 ] Training loss: 0.178294
DEBUG:root:[ Iteration 105 ] Training loss: 0.178272
DEBUG:root:[ Iteration 108 ] Training loss: 0.178252
DEBUG:root:[ Iteration 111 ] Training loss: 0.178233
DEBUG:root:[ Iteration 114 ] Training loss: 0.178215
DEBUG:root:[ Iteration 117 ] Training loss: 0.178198
DEBUG:root:[ Iteration 120 ] Training loss: 0.178181
DEBUG:root:[ Iteration 120 ] Test loss: 0.168332
DEBUG:root:[ Iteration 123 ] Training loss: 0.178165
DEBUG:root:[ Iteration 126 ] Training loss: 0.178151
DEBUG:root:[ Iteration 129 ] Training loss: 0.178136
DEBUG:root:[ Iteration 132 ] Training loss: 0.178123
DEBUG:root:[ Iteration 135 ] Training loss: 0.17811
DEBUG:root:[ Iteration 138 ] Training loss: 0.178097
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h36m18s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.183775
DEBUG:root:[ Iteration 0 ] Test loss: 0.173884
DEBUG:root:[ Iteration 3 ] Training loss: 0.183362
DEBUG:root:[ Iteration 6 ] Training loss: 0.182937
DEBUG:root:[ Iteration 9 ] Training loss: 0.182514
DEBUG:root:[ Iteration 12 ] Training loss: 0.182093
DEBUG:root:[ Iteration 15 ] Training loss: 0.181674
DEBUG:root:[ Iteration 18 ] Training loss: 0.181257
DEBUG:root:[ Iteration 20 ] Test loss: 0.174666
DEBUG:root:[ Iteration 21 ] Training loss: 0.180842
DEBUG:root:[ Iteration 24 ] Training loss: 0.18043
DEBUG:root:[ Iteration 27 ] Training loss: 0.180019
DEBUG:root:[ Iteration 30 ] Training loss: 0.179611
DEBUG:root:[ Iteration 33 ] Training loss: 0.179204
DEBUG:root:[ Iteration 36 ] Training loss: 0.1788
DEBUG:root:[ Iteration 39 ] Training loss: 0.178397
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h38m19s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.175754
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h38m49s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.200106
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h41m37s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.557407
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h42m56s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.266401
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h44m03s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.189743
DEBUG:root:[ Iteration 0 ] Test loss: 0.188355
DEBUG:root:[ Iteration 3 ] Training loss: 0.179728
DEBUG:root:[ Iteration 6 ] Training loss: 0.172093
DEBUG:root:[ Iteration 9 ] Training loss: 0.16589
DEBUG:root:[ Iteration 12 ] Training loss: 0.16054
DEBUG:root:[ Iteration 15 ] Training loss: 0.155747
DEBUG:root:[ Iteration 18 ] Training loss: 0.151464
DEBUG:root:[ Iteration 20 ] Test loss: 0.2095
DEBUG:root:[ Iteration 21 ] Training loss: 0.147599
DEBUG:root:[ Iteration 24 ] Training loss: 0.144198
DEBUG:root:[ Iteration 27 ] Training loss: 0.141329
DEBUG:root:[ Iteration 30 ] Training loss: 0.138786
DEBUG:root:[ Iteration 33 ] Training loss: 0.136602
DEBUG:root:[ Iteration 36 ] Training loss: 0.134716
DEBUG:root:[ Iteration 39 ] Training loss: 0.133098
DEBUG:root:[ Iteration 40 ] Test loss: 0.236134
DEBUG:root:[ Iteration 42 ] Training loss: 0.131696
DEBUG:root:[ Iteration 45 ] Training loss: 0.130515
DEBUG:root:[ Iteration 48 ] Training loss: 0.129483
DEBUG:root:[ Iteration 51 ] Training loss: 0.128577
DEBUG:root:[ Iteration 54 ] Training loss: 0.127778
DEBUG:root:[ Iteration 57 ] Training loss: 0.12706
DEBUG:root:[ Iteration 60 ] Training loss: 0.126325
DEBUG:root:[ Iteration 60 ] Test loss: 0.251347
DEBUG:root:[ Iteration 63 ] Training loss: 0.125486
DEBUG:root:[ Iteration 66 ] Training loss: 0.124671
DEBUG:root:[ Iteration 69 ] Training loss: 0.124065
DEBUG:root:[ Iteration 72 ] Training loss: 0.123513
DEBUG:root:[ Iteration 75 ] Training loss: 0.123059
DEBUG:root:[ Iteration 78 ] Training loss: 0.122609
DEBUG:root:[ Iteration 80 ] Test loss: 0.260893
DEBUG:root:[ Iteration 81 ] Training loss: 0.122158
DEBUG:root:[ Iteration 84 ] Training loss: 0.121732
DEBUG:root:[ Iteration 87 ] Training loss: 0.121287
DEBUG:root:[ Iteration 90 ] Training loss: 0.120843
DEBUG:root:[ Iteration 93 ] Training loss: 0.120373
DEBUG:root:[ Iteration 96 ] Training loss: 0.119911
DEBUG:root:[ Iteration 99 ] Training loss: 0.119439
DEBUG:root:[ Iteration 100 ] Test loss: 0.267551
DEBUG:root:[ Iteration 102 ] Training loss: 0.118953
DEBUG:root:[ Iteration 105 ] Training loss: 0.118463
DEBUG:root:[ Iteration 108 ] Training loss: 0.117965
DEBUG:root:[ Iteration 111 ] Training loss: 0.117466
DEBUG:root:[ Iteration 114 ] Training loss: 0.116932
DEBUG:root:[ Iteration 117 ] Training loss: 0.116391
DEBUG:root:[ Iteration 120 ] Training loss: 0.115844
DEBUG:root:[ Iteration 120 ] Test loss: 0.272217
DEBUG:root:[ Iteration 123 ] Training loss: 0.115281
DEBUG:root:[ Iteration 126 ] Training loss: 0.1147
DEBUG:root:[ Iteration 129 ] Training loss: 0.114096
DEBUG:root:[ Iteration 132 ] Training loss: 0.113487
DEBUG:root:[ Iteration 135 ] Training loss: 0.112868
DEBUG:root:[ Iteration 138 ] Training loss: 0.112214
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h45m27s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.18998
DEBUG:root:[ Iteration 0 ] Test loss: 0.18861
DEBUG:root:[ Iteration 3 ] Training loss: 0.180704
DEBUG:root:[ Iteration 6 ] Training loss: 0.178644
DEBUG:root:[ Iteration 9 ] Training loss: 0.178686
DEBUG:root:[ Iteration 12 ] Training loss: 0.176136
DEBUG:root:[ Iteration 15 ] Training loss: 0.173854
DEBUG:root:[ Iteration 18 ] Training loss: 0.177637
DEBUG:root:[ Iteration 20 ] Test loss: 0.175632
DEBUG:root:[ Iteration 21 ] Training loss: 0.175101
DEBUG:root:[ Iteration 24 ] Training loss: 0.173204
DEBUG:root:[ Iteration 27 ] Training loss: 0.172706
DEBUG:root:[ Iteration 30 ] Training loss: 0.176133
DEBUG:root:[ Iteration 33 ] Training loss: 0.175326
DEBUG:root:[ Iteration 36 ] Training loss: 0.169879
DEBUG:root:[ Iteration 39 ] Training loss: 0.174887
DEBUG:root:[ Iteration 40 ] Test loss: 0.173195
DEBUG:root:[ Iteration 42 ] Training loss: 0.173737
DEBUG:root:[ Iteration 45 ] Training loss: 0.170875
DEBUG:root:[ Iteration 48 ] Training loss: 0.172533
DEBUG:root:[ Iteration 51 ] Training loss: 0.170213
DEBUG:root:[ Iteration 54 ] Training loss: 0.170799
DEBUG:root:[ Iteration 57 ] Training loss: 0.177115
DEBUG:root:[ Iteration 60 ] Training loss: 0.164246
DEBUG:root:[ Iteration 60 ] Test loss: 0.172228
DEBUG:root:[ Iteration 63 ] Training loss: 0.176428
DEBUG:root:[ Iteration 66 ] Training loss: 0.1753
DEBUG:root:[ Iteration 69 ] Training loss: 0.168545
DEBUG:root:[ Iteration 72 ] Training loss: 0.170092
DEBUG:root:[ Iteration 75 ] Training loss: 0.165573
DEBUG:root:[ Iteration 78 ] Training loss: 0.174132
DEBUG:root:[ Iteration 80 ] Test loss: 0.171302
DEBUG:root:[ Iteration 81 ] Training loss: 0.169767
DEBUG:root:[ Iteration 84 ] Training loss: 0.169025
DEBUG:root:[ Iteration 87 ] Training loss: 0.171219
DEBUG:root:[ Iteration 90 ] Training loss: 0.168345
DEBUG:root:[ Iteration 93 ] Training loss: 0.172003
DEBUG:root:[ Iteration 96 ] Training loss: 0.164018
DEBUG:root:[ Iteration 99 ] Training loss: 0.17251
DEBUG:root:[ Iteration 100 ] Test loss: 0.170394
DEBUG:root:[ Iteration 102 ] Training loss: 0.170893
DEBUG:root:[ Iteration 105 ] Training loss: 0.170589
DEBUG:root:[ Iteration 108 ] Training loss: 0.168622
DEBUG:root:[ Iteration 111 ] Training loss: 0.16378
DEBUG:root:[ Iteration 114 ] Training loss: 0.172446
DEBUG:root:[ Iteration 117 ] Training loss: 0.166617
DEBUG:root:[ Iteration 120 ] Training loss: 0.163143
DEBUG:root:[ Iteration 120 ] Test loss: 0.169477
DEBUG:root:[ Iteration 123 ] Training loss: 0.172897
DEBUG:root:[ Iteration 126 ] Training loss: 0.170348
DEBUG:root:[ Iteration 129 ] Training loss: 0.165646
DEBUG:root:[ Iteration 132 ] Training loss: 0.164003
DEBUG:root:[ Iteration 135 ] Training loss: 0.174128
DEBUG:root:[ Iteration 138 ] Training loss: 0.165048
DEBUG:root:[ Iteration 140 ] Test loss: 0.168526
DEBUG:root:[ Iteration 141 ] Training loss: 0.169251
DEBUG:root:[ Iteration 144 ] Training loss: 0.171967
DEBUG:root:[ Iteration 147 ] Training loss: 0.160545
DEBUG:root:[ Iteration 150 ] Training loss: 0.163597
DEBUG:root:[ Iteration 153 ] Training loss: 0.163677
DEBUG:root:[ Iteration 156 ] Training loss: 0.16906
DEBUG:root:[ Iteration 159 ] Training loss: 0.161482
DEBUG:root:[ Iteration 160 ] Test loss: 0.16747
DEBUG:root:[ Iteration 162 ] Training loss: 0.171815
DEBUG:root:[ Iteration 165 ] Training loss: 0.160828
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-14-2016_19h52m41s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.543662
DEBUG:root:[ Iteration 0 ] Test loss: 0.566447
DEBUG:root:[ Iteration 3 ] Training loss: 0.201591
DEBUG:root:[ Iteration 6 ] Training loss: 0.16694
DEBUG:root:[ Iteration 9 ] Training loss: 0.157172
DEBUG:root:[ Iteration 12 ] Training loss: 0.154956
DEBUG:root:[ Iteration 15 ] Training loss: 0.154302
DEBUG:root:[ Iteration 18 ] Training loss: 0.158666
DEBUG:root:[ Iteration 20 ] Test loss: 0.154368
DEBUG:root:[ Iteration 21 ] Training loss: 0.148697
DEBUG:root:[ Iteration 24 ] Training loss: 0.14612
DEBUG:root:[ Iteration 27 ] Training loss: 0.141993
DEBUG:root:[ Iteration 30 ] Training loss: 0.148857
DEBUG:root:[ Iteration 33 ] Training loss: 0.147198
DEBUG:root:[ Iteration 36 ] Training loss: 0.143977
DEBUG:root:[ Iteration 39 ] Training loss: 0.149767
DEBUG:root:[ Iteration 40 ] Test loss: 0.145655
DEBUG:root:[ Iteration 42 ] Training loss: 0.139296
DEBUG:root:[ Iteration 45 ] Training loss: 0.139273
DEBUG:root:[ Iteration 48 ] Training loss: 0.135423
DEBUG:root:[ Iteration 51 ] Training loss: 0.13053
DEBUG:root:[ Iteration 54 ] Training loss: 0.133196
DEBUG:root:[ Iteration 57 ] Training loss: 0.141797
DEBUG:root:[ Iteration 60 ] Training loss: 0.132189
DEBUG:root:[ Iteration 60 ] Test loss: 0.140526
DEBUG:root:[ Iteration 63 ] Training loss: 0.13615
DEBUG:root:[ Iteration 66 ] Training loss: 0.147198
DEBUG:root:[ Iteration 69 ] Training loss: 0.137802
DEBUG:root:[ Iteration 72 ] Training loss: 0.13943
DEBUG:root:[ Iteration 75 ] Training loss: 0.146376
DEBUG:root:[ Iteration 78 ] Training loss: 0.121616
DEBUG:root:[ Iteration 80 ] Test loss: 0.13745
DEBUG:root:[ Iteration 81 ] Training loss: 0.128532
DEBUG:root:[ Iteration 84 ] Training loss: 0.142504
DEBUG:root:[ Iteration 87 ] Training loss: 0.142561
DEBUG:root:[ Iteration 90 ] Training loss: 0.13416
DEBUG:root:[ Iteration 93 ] Training loss: 0.129005
DEBUG:root:[ Iteration 96 ] Training loss: 0.1267
DEBUG:root:[ Iteration 99 ] Training loss: 0.130197
DEBUG:root:[ Iteration 100 ] Test loss: 0.135507
DEBUG:root:[ Iteration 102 ] Training loss: 0.144774
DEBUG:root:[ Iteration 105 ] Training loss: 0.131918
DEBUG:root:[ Iteration 108 ] Training loss: 0.139696
DEBUG:root:[ Iteration 111 ] Training loss: 0.118006
DEBUG:root:[ Iteration 114 ] Training loss: 0.12175
DEBUG:root:[ Iteration 117 ] Training loss: 0.121576
DEBUG:root:[ Iteration 120 ] Training loss: 0.14415
DEBUG:root:[ Iteration 120 ] Test loss: 0.134206
DEBUG:root:[ Iteration 123 ] Training loss: 0.134563
DEBUG:root:[ Iteration 126 ] Training loss: 0.12194
DEBUG:root:[ Iteration 129 ] Training loss: 0.122032
DEBUG:root:[ Iteration 132 ] Training loss: 0.130306
DEBUG:root:[ Iteration 135 ] Training loss: 0.132184
DEBUG:root:[ Iteration 138 ] Training loss: 0.130601
DEBUG:root:[ Iteration 140 ] Test loss: 0.133284
DEBUG:root:[ Iteration 141 ] Training loss: 0.113022
DEBUG:root:[ Iteration 144 ] Training loss: 0.130246
DEBUG:root:[ Iteration 147 ] Training loss: 0.121282
DEBUG:root:[ Iteration 150 ] Training loss: 0.123306
DEBUG:root:[ Iteration 153 ] Training loss: 0.137286
DEBUG:root:[ Iteration 156 ] Training loss: 0.119858
DEBUG:root:[ Iteration 159 ] Training loss: 0.129459
DEBUG:root:[ Iteration 160 ] Test loss: 0.132658
DEBUG:root:[ Iteration 162 ] Training loss: 0.128409
DEBUG:root:[ Iteration 165 ] Training loss: 0.135586
DEBUG:root:[ Iteration 168 ] Training loss: 0.107508
DEBUG:root:[ Iteration 171 ] Training loss: 0.116868
DEBUG:root:[ Iteration 174 ] Training loss: 0.121304
DEBUG:root:[ Iteration 177 ] Training loss: 0.119805
DEBUG:root:[ Iteration 180 ] Training loss: 0.132372
DEBUG:root:[ Iteration 180 ] Test loss: 0.132114
DEBUG:root:[ Iteration 183 ] Training loss: 0.13667
DEBUG:root:[ Iteration 186 ] Training loss: 0.118312
DEBUG:root:[ Iteration 189 ] Training loss: 0.122916
DEBUG:root:[ Iteration 192 ] Training loss: 0.132281
DEBUG:root:[ Iteration 195 ] Training loss: 0.126608
DEBUG:root:[ Iteration 198 ] Training loss: 0.11893
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-15-2016_12h48m51s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.134029
DEBUG:root:[ Iteration 0 ] Test loss: 0.131742
DEBUG:root:[ Iteration 3 ] Training loss: 0.122359
DEBUG:root:[ Iteration 6 ] Training loss: 0.124041
DEBUG:root:[ Iteration 9 ] Training loss: 0.125284
DEBUG:root:[ Iteration 12 ] Training loss: 0.130677
DEBUG:root:[ Iteration 15 ] Training loss: 0.127507
DEBUG:root:[ Iteration 18 ] Training loss: 0.13131
DEBUG:root:[ Iteration 20 ] Test loss: 0.131284
DEBUG:root:[ Iteration 21 ] Training loss: 0.12447
DEBUG:root:[ Iteration 24 ] Training loss: 0.123738
DEBUG:root:[ Iteration 27 ] Training loss: 0.124976
DEBUG:root:[ Iteration 30 ] Training loss: 0.130955
DEBUG:root:[ Iteration 33 ] Training loss: 0.131952
DEBUG:root:[ Iteration 36 ] Training loss: 0.131195
DEBUG:root:[ Iteration 39 ] Training loss: 0.12235
DEBUG:root:[ Iteration 40 ] Test loss: 0.131011
DEBUG:root:[ Iteration 42 ] Training loss: 0.133362
DEBUG:root:[ Iteration 45 ] Training loss: 0.127361
DEBUG:root:[ Iteration 48 ] Training loss: 0.121394
DEBUG:root:[ Iteration 51 ] Training loss: 0.110515
DEBUG:root:[ Iteration 54 ] Training loss: 0.133534
DEBUG:root:[ Iteration 57 ] Training loss: 0.124112
DEBUG:root:[ Iteration 60 ] Training loss: 0.131603
DEBUG:root:[ Iteration 60 ] Test loss: 0.130626
DEBUG:root:[ Iteration 63 ] Training loss: 0.114665
DEBUG:root:[ Iteration 66 ] Training loss: 0.124101
DEBUG:root:[ Iteration 69 ] Training loss: 0.118172
DEBUG:root:[ Iteration 72 ] Training loss: 0.115442
DEBUG:root:[ Iteration 75 ] Training loss: 0.141035
DEBUG:root:[ Iteration 78 ] Training loss: 0.122226
DEBUG:root:[ Iteration 80 ] Test loss: 0.130295
DEBUG:root:[ Iteration 81 ] Training loss: 0.11738
DEBUG:root:[ Iteration 84 ] Training loss: 0.114805
DEBUG:root:[ Iteration 87 ] Training loss: 0.117079
DEBUG:root:[ Iteration 90 ] Training loss: 0.129824
DEBUG:root:[ Iteration 93 ] Training loss: 0.116034
DEBUG:root:[ Iteration 96 ] Training loss: 0.142086
DEBUG:root:[ Iteration 99 ] Training loss: 0.122348
DEBUG:root:[ Iteration 100 ] Test loss: 0.130024
DEBUG:root:[ Iteration 102 ] Training loss: 0.133794
DEBUG:root:[ Iteration 105 ] Training loss: 0.122147
DEBUG:root:Saving...
DEBUG:root:[ Iteration 0 ] Training loss: 0.680741
DEBUG:root:[ Iteration 0 ] Test loss: 0.698288
DEBUG:root:[ Iteration 0 ] Training loss: 0.542222
DEBUG:root:[ Iteration 0 ] Test loss: 0.477242
DEBUG:root:[ Iteration 3 ] Training loss: 0.378234
DEBUG:root:[ Iteration 6 ] Training loss: 0.32523
DEBUG:root:[ Iteration 9 ] Training loss: 0.361871
DEBUG:root:[ Iteration 12 ] Training loss: 0.23526
DEBUG:root:[ Iteration 15 ] Training loss: 0.253741
DEBUG:root:[ Iteration 18 ] Training loss: 0.242699
DEBUG:root:[ Iteration 20 ] Test loss: 0.259008
DEBUG:root:[ Iteration 21 ] Training loss: 0.226739
DEBUG:root:[ Iteration 24 ] Training loss: 0.237972
DEBUG:root:[ Iteration 27 ] Training loss: 0.194077
DEBUG:root:[ Iteration 30 ] Training loss: 0.245766
DEBUG:root:[ Iteration 33 ] Training loss: 0.203908
DEBUG:root:[ Iteration 36 ] Training loss: 0.168665
DEBUG:root:[ Iteration 39 ] Training loss: 0.15879
DEBUG:root:[ Iteration 40 ] Test loss: 0.174787
DEBUG:root:[ Iteration 42 ] Training loss: 0.200719
DEBUG:root:[ Iteration 45 ] Training loss: 0.170061
DEBUG:root:[ Iteration 48 ] Training loss: 0.182317
DEBUG:root:[ Iteration 51 ] Training loss: 0.186176
DEBUG:root:[ Iteration 54 ] Training loss: 0.149352
DEBUG:root:[ Iteration 57 ] Training loss: 0.168221
DEBUG:root:[ Iteration 60 ] Training loss: 0.168657
DEBUG:root:[ Iteration 60 ] Test loss: 0.149366
DEBUG:root:[ Iteration 63 ] Training loss: 0.143689
DEBUG:root:[ Iteration 66 ] Training loss: 0.141336
DEBUG:root:[ Iteration 69 ] Training loss: 0.128344
DEBUG:root:[ Iteration 72 ] Training loss: 0.166366
DEBUG:root:[ Iteration 75 ] Training loss: 0.146576
DEBUG:root:[ Iteration 78 ] Training loss: 0.140626
DEBUG:root:[ Iteration 80 ] Test loss: 0.137041
DEBUG:root:[ Iteration 81 ] Training loss: 0.19446
DEBUG:root:[ Iteration 84 ] Training loss: 0.140973
DEBUG:root:[ Iteration 87 ] Training loss: 0.144235
DEBUG:root:[ Iteration 90 ] Training loss: 0.108227
DEBUG:root:[ Iteration 93 ] Training loss: 0.119521
DEBUG:root:[ Iteration 96 ] Training loss: 0.135227
DEBUG:root:[ Iteration 99 ] Training loss: 0.149005
DEBUG:root:[ Iteration 100 ] Test loss: 0.132857
DEBUG:root:[ Iteration 102 ] Training loss: 0.138258
DEBUG:root:[ Iteration 105 ] Training loss: 0.116177
DEBUG:root:[ Iteration 108 ] Training loss: 0.143794
DEBUG:root:[ Iteration 111 ] Training loss: 0.129642
DEBUG:root:[ Iteration 114 ] Training loss: 0.116737
DEBUG:root:[ Iteration 117 ] Training loss: 0.162632
DEBUG:root:[ Iteration 120 ] Training loss: 0.114155
DEBUG:root:[ Iteration 120 ] Test loss: 0.127118
DEBUG:root:[ Iteration 123 ] Training loss: 0.132674
DEBUG:root:[ Iteration 126 ] Training loss: 0.129771
DEBUG:root:[ Iteration 129 ] Training loss: 0.120995
DEBUG:root:[ Iteration 132 ] Training loss: 0.114147
DEBUG:root:[ Iteration 135 ] Training loss: 0.149544
DEBUG:root:[ Iteration 138 ] Training loss: 0.143253
DEBUG:root:[ Iteration 140 ] Test loss: 0.120686
DEBUG:root:[ Iteration 141 ] Training loss: 0.118262
DEBUG:root:[ Iteration 144 ] Training loss: 0.121839
DEBUG:root:[ Iteration 147 ] Training loss: 0.120908
DEBUG:root:[ Iteration 150 ] Training loss: 0.115537
DEBUG:root:[ Iteration 153 ] Training loss: 0.12518
DEBUG:root:[ Iteration 156 ] Training loss: 0.117498
DEBUG:root:[ Iteration 159 ] Training loss: 0.122495
DEBUG:root:[ Iteration 160 ] Test loss: 0.116777
DEBUG:root:[ Iteration 162 ] Training loss: 0.118593
DEBUG:root:[ Iteration 165 ] Training loss: 0.102919
DEBUG:root:[ Iteration 168 ] Training loss: 0.127421
DEBUG:root:[ Iteration 171 ] Training loss: 0.102137
DEBUG:root:[ Iteration 174 ] Training loss: 0.148283
DEBUG:root:[ Iteration 177 ] Training loss: 0.138579
DEBUG:root:[ Iteration 180 ] Training loss: 0.139621
DEBUG:root:[ Iteration 180 ] Test loss: 0.114831
DEBUG:root:[ Iteration 183 ] Training loss: 0.129803
DEBUG:root:[ Iteration 186 ] Training loss: 0.136717
DEBUG:root:[ Iteration 189 ] Training loss: 0.112552
DEBUG:root:[ Iteration 192 ] Training loss: 0.101781
DEBUG:root:[ Iteration 195 ] Training loss: 0.136025
DEBUG:root:[ Iteration 198 ] Training loss: 0.113399
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/./net6/net6_02-15-2016_13h29m29s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.736196
DEBUG:root:Saving...
DEBUG:root:[ Iteration 0 ] Training loss: 0.128512
DEBUG:root:[ Iteration 0 ] Test loss: 0.113351
DEBUG:root:[ Iteration 3 ] Training loss: 0.102719
DEBUG:root:[ Iteration 6 ] Training loss: 0.12465
DEBUG:root:[ Iteration 9 ] Training loss: 0.122578
DEBUG:root:[ Iteration 12 ] Training loss: 0.115419
DEBUG:root:[ Iteration 15 ] Training loss: 0.102003
DEBUG:root:[ Iteration 18 ] Training loss: 0.118328
DEBUG:root:[ Iteration 20 ] Test loss: 0.112278
DEBUG:root:[ Iteration 21 ] Training loss: 0.110736
DEBUG:root:[ Iteration 24 ] Training loss: 0.0951936
DEBUG:root:[ Iteration 27 ] Training loss: 0.1158
DEBUG:root:[ Iteration 30 ] Training loss: 0.105534
DEBUG:root:[ Iteration 33 ] Training loss: 0.110213
DEBUG:root:[ Iteration 36 ] Training loss: 0.0958546
DEBUG:root:[ Iteration 39 ] Training loss: 0.14163
DEBUG:root:[ Iteration 40 ] Test loss: 0.113104
DEBUG:root:[ Iteration 42 ] Training loss: 0.120737
DEBUG:root:[ Iteration 45 ] Training loss: 0.125917
DEBUG:root:[ Iteration 48 ] Training loss: 0.135109
DEBUG:root:[ Iteration 51 ] Training loss: 0.114595
DEBUG:root:[ Iteration 54 ] Training loss: 0.105075
DEBUG:root:[ Iteration 57 ] Training loss: 0.109014
DEBUG:root:[ Iteration 60 ] Training loss: 0.133503
DEBUG:root:[ Iteration 60 ] Test loss: 0.109558
DEBUG:root:[ Iteration 63 ] Training loss: 0.0960028
DEBUG:root:[ Iteration 66 ] Training loss: 0.116799
DEBUG:root:[ Iteration 69 ] Training loss: 0.0924438
DEBUG:root:[ Iteration 72 ] Training loss: 0.116216
DEBUG:root:[ Iteration 75 ] Training loss: 0.113413
DEBUG:root:[ Iteration 78 ] Training loss: 0.132551
DEBUG:root:[ Iteration 80 ] Test loss: 0.109271
DEBUG:root:[ Iteration 81 ] Training loss: 0.132717
DEBUG:root:[ Iteration 84 ] Training loss: 0.140909
DEBUG:root:[ Iteration 87 ] Training loss: 0.088364
DEBUG:root:[ Iteration 90 ] Training loss: 0.104079
DEBUG:root:[ Iteration 93 ] Training loss: 0.102339
DEBUG:root:[ Iteration 96 ] Training loss: 0.12518
DEBUG:root:[ Iteration 99 ] Training loss: 0.108485
DEBUG:root:[ Iteration 100 ] Test loss: 0.106893
DEBUG:root:[ Iteration 102 ] Training loss: 0.10598
DEBUG:root:[ Iteration 105 ] Training loss: 0.100287
DEBUG:root:[ Iteration 108 ] Training loss: 0.0981877
DEBUG:root:[ Iteration 111 ] Training loss: 0.10179
DEBUG:root:[ Iteration 114 ] Training loss: 0.105573
DEBUG:root:[ Iteration 117 ] Training loss: 0.0940692
DEBUG:root:[ Iteration 120 ] Training loss: 0.127544
DEBUG:root:[ Iteration 120 ] Test loss: 0.108476
DEBUG:root:[ Iteration 123 ] Training loss: 0.102371
DEBUG:root:[ Iteration 126 ] Training loss: 0.102103
DEBUG:root:[ Iteration 129 ] Training loss: 0.0977843
DEBUG:root:[ Iteration 132 ] Training loss: 0.0867964
DEBUG:root:[ Iteration 135 ] Training loss: 0.107994
DEBUG:root:[ Iteration 138 ] Training loss: 0.131411
DEBUG:root:[ Iteration 140 ] Test loss: 0.108203
DEBUG:root:[ Iteration 141 ] Training loss: 0.142631
DEBUG:root:[ Iteration 144 ] Training loss: 0.121555
DEBUG:root:[ Iteration 147 ] Training loss: 0.125996
DEBUG:root:[ Iteration 150 ] Training loss: 0.095924
DEBUG:root:[ Iteration 153 ] Training loss: 0.0992566
DEBUG:root:[ Iteration 156 ] Training loss: 0.102172
DEBUG:root:[ Iteration 159 ] Training loss: 0.0970656
DEBUG:root:[ Iteration 160 ] Test loss: 0.106058
DEBUG:root:[ Iteration 162 ] Training loss: 0.095325
DEBUG:root:[ Iteration 165 ] Training loss: 0.0957922
DEBUG:root:[ Iteration 168 ] Training loss: 0.0878445
DEBUG:root:[ Iteration 171 ] Training loss: 0.101606
DEBUG:root:[ Iteration 174 ] Training loss: 0.121797
DEBUG:root:[ Iteration 177 ] Training loss: 0.111145
DEBUG:root:[ Iteration 180 ] Training loss: 0.114908
DEBUG:root:[ Iteration 180 ] Test loss: 0.105151
DEBUG:root:[ Iteration 183 ] Training loss: 0.10743
DEBUG:root:[ Iteration 186 ] Training loss: 0.111187
DEBUG:root:[ Iteration 189 ] Training loss: 0.0953474
DEBUG:root:[ Iteration 192 ] Training loss: 0.10429
DEBUG:root:[ Iteration 195 ] Training loss: 0.0964178
DEBUG:root:[ Iteration 198 ] Training loss: 0.100739
DEBUG:root:Saving...
DEBUG:root:[ Iteration 0 ] Training loss: 0.123312
DEBUG:root:Saving...
DEBUG:root:[ Iteration 0 ] Training loss: 0.133029
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/net6/net6_02-15-2016_13h44m16s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.119189
DEBUG:root:[ Iteration 0 ] Test loss: 0.11335
DEBUG:root:[ Iteration 3 ] Training loss: 0.110114
DEBUG:root:[ Iteration 6 ] Training loss: 0.10752
DEBUG:root:[ Iteration 9 ] Training loss: 0.108812
DEBUG:root:[ Iteration 12 ] Training loss: 0.133335
DEBUG:root:[ Iteration 15 ] Training loss: 0.112957
DEBUG:root:[ Iteration 18 ] Training loss: 0.129161
DEBUG:root:[ Iteration 20 ] Test loss: 0.112202
DEBUG:root:[ Iteration 21 ] Training loss: 0.113814
DEBUG:root:[ Iteration 24 ] Training loss: 0.113295
DEBUG:root:[ Iteration 27 ] Training loss: 0.136231
DEBUG:root:[ Iteration 30 ] Training loss: 0.117016
DEBUG:root:[ Iteration 33 ] Training loss: 0.109096
DEBUG:root:[ Iteration 36 ] Training loss: 0.0935028
DEBUG:root:[ Iteration 39 ] Training loss: 0.123792
DEBUG:root:[ Iteration 40 ] Test loss: 0.110565
DEBUG:root:[ Iteration 42 ] Training loss: 0.111226
DEBUG:root:[ Iteration 45 ] Training loss: 0.078434
DEBUG:root:[ Iteration 48 ] Training loss: 0.105718
DEBUG:root:[ Iteration 51 ] Training loss: 0.113818
DEBUG:root:[ Iteration 54 ] Training loss: 0.105794
DEBUG:root:[ Iteration 57 ] Training loss: 0.116589
DEBUG:root:[ Iteration 60 ] Training loss: 0.114638
DEBUG:root:[ Iteration 60 ] Test loss: 0.109811
DEBUG:root:[ Iteration 63 ] Training loss: 0.101608
DEBUG:root:[ Iteration 66 ] Training loss: 0.105813
DEBUG:root:[ Iteration 69 ] Training loss: 0.104291
DEBUG:root:[ Iteration 72 ] Training loss: 0.101696
DEBUG:root:[ Iteration 75 ] Training loss: 0.117739
DEBUG:root:[ Iteration 78 ] Training loss: 0.121253
DEBUG:root:[ Iteration 80 ] Test loss: 0.107976
DEBUG:root:[ Iteration 81 ] Training loss: 0.11793
DEBUG:root:[ Iteration 84 ] Training loss: 0.106399
DEBUG:root:[ Iteration 87 ] Training loss: 0.103286
DEBUG:root:[ Iteration 90 ] Training loss: 0.108926
DEBUG:root:[ Iteration 93 ] Training loss: 0.117188
DEBUG:root:[ Iteration 96 ] Training loss: 0.10679
DEBUG:root:[ Iteration 99 ] Training loss: 0.109458
DEBUG:root:[ Iteration 100 ] Test loss: 0.107238
DEBUG:root:[ Iteration 102 ] Training loss: 0.13517
DEBUG:root:[ Iteration 105 ] Training loss: 0.115945
DEBUG:root:[ Iteration 108 ] Training loss: 0.107733
DEBUG:root:[ Iteration 111 ] Training loss: 0.0903432
DEBUG:root:[ Iteration 114 ] Training loss: 0.103768
DEBUG:root:[ Iteration 117 ] Training loss: 0.126736
DEBUG:root:[ Iteration 120 ] Training loss: 0.0966899
DEBUG:root:[ Iteration 120 ] Test loss: 0.107369
DEBUG:root:[ Iteration 123 ] Training loss: 0.0942526
DEBUG:root:[ Iteration 126 ] Training loss: 0.0945421
DEBUG:root:[ Iteration 129 ] Training loss: 0.105493
DEBUG:root:[ Iteration 132 ] Training loss: 0.106836
DEBUG:root:[ Iteration 135 ] Training loss: 0.104689
DEBUG:root:[ Iteration 138 ] Training loss: 0.0908785
DEBUG:root:[ Iteration 140 ] Test loss: 0.104872
DEBUG:root:[ Iteration 141 ] Training loss: 0.115012
DEBUG:root:[ Iteration 144 ] Training loss: 0.121201
DEBUG:root:[ Iteration 147 ] Training loss: 0.104571
DEBUG:root:[ Iteration 150 ] Training loss: 0.101475
DEBUG:root:[ Iteration 153 ] Training loss: 0.115572
DEBUG:root:[ Iteration 156 ] Training loss: 0.103851
DEBUG:root:[ Iteration 159 ] Training loss: 0.100552
DEBUG:root:[ Iteration 160 ] Test loss: 0.104267
DEBUG:root:[ Iteration 162 ] Training loss: 0.110361
DEBUG:root:[ Iteration 165 ] Training loss: 0.103709
DEBUG:root:[ Iteration 168 ] Training loss: 0.0969238
DEBUG:root:[ Iteration 171 ] Training loss: 0.111408
DEBUG:root:[ Iteration 174 ] Training loss: 0.100297
DEBUG:root:[ Iteration 177 ] Training loss: 0.120532
DEBUG:root:[ Iteration 180 ] Training loss: 0.100439
DEBUG:root:[ Iteration 180 ] Test loss: 0.103237
DEBUG:root:[ Iteration 183 ] Training loss: 0.0873775
DEBUG:root:[ Iteration 186 ] Training loss: 0.0960252
DEBUG:root:[ Iteration 189 ] Training loss: 0.0998988
DEBUG:root:[ Iteration 192 ] Training loss: 0.0987946
DEBUG:root:[ Iteration 195 ] Training loss: 0.0907203
DEBUG:root:[ Iteration 198 ] Training loss: 0.10667
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/net6/net6_02-15-2016_13h48m35s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.10129
DEBUG:root:[ Iteration 0 ] Test loss: 0.103436
DEBUG:root:[ Iteration 3 ] Training loss: 0.103642
DEBUG:root:[ Iteration 6 ] Training loss: 0.107876
DEBUG:root:[ Iteration 9 ] Training loss: 0.0968528
DEBUG:root:[ Iteration 12 ] Training loss: 0.102248
DEBUG:root:[ Iteration 15 ] Training loss: 0.0953402
DEBUG:root:[ Iteration 18 ] Training loss: 0.10794
DEBUG:root:[ Iteration 20 ] Test loss: 0.103737
DEBUG:root:[ Iteration 21 ] Training loss: 0.10275
DEBUG:root:[ Iteration 24 ] Training loss: 0.0962745
DEBUG:root:[ Iteration 27 ] Training loss: 0.0705856
DEBUG:root:[ Iteration 30 ] Training loss: 0.0979792
DEBUG:root:[ Iteration 33 ] Training loss: 0.114157
DEBUG:root:[ Iteration 36 ] Training loss: 0.0986293
DEBUG:root:[ Iteration 39 ] Training loss: 0.0963812
DEBUG:root:[ Iteration 40 ] Test loss: 0.100782
DEBUG:root:[ Iteration 42 ] Training loss: 0.104023
DEBUG:root:[ Iteration 45 ] Training loss: 0.0822531
DEBUG:root:[ Iteration 48 ] Training loss: 0.0922795
DEBUG:root:[ Iteration 51 ] Training loss: 0.101556
DEBUG:root:[ Iteration 54 ] Training loss: 0.0913708
DEBUG:root:[ Iteration 57 ] Training loss: 0.0915227
DEBUG:root:[ Iteration 60 ] Training loss: 0.104911
DEBUG:root:[ Iteration 60 ] Test loss: 0.0997215
DEBUG:root:[ Iteration 63 ] Training loss: 0.110762
DEBUG:root:[ Iteration 66 ] Training loss: 0.0879966
DEBUG:root:[ Iteration 69 ] Training loss: 0.0981693
DEBUG:root:[ Iteration 72 ] Training loss: 0.102336
DEBUG:root:[ Iteration 75 ] Training loss: 0.121293
DEBUG:root:[ Iteration 78 ] Training loss: 0.109288
DEBUG:root:[ Iteration 80 ] Test loss: 0.0986863
DEBUG:root:[ Iteration 81 ] Training loss: 0.0824973
DEBUG:root:[ Iteration 84 ] Training loss: 0.0964464
DEBUG:root:[ Iteration 87 ] Training loss: 0.0767409
DEBUG:root:[ Iteration 90 ] Training loss: 0.0973205
DEBUG:root:[ Iteration 93 ] Training loss: 0.132461
DEBUG:root:[ Iteration 96 ] Training loss: 0.0794681
DEBUG:root:[ Iteration 99 ] Training loss: 0.0941559
DEBUG:root:[ Iteration 100 ] Test loss: 0.0977624
DEBUG:root:[ Iteration 102 ] Training loss: 0.105686
DEBUG:root:[ Iteration 105 ] Training loss: 0.123835
DEBUG:root:[ Iteration 108 ] Training loss: 0.0828095
DEBUG:root:[ Iteration 111 ] Training loss: 0.0920884
DEBUG:root:[ Iteration 114 ] Training loss: 0.0862641
DEBUG:root:[ Iteration 117 ] Training loss: 0.0987314
DEBUG:root:[ Iteration 120 ] Training loss: 0.103442
DEBUG:root:[ Iteration 120 ] Test loss: 0.098037
DEBUG:root:[ Iteration 123 ] Training loss: 0.0950995
DEBUG:root:[ Iteration 126 ] Training loss: 0.0817248
DEBUG:root:[ Iteration 129 ] Training loss: 0.0939955
DEBUG:root:[ Iteration 132 ] Training loss: 0.093302
DEBUG:root:[ Iteration 135 ] Training loss: 0.0981713
DEBUG:root:[ Iteration 138 ] Training loss: 0.0970903
DEBUG:root:[ Iteration 140 ] Test loss: 0.0961633
DEBUG:root:[ Iteration 141 ] Training loss: 0.102649
DEBUG:root:[ Iteration 144 ] Training loss: 0.0942431
DEBUG:root:[ Iteration 147 ] Training loss: 0.0860566
DEBUG:root:[ Iteration 150 ] Training loss: 0.102258
DEBUG:root:[ Iteration 153 ] Training loss: 0.0681115
DEBUG:root:[ Iteration 156 ] Training loss: 0.0941001
DEBUG:root:[ Iteration 159 ] Training loss: 0.100297
DEBUG:root:[ Iteration 160 ] Test loss: 0.0956383
DEBUG:root:[ Iteration 162 ] Training loss: 0.101699
DEBUG:root:[ Iteration 165 ] Training loss: 0.092614
DEBUG:root:[ Iteration 168 ] Training loss: 0.110809
DEBUG:root:[ Iteration 171 ] Training loss: 0.105897
DEBUG:root:[ Iteration 174 ] Training loss: 0.108216
DEBUG:root:[ Iteration 177 ] Training loss: 0.0821529
DEBUG:root:[ Iteration 180 ] Training loss: 0.092768
DEBUG:root:[ Iteration 180 ] Test loss: 0.0951607
DEBUG:root:[ Iteration 183 ] Training loss: 0.10519
DEBUG:root:[ Iteration 186 ] Training loss: 0.0774215
DEBUG:root:[ Iteration 189 ] Training loss: 0.0921951
DEBUG:root:[ Iteration 192 ] Training loss: 0.0899538
DEBUG:root:[ Iteration 195 ] Training loss: 0.0893446
DEBUG:root:[ Iteration 198 ] Training loss: 0.0837681
DEBUG:root:[ Iteration 200 ] Test loss: 0.0938902
DEBUG:root:[ Iteration 201 ] Training loss: 0.104305
DEBUG:root:[ Iteration 204 ] Training loss: 0.106954
DEBUG:root:[ Iteration 207 ] Training loss: 0.0838279
DEBUG:root:[ Iteration 210 ] Training loss: 0.110953
DEBUG:root:[ Iteration 213 ] Training loss: 0.0842635
DEBUG:root:[ Iteration 216 ] Training loss: 0.0883103
DEBUG:root:[ Iteration 219 ] Training loss: 0.0919074
DEBUG:root:[ Iteration 220 ] Test loss: 0.0932788
DEBUG:root:[ Iteration 222 ] Training loss: 0.0913478
DEBUG:root:[ Iteration 225 ] Training loss: 0.102491
DEBUG:root:[ Iteration 228 ] Training loss: 0.0842305
DEBUG:root:[ Iteration 231 ] Training loss: 0.0926299
DEBUG:root:[ Iteration 234 ] Training loss: 0.098329
DEBUG:root:[ Iteration 237 ] Training loss: 0.0837995
DEBUG:root:[ Iteration 240 ] Training loss: 0.0825771
DEBUG:root:[ Iteration 240 ] Test loss: 0.0933612
DEBUG:root:[ Iteration 243 ] Training loss: 0.0770338
DEBUG:root:[ Iteration 246 ] Training loss: 0.0967049
DEBUG:root:[ Iteration 249 ] Training loss: 0.0763203
DEBUG:root:[ Iteration 252 ] Training loss: 0.100426
DEBUG:root:[ Iteration 255 ] Training loss: 0.0878457
DEBUG:root:[ Iteration 258 ] Training loss: 0.085398
DEBUG:root:[ Iteration 260 ] Test loss: 0.0917435
DEBUG:root:[ Iteration 261 ] Training loss: 0.0963184
DEBUG:root:[ Iteration 264 ] Training loss: 0.0853424
DEBUG:root:[ Iteration 267 ] Training loss: 0.0873075
DEBUG:root:[ Iteration 270 ] Training loss: 0.105692
DEBUG:root:[ Iteration 273 ] Training loss: 0.0934893
DEBUG:root:[ Iteration 276 ] Training loss: 0.101669
DEBUG:root:[ Iteration 279 ] Training loss: 0.0957656
DEBUG:root:[ Iteration 280 ] Test loss: 0.0914243
DEBUG:root:[ Iteration 282 ] Training loss: 0.0874146
DEBUG:root:[ Iteration 285 ] Training loss: 0.0852114
DEBUG:root:[ Iteration 288 ] Training loss: 0.0902067
DEBUG:root:[ Iteration 291 ] Training loss: 0.0742364
DEBUG:root:[ Iteration 294 ] Training loss: 0.0978371
DEBUG:root:[ Iteration 297 ] Training loss: 0.0953097
DEBUG:root:[ Iteration 300 ] Training loss: 0.0721929
DEBUG:root:[ Iteration 300 ] Test loss: 0.0905916
DEBUG:root:[ Iteration 303 ] Training loss: 0.0875486
DEBUG:root:[ Iteration 306 ] Training loss: 0.093289
DEBUG:root:[ Iteration 309 ] Training loss: 0.0759501
DEBUG:root:[ Iteration 312 ] Training loss: 0.0911291
DEBUG:root:[ Iteration 315 ] Training loss: 0.0799277
DEBUG:root:[ Iteration 318 ] Training loss: 0.0846449
DEBUG:root:[ Iteration 320 ] Test loss: 0.0896681
DEBUG:root:[ Iteration 321 ] Training loss: 0.105043
DEBUG:root:[ Iteration 324 ] Training loss: 0.0845488
DEBUG:root:[ Iteration 327 ] Training loss: 0.096545
DEBUG:root:[ Iteration 330 ] Training loss: 0.0828573
DEBUG:root:[ Iteration 333 ] Training loss: 0.0967525
DEBUG:root:[ Iteration 336 ] Training loss: 0.0682163
DEBUG:root:[ Iteration 339 ] Training loss: 0.0844021
DEBUG:root:[ Iteration 340 ] Test loss: 0.0890553
DEBUG:root:[ Iteration 342 ] Training loss: 0.0748248
DEBUG:root:[ Iteration 345 ] Training loss: 0.0971247
DEBUG:root:[ Iteration 348 ] Training loss: 0.0829332
DEBUG:root:[ Iteration 351 ] Training loss: 0.0961045
DEBUG:root:[ Iteration 354 ] Training loss: 0.0805125
DEBUG:root:[ Iteration 357 ] Training loss: 0.0888349
DEBUG:root:[ Iteration 360 ] Training loss: 0.0855489
DEBUG:root:[ Iteration 360 ] Test loss: 0.088167
DEBUG:root:[ Iteration 363 ] Training loss: 0.0821129
DEBUG:root:[ Iteration 366 ] Training loss: 0.0876065
DEBUG:root:[ Iteration 369 ] Training loss: 0.0946883
DEBUG:root:[ Iteration 372 ] Training loss: 0.0861949
DEBUG:root:[ Iteration 375 ] Training loss: 0.0747334
DEBUG:root:[ Iteration 378 ] Training loss: 0.0908182
DEBUG:root:[ Iteration 380 ] Test loss: 0.0902299
DEBUG:root:[ Iteration 381 ] Training loss: 0.0856411
DEBUG:root:[ Iteration 384 ] Training loss: 0.0961682
DEBUG:root:[ Iteration 387 ] Training loss: 0.0784799
DEBUG:root:[ Iteration 390 ] Training loss: 0.0882843
DEBUG:root:[ Iteration 393 ] Training loss: 0.0817166
DEBUG:root:[ Iteration 396 ] Training loss: 0.094608
DEBUG:root:[ Iteration 399 ] Training loss: 0.0846519
DEBUG:root:Saving...
DEBUG:root:Saved model to /home/annal/Izzy/vision_amt/Net/tensor/net6/net6_02-15-2016_13h57m43s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.550097
DEBUG:root:[ Iteration 0 ] Test loss: 0.554209
DEBUG:root:[ Iteration 0 ] Training loss: 0.554746
DEBUG:root:[ Iteration 0 ] Test loss: 0.511749
DEBUG:root:[ Iteration 3 ] Training loss: 0.424506
DEBUG:root:[ Iteration 6 ] Training loss: 0.387756
DEBUG:root:[ Iteration 9 ] Training loss: 0.323816
DEBUG:root:[ Iteration 12 ] Training loss: 0.329896
DEBUG:root:[ Iteration 15 ] Training loss: 0.31825
DEBUG:root:[ Iteration 18 ] Training loss: 0.273847
DEBUG:root:[ Iteration 20 ] Test loss: 0.292394
DEBUG:root:[ Iteration 21 ] Training loss: 0.296242
DEBUG:root:[ Iteration 24 ] Training loss: 0.291828
DEBUG:root:[ Iteration 27 ] Training loss: 0.270209
DEBUG:root:[ Iteration 30 ] Training loss: 0.238
DEBUG:root:[ Iteration 33 ] Training loss: 0.251161
DEBUG:root:[ Iteration 36 ] Training loss: 0.221567
DEBUG:root:[ Iteration 39 ] Training loss: 0.255125
DEBUG:root:[ Iteration 40 ] Test loss: 0.234138
DEBUG:root:[ Iteration 42 ] Training loss: 0.254024
DEBUG:root:[ Iteration 45 ] Training loss: 0.220591
DEBUG:root:[ Iteration 48 ] Training loss: 0.230891
DEBUG:root:[ Iteration 51 ] Training loss: 0.189414
DEBUG:root:[ Iteration 54 ] Training loss: 0.242858
DEBUG:root:[ Iteration 57 ] Training loss: 0.175212
DEBUG:root:[ Iteration 60 ] Training loss: 0.197568
DEBUG:root:[ Iteration 60 ] Test loss: 0.186863
DEBUG:root:[ Iteration 63 ] Training loss: 0.193498
DEBUG:root:[ Iteration 66 ] Training loss: 0.16961
DEBUG:root:[ Iteration 69 ] Training loss: 0.167491
DEBUG:root:[ Iteration 72 ] Training loss: 0.158946
DEBUG:root:[ Iteration 75 ] Training loss: 0.151529
DEBUG:root:[ Iteration 78 ] Training loss: 0.155857
DEBUG:root:[ Iteration 80 ] Test loss: 0.153415
DEBUG:root:[ Iteration 81 ] Training loss: 0.145133
DEBUG:root:[ Iteration 84 ] Training loss: 0.181868
DEBUG:root:[ Iteration 87 ] Training loss: 0.15249
DEBUG:root:[ Iteration 90 ] Training loss: 0.158486
DEBUG:root:[ Iteration 93 ] Training loss: 0.155873
DEBUG:root:[ Iteration 96 ] Training loss: 0.131188
DEBUG:root:[ Iteration 99 ] Training loss: 0.137412
DEBUG:root:[ Iteration 100 ] Test loss: 0.139431
DEBUG:root:[ Iteration 102 ] Training loss: 0.162786
DEBUG:root:[ Iteration 105 ] Training loss: 0.147165
DEBUG:root:[ Iteration 108 ] Training loss: 0.131567
DEBUG:root:[ Iteration 111 ] Training loss: 0.155772
DEBUG:root:[ Iteration 114 ] Training loss: 0.131626
DEBUG:root:[ Iteration 117 ] Training loss: 0.122157
DEBUG:root:[ Iteration 120 ] Training loss: 0.127891
DEBUG:root:[ Iteration 120 ] Test loss: 0.131365
DEBUG:root:[ Iteration 123 ] Training loss: 0.131743
DEBUG:root:[ Iteration 126 ] Training loss: 0.126641
DEBUG:root:[ Iteration 129 ] Training loss: 0.140945
DEBUG:root:[ Iteration 132 ] Training loss: 0.115023
DEBUG:root:[ Iteration 135 ] Training loss: 0.12856
DEBUG:root:[ Iteration 138 ] Training loss: 0.145268
DEBUG:root:[ Iteration 140 ] Test loss: 0.125732
DEBUG:root:[ Iteration 141 ] Training loss: 0.113449
DEBUG:root:[ Iteration 144 ] Training loss: 0.108863
DEBUG:root:[ Iteration 147 ] Training loss: 0.135433
DEBUG:root:[ Iteration 150 ] Training loss: 0.168044
DEBUG:root:[ Iteration 153 ] Training loss: 0.114587
DEBUG:root:[ Iteration 156 ] Training loss: 0.133125
DEBUG:root:[ Iteration 159 ] Training loss: 0.123201
DEBUG:root:[ Iteration 160 ] Test loss: 0.121394
DEBUG:root:[ Iteration 162 ] Training loss: 0.118945
DEBUG:root:[ Iteration 165 ] Training loss: 0.12782
DEBUG:root:[ Iteration 168 ] Training loss: 0.122252
DEBUG:root:[ Iteration 171 ] Training loss: 0.116479
DEBUG:root:[ Iteration 174 ] Training loss: 0.125393
DEBUG:root:[ Iteration 177 ] Training loss: 0.112016
DEBUG:root:[ Iteration 180 ] Training loss: 0.10617
DEBUG:root:[ Iteration 180 ] Test loss: 0.118048
DEBUG:root:[ Iteration 183 ] Training loss: 0.118524
DEBUG:root:[ Iteration 186 ] Training loss: 0.137606
DEBUG:root:[ Iteration 189 ] Training loss: 0.114333
DEBUG:root:[ Iteration 192 ] Training loss: 0.110767
DEBUG:root:[ Iteration 195 ] Training loss: 0.126068
DEBUG:root:[ Iteration 198 ] Training loss: 0.116438
DEBUG:root:[ Iteration 200 ] Test loss: 0.115562
DEBUG:root:[ Iteration 201 ] Training loss: 0.122343
DEBUG:root:[ Iteration 204 ] Training loss: 0.119509
DEBUG:root:[ Iteration 207 ] Training loss: 0.120092
DEBUG:root:[ Iteration 210 ] Training loss: 0.107711
DEBUG:root:[ Iteration 213 ] Training loss: 0.10826
DEBUG:root:[ Iteration 216 ] Training loss: 0.104613
DEBUG:root:[ Iteration 219 ] Training loss: 0.114076
DEBUG:root:[ Iteration 220 ] Test loss: 0.113629
DEBUG:root:[ Iteration 222 ] Training loss: 0.109015
DEBUG:root:[ Iteration 225 ] Training loss: 0.126791
DEBUG:root:[ Iteration 228 ] Training loss: 0.122608
DEBUG:root:[ Iteration 231 ] Training loss: 0.125567
DEBUG:root:[ Iteration 234 ] Training loss: 0.121067
DEBUG:root:[ Iteration 237 ] Training loss: 0.129687
DEBUG:root:[ Iteration 240 ] Training loss: 0.0909492
DEBUG:root:[ Iteration 240 ] Test loss: 0.112129
DEBUG:root:[ Iteration 243 ] Training loss: 0.0985657
DEBUG:root:[ Iteration 246 ] Training loss: 0.103045
DEBUG:root:[ Iteration 249 ] Training loss: 0.107786
DEBUG:root:[ Iteration 252 ] Training loss: 0.0896735
DEBUG:root:[ Iteration 255 ] Training loss: 0.104231
DEBUG:root:[ Iteration 258 ] Training loss: 0.13321
DEBUG:root:[ Iteration 260 ] Test loss: 0.110356
DEBUG:root:[ Iteration 261 ] Training loss: 0.110005
DEBUG:root:[ Iteration 264 ] Training loss: 0.101451
DEBUG:root:[ Iteration 267 ] Training loss: 0.132732
DEBUG:root:[ Iteration 270 ] Training loss: 0.098138
DEBUG:root:[ Iteration 273 ] Training loss: 0.1144
DEBUG:root:[ Iteration 276 ] Training loss: 0.107048
DEBUG:root:[ Iteration 279 ] Training loss: 0.117694
DEBUG:root:[ Iteration 280 ] Test loss: 0.108877
DEBUG:root:[ Iteration 282 ] Training loss: 0.113074
DEBUG:root:[ Iteration 285 ] Training loss: 0.118922
DEBUG:root:[ Iteration 288 ] Training loss: 0.10526
DEBUG:root:[ Iteration 291 ] Training loss: 0.100892
DEBUG:root:[ Iteration 294 ] Training loss: 0.0999168
DEBUG:root:[ Iteration 297 ] Training loss: 0.0976288
DEBUG:root:[ Iteration 300 ] Training loss: 0.109556
DEBUG:root:[ Iteration 300 ] Test loss: 0.107026
DEBUG:root:[ Iteration 303 ] Training loss: 0.113534
DEBUG:root:[ Iteration 306 ] Training loss: 0.120611
DEBUG:root:[ Iteration 309 ] Training loss: 0.106501
DEBUG:root:[ Iteration 312 ] Training loss: 0.0891278
DEBUG:root:[ Iteration 315 ] Training loss: 0.105335
DEBUG:root:[ Iteration 318 ] Training loss: 0.112364
DEBUG:root:[ Iteration 320 ] Test loss: 0.105672
DEBUG:root:[ Iteration 321 ] Training loss: 0.119368
DEBUG:root:[ Iteration 324 ] Training loss: 0.113955
DEBUG:root:[ Iteration 327 ] Training loss: 0.120967
DEBUG:root:[ Iteration 330 ] Training loss: 0.0907712
DEBUG:root:[ Iteration 333 ] Training loss: 0.133913
DEBUG:root:[ Iteration 336 ] Training loss: 0.109496
DEBUG:root:[ Iteration 339 ] Training loss: 0.126257
DEBUG:root:[ Iteration 340 ] Test loss: 0.104657
DEBUG:root:[ Iteration 342 ] Training loss: 0.100279
DEBUG:root:[ Iteration 345 ] Training loss: 0.120444
DEBUG:root:[ Iteration 348 ] Training loss: 0.101392
DEBUG:root:[ Iteration 351 ] Training loss: 0.108522
DEBUG:root:[ Iteration 354 ] Training loss: 0.0924464
DEBUG:root:[ Iteration 357 ] Training loss: 0.107393
DEBUG:root:[ Iteration 360 ] Training loss: 0.0851092
DEBUG:root:[ Iteration 360 ] Test loss: 0.103328
DEBUG:root:[ Iteration 363 ] Training loss: 0.111779
DEBUG:root:[ Iteration 366 ] Training loss: 0.115206
DEBUG:root:[ Iteration 369 ] Training loss: 0.106572
DEBUG:root:[ Iteration 372 ] Training loss: 0.104629
DEBUG:root:[ Iteration 375 ] Training loss: 0.0918817
DEBUG:root:[ Iteration 378 ] Training loss: 0.107151
DEBUG:root:[ Iteration 380 ] Test loss: 0.102173
DEBUG:root:[ Iteration 381 ] Training loss: 0.111764
DEBUG:root:[ Iteration 384 ] Training loss: 0.107121
DEBUG:root:[ Iteration 387 ] Training loss: 0.103272
DEBUG:root:[ Iteration 390 ] Training loss: 0.113833
DEBUG:root:[ Iteration 393 ] Training loss: 0.102884
DEBUG:root:[ Iteration 396 ] Training loss: 0.106313
DEBUG:root:[ Iteration 399 ] Training loss: 0.110862
DEBUG:root:Saving...
DEBUG:root:[ Iteration 0 ] Training loss: 0.782467
DEBUG:root:Saving...
DEBUG:root:[ Iteration 0 ] Training loss: 0.493393
DEBUG:root:Saving...
DEBUG:root:[ Iteration 0 ] Training loss: 0.392942
DEBUG:root:Saving...
DEBUG:root:[ Iteration 0 ] Training loss: 0.368055
DEBUG:root:[ Iteration 0 ] Test loss: 0.374361
DEBUG:root:[ Iteration 3 ] Training loss: 0.299851
DEBUG:root:[ Iteration 6 ] Training loss: 0.283034
DEBUG:root:[ Iteration 9 ] Training loss: 0.2368
DEBUG:root:[ Iteration 12 ] Training loss: 0.203583
DEBUG:root:[ Iteration 15 ] Training loss: 0.214776
DEBUG:root:[ Iteration 18 ] Training loss: 0.241186
DEBUG:root:[ Iteration 20 ] Test loss: 0.197347
DEBUG:root:[ Iteration 21 ] Training loss: 0.211921
DEBUG:root:[ Iteration 24 ] Training loss: 0.185994
DEBUG:root:[ Iteration 27 ] Training loss: 0.176041
DEBUG:root:[ Iteration 30 ] Training loss: 0.167019
DEBUG:root:[ Iteration 33 ] Training loss: 0.171738
DEBUG:root:[ Iteration 36 ] Training loss: 0.182356
DEBUG:root:[ Iteration 39 ] Training loss: 0.161762
DEBUG:root:[ Iteration 40 ] Test loss: 0.165909
DEBUG:root:[ Iteration 42 ] Training loss: 0.149759
DEBUG:root:[ Iteration 45 ] Training loss: 0.173379
DEBUG:root:[ Iteration 48 ] Training loss: 0.170151
DEBUG:root:[ Iteration 51 ] Training loss: 0.131553
DEBUG:root:[ Iteration 54 ] Training loss: 0.153842
DEBUG:root:[ Iteration 57 ] Training loss: 0.183421
DEBUG:root:[ Iteration 60 ] Training loss: 0.158391
DEBUG:root:[ Iteration 60 ] Test loss: 0.149622
DEBUG:root:[ Iteration 63 ] Training loss: 0.144953
DEBUG:root:[ Iteration 66 ] Training loss: 0.178592
DEBUG:root:[ Iteration 69 ] Training loss: 0.13595
DEBUG:root:[ Iteration 72 ] Training loss: 0.170829
DEBUG:root:[ Iteration 75 ] Training loss: 0.122804
DEBUG:root:[ Iteration 78 ] Training loss: 0.132796
DEBUG:root:[ Iteration 80 ] Test loss: 0.138669
DEBUG:root:[ Iteration 81 ] Training loss: 0.134224
DEBUG:root:[ Iteration 84 ] Training loss: 0.124933
DEBUG:root:[ Iteration 87 ] Training loss: 0.142349
DEBUG:root:[ Iteration 90 ] Training loss: 0.142061
DEBUG:root:[ Iteration 93 ] Training loss: 0.147586
DEBUG:root:[ Iteration 96 ] Training loss: 0.160352
DEBUG:root:[ Iteration 99 ] Training loss: 0.109807
DEBUG:root:[ Iteration 100 ] Test loss: 0.132532
DEBUG:root:[ Iteration 102 ] Training loss: 0.135066
DEBUG:root:[ Iteration 105 ] Training loss: 0.124143
DEBUG:root:[ Iteration 108 ] Training loss: 0.145954
DEBUG:root:[ Iteration 111 ] Training loss: 0.138219
DEBUG:root:[ Iteration 114 ] Training loss: 0.134378
DEBUG:root:[ Iteration 117 ] Training loss: 0.130145
DEBUG:root:[ Iteration 120 ] Training loss: 0.133931
DEBUG:root:[ Iteration 120 ] Test loss: 0.127933
DEBUG:root:[ Iteration 123 ] Training loss: 0.126589
DEBUG:root:[ Iteration 126 ] Training loss: 0.129318
DEBUG:root:[ Iteration 129 ] Training loss: 0.143607
DEBUG:root:[ Iteration 132 ] Training loss: 0.137866
DEBUG:root:[ Iteration 135 ] Training loss: 0.118384
DEBUG:root:[ Iteration 138 ] Training loss: 0.116068
DEBUG:root:[ Iteration 140 ] Test loss: 0.124795
DEBUG:root:[ Iteration 141 ] Training loss: 0.116611
DEBUG:root:[ Iteration 144 ] Training loss: 0.140037
DEBUG:root:[ Iteration 147 ] Training loss: 0.126125
DEBUG:root:[ Iteration 150 ] Training loss: 0.139743
DEBUG:root:[ Iteration 153 ] Training loss: 0.127323
DEBUG:root:[ Iteration 156 ] Training loss: 0.136807
DEBUG:root:[ Iteration 159 ] Training loss: 0.126928
DEBUG:root:[ Iteration 160 ] Test loss: 0.122008
DEBUG:root:[ Iteration 162 ] Training loss: 0.130153
DEBUG:root:[ Iteration 165 ] Training loss: 0.128111
DEBUG:root:[ Iteration 168 ] Training loss: 0.111445
DEBUG:root:[ Iteration 171 ] Training loss: 0.114143
DEBUG:root:[ Iteration 174 ] Training loss: 0.118711
DEBUG:root:[ Iteration 177 ] Training loss: 0.1083
DEBUG:root:[ Iteration 180 ] Training loss: 0.134683
DEBUG:root:[ Iteration 180 ] Test loss: 0.119698
DEBUG:root:[ Iteration 183 ] Training loss: 0.117128
DEBUG:root:[ Iteration 186 ] Training loss: 0.110475
DEBUG:root:[ Iteration 189 ] Training loss: 0.129371
DEBUG:root:[ Iteration 192 ] Training loss: 0.132173
DEBUG:root:[ Iteration 195 ] Training loss: 0.119931
DEBUG:root:[ Iteration 198 ] Training loss: 0.127324
DEBUG:root:[ Iteration 200 ] Test loss: 0.117501
DEBUG:root:[ Iteration 201 ] Training loss: 0.127791
DEBUG:root:[ Iteration 204 ] Training loss: 0.127906
DEBUG:root:[ Iteration 207 ] Training loss: 0.126511
DEBUG:root:[ Iteration 210 ] Training loss: 0.108262
DEBUG:root:[ Iteration 213 ] Training loss: 0.112165
DEBUG:root:[ Iteration 216 ] Training loss: 0.135863
DEBUG:root:[ Iteration 219 ] Training loss: 0.12604
DEBUG:root:[ Iteration 220 ] Test loss: 0.115844
DEBUG:root:[ Iteration 222 ] Training loss: 0.115136
DEBUG:root:[ Iteration 225 ] Training loss: 0.125564
DEBUG:root:[ Iteration 228 ] Training loss: 0.11048
DEBUG:root:[ Iteration 231 ] Training loss: 0.113594
DEBUG:root:[ Iteration 234 ] Training loss: 0.114725
DEBUG:root:[ Iteration 237 ] Training loss: 0.124205
DEBUG:root:[ Iteration 240 ] Training loss: 0.118653
DEBUG:root:[ Iteration 240 ] Test loss: 0.114293
DEBUG:root:[ Iteration 243 ] Training loss: 0.142068
DEBUG:root:[ Iteration 246 ] Training loss: 0.126469
DEBUG:root:[ Iteration 249 ] Training loss: 0.112115
DEBUG:root:[ Iteration 252 ] Training loss: 0.106958
DEBUG:root:[ Iteration 255 ] Training loss: 0.103003
DEBUG:root:[ Iteration 258 ] Training loss: 0.129351
DEBUG:root:[ Iteration 260 ] Test loss: 0.112861
DEBUG:root:[ Iteration 261 ] Training loss: 0.110045
DEBUG:root:[ Iteration 264 ] Training loss: 0.107647
DEBUG:root:[ Iteration 267 ] Training loss: 0.10072
DEBUG:root:[ Iteration 270 ] Training loss: 0.102026
DEBUG:root:[ Iteration 273 ] Training loss: 0.10993
DEBUG:root:[ Iteration 276 ] Training loss: 0.132456
DEBUG:root:[ Iteration 279 ] Training loss: 0.13295
DEBUG:root:[ Iteration 280 ] Test loss: 0.111612
DEBUG:root:[ Iteration 282 ] Training loss: 0.10762
DEBUG:root:[ Iteration 285 ] Training loss: 0.112614
DEBUG:root:[ Iteration 288 ] Training loss: 0.109306
DEBUG:root:[ Iteration 291 ] Training loss: 0.103718
DEBUG:root:[ Iteration 294 ] Training loss: 0.126713
DEBUG:root:[ Iteration 297 ] Training loss: 0.112731
DEBUG:root:[ Iteration 300 ] Training loss: 0.0903235
DEBUG:root:[ Iteration 300 ] Test loss: 0.110584
DEBUG:root:[ Iteration 303 ] Training loss: 0.0998811
DEBUG:root:[ Iteration 306 ] Training loss: 0.11586
DEBUG:root:[ Iteration 309 ] Training loss: 0.107625
DEBUG:root:[ Iteration 312 ] Training loss: 0.115157
DEBUG:root:[ Iteration 315 ] Training loss: 0.107903
DEBUG:root:[ Iteration 318 ] Training loss: 0.131771
DEBUG:root:[ Iteration 320 ] Test loss: 0.109613
DEBUG:root:[ Iteration 321 ] Training loss: 0.106273
DEBUG:root:[ Iteration 324 ] Training loss: 0.114277
DEBUG:root:[ Iteration 327 ] Training loss: 0.111927
DEBUG:root:[ Iteration 330 ] Training loss: 0.109446
DEBUG:root:[ Iteration 333 ] Training loss: 0.114279
DEBUG:root:[ Iteration 336 ] Training loss: 0.11157
DEBUG:root:[ Iteration 339 ] Training loss: 0.121416
DEBUG:root:[ Iteration 340 ] Test loss: 0.108476
DEBUG:root:[ Iteration 342 ] Training loss: 0.0924697
DEBUG:root:[ Iteration 345 ] Training loss: 0.0913877
DEBUG:root:[ Iteration 348 ] Training loss: 0.121698
DEBUG:root:[ Iteration 351 ] Training loss: 0.109612
DEBUG:root:[ Iteration 354 ] Training loss: 0.10403
DEBUG:root:[ Iteration 357 ] Training loss: 0.109658
DEBUG:root:[ Iteration 360 ] Training loss: 0.0907404
DEBUG:root:[ Iteration 360 ] Test loss: 0.107491
DEBUG:root:[ Iteration 363 ] Training loss: 0.0888771
DEBUG:root:[ Iteration 366 ] Training loss: 0.0997231
DEBUG:root:[ Iteration 369 ] Training loss: 0.113681
DEBUG:root:[ Iteration 372 ] Training loss: 0.109374
DEBUG:root:[ Iteration 375 ] Training loss: 0.116733
DEBUG:root:[ Iteration 378 ] Training loss: 0.104361
DEBUG:root:[ Iteration 380 ] Test loss: 0.106537
DEBUG:root:[ Iteration 381 ] Training loss: 0.11874
DEBUG:root:[ Iteration 384 ] Training loss: 0.107271
DEBUG:root:[ Iteration 387 ] Training loss: 0.106201
DEBUG:root:[ Iteration 390 ] Training loss: 0.114342
DEBUG:root:[ Iteration 393 ] Training loss: 0.103867
DEBUG:root:[ Iteration 396 ] Training loss: 0.0983829
DEBUG:root:[ Iteration 399 ] Training loss: 0.0996799
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-16-2016_19h41m11s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.177089
DEBUG:root:[ Iteration 0 ] Test loss: 0.177722
DEBUG:root:[ Iteration 3 ] Training loss: 0.175317
DEBUG:root:[ Iteration 6 ] Training loss: 0.174883
DEBUG:root:[ Iteration 9 ] Training loss: 0.181925
DEBUG:root:[ Iteration 12 ] Training loss: 0.174428
DEBUG:root:[ Iteration 15 ] Training loss: 0.174867
DEBUG:root:[ Iteration 18 ] Training loss: 0.175937
DEBUG:root:[ Iteration 20 ] Test loss: 0.17439
DEBUG:root:[ Iteration 21 ] Training loss: 0.166627
DEBUG:root:[ Iteration 24 ] Training loss: 0.175392
DEBUG:root:[ Iteration 27 ] Training loss: 0.170032
DEBUG:root:[ Iteration 30 ] Training loss: 0.170954
DEBUG:root:[ Iteration 33 ] Training loss: 0.165885
DEBUG:root:[ Iteration 36 ] Training loss: 0.146915
DEBUG:root:[ Iteration 39 ] Training loss: 0.168332
DEBUG:root:[ Iteration 40 ] Test loss: 0.164625
DEBUG:root:[ Iteration 42 ] Training loss: 0.163391
DEBUG:root:[ Iteration 45 ] Training loss: 0.176344
DEBUG:root:[ Iteration 48 ] Training loss: 0.142571
DEBUG:root:[ Iteration 51 ] Training loss: 0.153311
DEBUG:root:[ Iteration 54 ] Training loss: 0.150813
DEBUG:root:[ Iteration 57 ] Training loss: 0.171324
DEBUG:root:[ Iteration 60 ] Training loss: 0.151787
DEBUG:root:[ Iteration 60 ] Test loss: 0.157951
DEBUG:root:[ Iteration 63 ] Training loss: 0.170033
DEBUG:root:[ Iteration 66 ] Training loss: 0.160392
DEBUG:root:[ Iteration 69 ] Training loss: 0.157213
DEBUG:root:[ Iteration 72 ] Training loss: 0.148758
DEBUG:root:[ Iteration 75 ] Training loss: 0.138972
DEBUG:root:[ Iteration 78 ] Training loss: 0.122812
DEBUG:root:[ Iteration 80 ] Test loss: 0.134486
DEBUG:root:[ Iteration 81 ] Training loss: 0.138144
DEBUG:root:[ Iteration 84 ] Training loss: 0.122468
DEBUG:root:[ Iteration 87 ] Training loss: 0.116853
DEBUG:root:[ Iteration 90 ] Training loss: 0.110009
DEBUG:root:[ Iteration 93 ] Training loss: 0.120531
DEBUG:root:[ Iteration 96 ] Training loss: 0.114055
DEBUG:root:[ Iteration 99 ] Training loss: 0.091299
DEBUG:root:[ Iteration 100 ] Test loss: 0.104054
DEBUG:root:[ Iteration 102 ] Training loss: 0.125037
DEBUG:root:[ Iteration 105 ] Training loss: 0.102427
DEBUG:root:[ Iteration 108 ] Training loss: 0.0958901
DEBUG:root:[ Iteration 111 ] Training loss: 0.0971976
DEBUG:root:[ Iteration 114 ] Training loss: 0.0740192
DEBUG:root:[ Iteration 117 ] Training loss: 0.0809431
DEBUG:root:[ Iteration 120 ] Training loss: 0.0844483
DEBUG:root:[ Iteration 120 ] Test loss: 0.0856999
DEBUG:root:[ Iteration 123 ] Training loss: 0.112226
DEBUG:root:[ Iteration 126 ] Training loss: 0.0761397
DEBUG:root:[ Iteration 129 ] Training loss: 0.0981799
DEBUG:root:[ Iteration 132 ] Training loss: 0.0994272
DEBUG:root:[ Iteration 135 ] Training loss: 0.128563
DEBUG:root:[ Iteration 138 ] Training loss: 0.125849
DEBUG:root:[ Iteration 140 ] Test loss: 0.0691073
DEBUG:root:[ Iteration 141 ] Training loss: 0.114548
DEBUG:root:[ Iteration 144 ] Training loss: 0.0761157
DEBUG:root:[ Iteration 147 ] Training loss: 0.0613074
DEBUG:root:[ Iteration 150 ] Training loss: 0.0588922
DEBUG:root:[ Iteration 153 ] Training loss: 0.0441195
DEBUG:root:[ Iteration 156 ] Training loss: 0.0602422
DEBUG:root:[ Iteration 159 ] Training loss: 0.0463554
DEBUG:root:[ Iteration 160 ] Test loss: 0.0454509
DEBUG:root:[ Iteration 162 ] Training loss: 0.035719
DEBUG:root:[ Iteration 0 ] Training loss: 0.179253
DEBUG:root:[ Iteration 0 ] Test loss: 0.177674
DEBUG:root:[ Iteration 3 ] Training loss: 0.173906
DEBUG:root:[ Iteration 6 ] Training loss: 0.168486
DEBUG:root:[ Iteration 9 ] Training loss: 0.171525
DEBUG:root:[ Iteration 12 ] Training loss: 0.17174
DEBUG:root:[ Iteration 15 ] Training loss: 0.151099
DEBUG:root:[ Iteration 18 ] Training loss: 0.180729
DEBUG:root:[ Iteration 20 ] Test loss: 0.165054
DEBUG:root:[ Iteration 21 ] Training loss: 0.179265
DEBUG:root:[ Iteration 24 ] Training loss: 0.162472
DEBUG:root:[ Iteration 27 ] Training loss: 0.139873
DEBUG:root:[ Iteration 30 ] Training loss: 0.174868
DEBUG:root:[ Iteration 33 ] Training loss: 0.161328
DEBUG:root:[ Iteration 36 ] Training loss: 0.155765
DEBUG:root:[ Iteration 39 ] Training loss: 0.162852
DEBUG:root:[ Iteration 40 ] Test loss: 0.158429
DEBUG:root:[ Iteration 42 ] Training loss: 0.148446
DEBUG:root:[ Iteration 45 ] Training loss: 0.143057
DEBUG:root:[ Iteration 48 ] Training loss: 0.14914
DEBUG:root:[ Iteration 51 ] Training loss: 0.141827
DEBUG:root:[ Iteration 54 ] Training loss: 0.147648
DEBUG:root:[ Iteration 57 ] Training loss: 0.158685
DEBUG:root:[ Iteration 60 ] Training loss: 0.134273
DEBUG:root:[ Iteration 60 ] Test loss: 0.142922
DEBUG:root:[ Iteration 63 ] Training loss: 0.142203
DEBUG:root:[ Iteration 66 ] Training loss: 0.112381
DEBUG:root:[ Iteration 69 ] Training loss: 0.139856
DEBUG:root:[ Iteration 72 ] Training loss: 0.118007
DEBUG:root:[ Iteration 75 ] Training loss: 0.108631
DEBUG:root:[ Iteration 78 ] Training loss: 0.0927251
DEBUG:root:[ Iteration 80 ] Test loss: 0.109375
DEBUG:root:[ Iteration 81 ] Training loss: 0.0991114
DEBUG:root:[ Iteration 84 ] Training loss: 0.14365
DEBUG:root:[ Iteration 87 ] Training loss: 0.0913849
DEBUG:root:[ Iteration 90 ] Training loss: 0.0825824
DEBUG:root:[ Iteration 93 ] Training loss: 0.0825819
DEBUG:root:[ Iteration 96 ] Training loss: 0.148291
DEBUG:root:[ Iteration 99 ] Training loss: 0.0930897
DEBUG:root:[ Iteration 100 ] Test loss: 0.135983
DEBUG:root:[ Iteration 102 ] Training loss: 0.131685
DEBUG:root:[ Iteration 105 ] Training loss: 0.125981
DEBUG:root:[ Iteration 108 ] Training loss: 0.15523
DEBUG:root:[ Iteration 111 ] Training loss: 0.0952541
DEBUG:root:[ Iteration 114 ] Training loss: 0.106201
DEBUG:root:[ Iteration 117 ] Training loss: 0.0863263
DEBUG:root:[ Iteration 120 ] Training loss: 0.0898074
DEBUG:root:[ Iteration 120 ] Test loss: 0.0901525
DEBUG:root:[ Iteration 123 ] Training loss: 0.094326
DEBUG:root:[ Iteration 126 ] Training loss: 0.0719132
DEBUG:root:[ Iteration 129 ] Training loss: 0.0858944
DEBUG:root:[ Iteration 132 ] Training loss: 0.0620586
DEBUG:root:[ Iteration 135 ] Training loss: 0.0713717
DEBUG:root:[ Iteration 138 ] Training loss: 0.0529551
DEBUG:root:[ Iteration 140 ] Test loss: 0.0531836
DEBUG:root:[ Iteration 141 ] Training loss: 0.0537351
DEBUG:root:[ Iteration 144 ] Training loss: 0.0499397
DEBUG:root:[ Iteration 147 ] Training loss: 0.0518604
DEBUG:root:[ Iteration 150 ] Training loss: 0.044962
DEBUG:root:[ Iteration 153 ] Training loss: 0.0245395
DEBUG:root:[ Iteration 156 ] Training loss: 0.0302424
DEBUG:root:[ Iteration 159 ] Training loss: 0.0387722
DEBUG:root:[ Iteration 160 ] Test loss: 0.0425856
DEBUG:root:[ Iteration 162 ] Training loss: 0.0524353
DEBUG:root:[ Iteration 165 ] Training loss: 0.0243682
DEBUG:root:[ Iteration 168 ] Training loss: 0.038011
DEBUG:root:[ Iteration 171 ] Training loss: 0.0232485
DEBUG:root:[ Iteration 174 ] Training loss: 0.0416101
DEBUG:root:[ Iteration 177 ] Training loss: 0.0369877
DEBUG:root:[ Iteration 180 ] Training loss: 0.0308753
DEBUG:root:[ Iteration 180 ] Test loss: 0.032647
DEBUG:root:[ Iteration 183 ] Training loss: 0.019612
DEBUG:root:[ Iteration 186 ] Training loss: 0.0196081
DEBUG:root:[ Iteration 189 ] Training loss: 0.016866
DEBUG:root:[ Iteration 192 ] Training loss: 0.0178624
DEBUG:root:[ Iteration 195 ] Training loss: 0.0171349
DEBUG:root:[ Iteration 198 ] Training loss: 0.0212073
DEBUG:root:[ Iteration 200 ] Test loss: 0.0180364
DEBUG:root:[ Iteration 201 ] Training loss: 0.010624
DEBUG:root:[ Iteration 204 ] Training loss: 0.0137434
DEBUG:root:[ Iteration 207 ] Training loss: 0.0160218
DEBUG:root:[ Iteration 210 ] Training loss: 0.0273923
DEBUG:root:[ Iteration 213 ] Training loss: 0.0391198
DEBUG:root:[ Iteration 216 ] Training loss: 0.0144638
DEBUG:root:[ Iteration 219 ] Training loss: 0.0122979
DEBUG:root:[ Iteration 220 ] Test loss: 0.0252092
DEBUG:root:[ Iteration 222 ] Training loss: 0.022505
DEBUG:root:[ Iteration 225 ] Training loss: 0.0116073
DEBUG:root:[ Iteration 228 ] Training loss: 0.00713368
DEBUG:root:[ Iteration 231 ] Training loss: 0.0124542
DEBUG:root:[ Iteration 234 ] Training loss: 0.0161116
DEBUG:root:[ Iteration 237 ] Training loss: 0.0141556
DEBUG:root:[ Iteration 240 ] Training loss: 0.0164502
DEBUG:root:[ Iteration 240 ] Test loss: 0.0123608
DEBUG:root:[ Iteration 243 ] Training loss: 0.00834366
DEBUG:root:[ Iteration 246 ] Training loss: 0.00927003
DEBUG:root:[ Iteration 249 ] Training loss: 0.00664601
DEBUG:root:[ Iteration 252 ] Training loss: 0.0194808
DEBUG:root:[ Iteration 255 ] Training loss: 0.0229886
DEBUG:root:[ Iteration 258 ] Training loss: 0.0151018
DEBUG:root:[ Iteration 260 ] Test loss: 0.0124806
DEBUG:root:[ Iteration 261 ] Training loss: 0.0219714
DEBUG:root:[ Iteration 264 ] Training loss: 0.0103194
DEBUG:root:[ Iteration 267 ] Training loss: 0.00517238
DEBUG:root:[ Iteration 270 ] Training loss: 0.00711638
DEBUG:root:[ Iteration 273 ] Training loss: 0.0109172
DEBUG:root:[ Iteration 276 ] Training loss: 0.00595067
DEBUG:root:[ Iteration 279 ] Training loss: 0.0156021
DEBUG:root:[ Iteration 280 ] Test loss: 0.00969733
DEBUG:root:[ Iteration 282 ] Training loss: 0.0104904
DEBUG:root:[ Iteration 285 ] Training loss: 0.00460699
DEBUG:root:[ Iteration 288 ] Training loss: 0.0127759
DEBUG:root:[ Iteration 291 ] Training loss: 0.00716885
DEBUG:root:[ Iteration 294 ] Training loss: 0.00561453
DEBUG:root:[ Iteration 297 ] Training loss: 0.00911945
DEBUG:root:[ Iteration 300 ] Training loss: 0.0142047
DEBUG:root:[ Iteration 300 ] Test loss: 0.0139874
DEBUG:root:[ Iteration 303 ] Training loss: 0.0171992
DEBUG:root:[ Iteration 306 ] Training loss: 0.00917417
DEBUG:root:[ Iteration 309 ] Training loss: 0.0099433
DEBUG:root:[ Iteration 312 ] Training loss: 0.0211388
DEBUG:root:[ Iteration 315 ] Training loss: 0.00881802
DEBUG:root:[ Iteration 318 ] Training loss: 0.0068146
DEBUG:root:[ Iteration 320 ] Test loss: 0.00708752
DEBUG:root:[ Iteration 321 ] Training loss: 0.00611461
DEBUG:root:[ Iteration 324 ] Training loss: 0.00489514
DEBUG:root:[ Iteration 327 ] Training loss: 0.00628385
DEBUG:root:[ Iteration 330 ] Training loss: 0.002662
DEBUG:root:[ Iteration 333 ] Training loss: 0.0133023
DEBUG:root:[ Iteration 336 ] Training loss: 0.00853235
DEBUG:root:[ Iteration 339 ] Training loss: 0.0116266
DEBUG:root:[ Iteration 340 ] Test loss: 0.00600915
DEBUG:root:[ Iteration 342 ] Training loss: 0.00670713
DEBUG:root:[ Iteration 345 ] Training loss: 0.00908557
DEBUG:root:[ Iteration 348 ] Training loss: 0.00279492
DEBUG:root:[ Iteration 351 ] Training loss: 0.00493705
DEBUG:root:[ Iteration 354 ] Training loss: 0.00748738
DEBUG:root:[ Iteration 357 ] Training loss: 0.00649809
DEBUG:root:[ Iteration 360 ] Training loss: 0.00680387
DEBUG:root:[ Iteration 360 ] Test loss: 0.00761667
DEBUG:root:[ Iteration 363 ] Training loss: 0.00330974
DEBUG:root:[ Iteration 366 ] Training loss: 0.00518513
DEBUG:root:[ Iteration 369 ] Training loss: 0.0028376
DEBUG:root:[ Iteration 372 ] Training loss: 0.0044185
DEBUG:root:[ Iteration 375 ] Training loss: 0.00466131
DEBUG:root:[ Iteration 378 ] Training loss: 0.00679854
DEBUG:root:[ Iteration 380 ] Test loss: 0.00496548
DEBUG:root:[ Iteration 381 ] Training loss: 0.00602006
DEBUG:root:[ Iteration 384 ] Training loss: 0.00408747
DEBUG:root:[ Iteration 387 ] Training loss: 0.00885134
DEBUG:root:[ Iteration 390 ] Training loss: 0.00346721
DEBUG:root:[ Iteration 393 ] Training loss: 0.00710706
DEBUG:root:[ Iteration 396 ] Training loss: 0.00865164
DEBUG:root:[ Iteration 399 ] Training loss: 0.00510483
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-17-2016_22h57m55s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.215895
DEBUG:root:[ Iteration 0 ] Test loss: 0.213649
DEBUG:root:[ Iteration 3 ] Training loss: 0.223761
DEBUG:root:[ Iteration 6 ] Training loss: 0.204439
DEBUG:root:[ Iteration 9 ] Training loss: 0.208799
DEBUG:root:[ Iteration 12 ] Training loss: 0.200424
DEBUG:root:[ Iteration 15 ] Training loss: 0.187916
DEBUG:root:[ Iteration 18 ] Training loss: 0.139247
DEBUG:root:[ Iteration 20 ] Test loss: 0.11299
DEBUG:root:[ Iteration 21 ] Training loss: 0.0987103
DEBUG:root:[ Iteration 24 ] Training loss: 0.128874
DEBUG:root:[ Iteration 27 ] Training loss: 0.0973827
DEBUG:root:[ Iteration 30 ] Training loss: 0.0999535
DEBUG:root:[ Iteration 33 ] Training loss: 0.109133
DEBUG:root:[ Iteration 36 ] Training loss: 0.0749438
DEBUG:root:[ Iteration 39 ] Training loss: 0.0899713
DEBUG:root:[ Iteration 40 ] Test loss: 0.0806742
DEBUG:root:[ Iteration 42 ] Training loss: 0.0796085
DEBUG:root:[ Iteration 45 ] Training loss: 0.0611037
DEBUG:root:[ Iteration 48 ] Training loss: 0.0650142
DEBUG:root:[ Iteration 51 ] Training loss: 0.0521923
DEBUG:root:[ Iteration 54 ] Training loss: 0.0694878
DEBUG:root:[ Iteration 57 ] Training loss: 0.0507335
DEBUG:root:[ Iteration 60 ] Training loss: 0.0769297
DEBUG:root:[ Iteration 60 ] Test loss: 0.0606672
DEBUG:root:[ Iteration 63 ] Training loss: 0.0360718
DEBUG:root:[ Iteration 66 ] Training loss: 0.0477007
DEBUG:root:[ Iteration 69 ] Training loss: 0.0444414
DEBUG:root:[ Iteration 72 ] Training loss: 0.0397844
DEBUG:root:[ Iteration 75 ] Training loss: 0.0224528
DEBUG:root:[ Iteration 78 ] Training loss: 0.0289214
DEBUG:root:[ Iteration 80 ] Test loss: 0.0340413
DEBUG:root:[ Iteration 81 ] Training loss: 0.0617645
DEBUG:root:[ Iteration 84 ] Training loss: 0.0466045
DEBUG:root:[ Iteration 87 ] Training loss: 0.049593
DEBUG:root:[ Iteration 90 ] Training loss: 0.0249906
DEBUG:root:[ Iteration 93 ] Training loss: 0.0543634
DEBUG:root:[ Iteration 96 ] Training loss: 0.0396497
DEBUG:root:[ Iteration 99 ] Training loss: 0.0530419
DEBUG:root:[ Iteration 100 ] Test loss: 0.0329444
DEBUG:root:[ Iteration 102 ] Training loss: 0.039374
DEBUG:root:[ Iteration 105 ] Training loss: 0.0484268
DEBUG:root:[ Iteration 108 ] Training loss: 0.0304141
DEBUG:root:[ Iteration 111 ] Training loss: 0.0269509
DEBUG:root:[ Iteration 114 ] Training loss: 0.0226325
DEBUG:root:[ Iteration 117 ] Training loss: 0.0240017
DEBUG:root:[ Iteration 120 ] Training loss: 0.0268878
DEBUG:root:[ Iteration 120 ] Test loss: 0.0333793
DEBUG:root:[ Iteration 123 ] Training loss: 0.024642
DEBUG:root:[ Iteration 126 ] Training loss: 0.0293282
DEBUG:root:[ Iteration 129 ] Training loss: 0.0265243
DEBUG:root:[ Iteration 132 ] Training loss: 0.0265874
DEBUG:root:[ Iteration 135 ] Training loss: 0.0345731
DEBUG:root:[ Iteration 138 ] Training loss: 0.0225838
DEBUG:root:[ Iteration 140 ] Test loss: 0.0247356
DEBUG:root:[ Iteration 141 ] Training loss: 0.0292855
DEBUG:root:[ Iteration 144 ] Training loss: 0.0241309
DEBUG:root:[ Iteration 147 ] Training loss: 0.0152042
DEBUG:root:[ Iteration 150 ] Training loss: 0.0200231
DEBUG:root:[ Iteration 153 ] Training loss: 0.0196147
DEBUG:root:[ Iteration 156 ] Training loss: 0.0087884
DEBUG:root:[ Iteration 159 ] Training loss: 0.00949416
DEBUG:root:[ Iteration 160 ] Test loss: 0.00908623
DEBUG:root:[ Iteration 162 ] Training loss: 0.0052498
DEBUG:root:[ Iteration 165 ] Training loss: 0.00498916
DEBUG:root:[ Iteration 168 ] Training loss: 0.00477114
DEBUG:root:[ Iteration 171 ] Training loss: 0.00415666
DEBUG:root:[ Iteration 174 ] Training loss: 0.00590432
DEBUG:root:[ Iteration 177 ] Training loss: 0.00639946
DEBUG:root:[ Iteration 180 ] Training loss: 0.00655497
DEBUG:root:[ Iteration 180 ] Test loss: 0.00410481
DEBUG:root:[ Iteration 183 ] Training loss: 0.00332089
DEBUG:root:[ Iteration 186 ] Training loss: 0.00223103
DEBUG:root:[ Iteration 189 ] Training loss: 0.00268562
DEBUG:root:[ Iteration 192 ] Training loss: 0.00223175
DEBUG:root:[ Iteration 195 ] Training loss: 0.00645038
DEBUG:root:[ Iteration 198 ] Training loss: 0.00454739
DEBUG:root:[ Iteration 200 ] Test loss: 0.00343364
DEBUG:root:[ Iteration 201 ] Training loss: 0.00364102
DEBUG:root:[ Iteration 204 ] Training loss: 0.00639228
DEBUG:root:[ Iteration 207 ] Training loss: 0.00420373
DEBUG:root:[ Iteration 210 ] Training loss: 0.00444635
DEBUG:root:[ Iteration 213 ] Training loss: 0.00288407
DEBUG:root:[ Iteration 216 ] Training loss: 0.00251424
DEBUG:root:[ Iteration 219 ] Training loss: 0.00232858
DEBUG:root:[ Iteration 220 ] Test loss: 0.00306929
DEBUG:root:[ Iteration 222 ] Training loss: 0.00308212
DEBUG:root:[ Iteration 225 ] Training loss: 0.00192705
DEBUG:root:[ Iteration 228 ] Training loss: 0.00201433
DEBUG:root:[ Iteration 231 ] Training loss: 0.00200628
DEBUG:root:[ Iteration 234 ] Training loss: 0.00452636
DEBUG:root:[ Iteration 237 ] Training loss: 0.00301651
DEBUG:root:[ Iteration 240 ] Training loss: 0.00254505
DEBUG:root:[ Iteration 240 ] Test loss: 0.00301199
DEBUG:root:[ Iteration 243 ] Training loss: 0.00107019
DEBUG:root:[ Iteration 246 ] Training loss: 0.00190684
DEBUG:root:[ Iteration 249 ] Training loss: 0.00417954
DEBUG:root:[ Iteration 252 ] Training loss: 0.00126059
DEBUG:root:[ Iteration 255 ] Training loss: 0.00410017
DEBUG:root:[ Iteration 258 ] Training loss: 0.00318121
DEBUG:root:[ Iteration 260 ] Test loss: 0.00237943
DEBUG:root:[ Iteration 261 ] Training loss: 0.00213779
DEBUG:root:[ Iteration 264 ] Training loss: 0.00311899
DEBUG:root:[ Iteration 267 ] Training loss: 0.00223332
DEBUG:root:[ Iteration 270 ] Training loss: 0.00141457
DEBUG:root:[ Iteration 273 ] Training loss: 0.0016801
DEBUG:root:[ Iteration 276 ] Training loss: 0.00131498
DEBUG:root:[ Iteration 279 ] Training loss: 0.00276701
DEBUG:root:[ Iteration 280 ] Test loss: 0.00245636
DEBUG:root:[ Iteration 282 ] Training loss: 0.00174094
DEBUG:root:[ Iteration 285 ] Training loss: 0.000971835
DEBUG:root:[ Iteration 288 ] Training loss: 0.00202595
DEBUG:root:[ Iteration 291 ] Training loss: 0.00477893
DEBUG:root:[ Iteration 294 ] Training loss: 0.00241031
DEBUG:root:[ Iteration 297 ] Training loss: 0.00223351
DEBUG:root:[ Iteration 300 ] Training loss: 0.0030371
DEBUG:root:[ Iteration 300 ] Test loss: 0.00294849
DEBUG:root:[ Iteration 303 ] Training loss: 0.00113726
DEBUG:root:[ Iteration 306 ] Training loss: 0.0021658
DEBUG:root:[ Iteration 309 ] Training loss: 0.00185313
DEBUG:root:[ Iteration 312 ] Training loss: 0.0026568
DEBUG:root:[ Iteration 315 ] Training loss: 0.00148676
DEBUG:root:[ Iteration 318 ] Training loss: 0.00205532
DEBUG:root:[ Iteration 320 ] Test loss: 0.00229673
DEBUG:root:[ Iteration 321 ] Training loss: 0.000989535
DEBUG:root:[ Iteration 324 ] Training loss: 0.00317549
DEBUG:root:[ Iteration 327 ] Training loss: 0.00209943
DEBUG:root:[ Iteration 330 ] Training loss: 0.00246523
DEBUG:root:[ Iteration 333 ] Training loss: 0.00216725
DEBUG:root:[ Iteration 336 ] Training loss: 0.000873241
DEBUG:root:[ Iteration 339 ] Training loss: 0.00245459
DEBUG:root:[ Iteration 340 ] Test loss: 0.00248806
DEBUG:root:[ Iteration 342 ] Training loss: 0.00177517
DEBUG:root:[ Iteration 345 ] Training loss: 0.00157648
DEBUG:root:[ Iteration 348 ] Training loss: 0.00158626
DEBUG:root:[ Iteration 351 ] Training loss: 0.00118962
DEBUG:root:[ Iteration 354 ] Training loss: 0.00251952
DEBUG:root:[ Iteration 357 ] Training loss: 0.00185319
DEBUG:root:[ Iteration 360 ] Training loss: 0.00527505
DEBUG:root:[ Iteration 360 ] Test loss: 0.0036479
DEBUG:root:[ Iteration 363 ] Training loss: 0.00278503
DEBUG:root:[ Iteration 366 ] Training loss: 0.00458425
DEBUG:root:[ Iteration 369 ] Training loss: 0.00168011
DEBUG:root:[ Iteration 372 ] Training loss: 0.00367241
DEBUG:root:[ Iteration 375 ] Training loss: 0.00256628
DEBUG:root:[ Iteration 378 ] Training loss: 0.00317036
DEBUG:root:[ Iteration 380 ] Test loss: 0.00224388
DEBUG:root:[ Iteration 381 ] Training loss: 0.00174689
DEBUG:root:[ Iteration 384 ] Training loss: 0.000943309
DEBUG:root:[ Iteration 387 ] Training loss: 0.00273505
DEBUG:root:[ Iteration 390 ] Training loss: 0.00196643
DEBUG:root:[ Iteration 393 ] Training loss: 0.00122726
DEBUG:root:[ Iteration 396 ] Training loss: 0.00292557
DEBUG:root:[ Iteration 399 ] Training loss: 0.00102935
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-19-2016_17h14m04s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-19-2016_17h14m58s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0576365
DEBUG:root:[ Iteration 0 ] Test loss: 0.0472741
DEBUG:root:[ Iteration 3 ] Training loss: 0.0364056
DEBUG:root:[ Iteration 6 ] Training loss: 0.0429522
DEBUG:root:[ Iteration 9 ] Training loss: 0.0348002
DEBUG:root:[ Iteration 12 ] Training loss: 0.0384946
DEBUG:root:[ Iteration 15 ] Training loss: 0.0311575
DEBUG:root:[ Iteration 18 ] Training loss: 0.0394328
DEBUG:root:[ Iteration 20 ] Test loss: 0.0259902
DEBUG:root:[ Iteration 21 ] Training loss: 0.0294466
DEBUG:root:[ Iteration 24 ] Training loss: 0.0313237
DEBUG:root:[ Iteration 27 ] Training loss: 0.0356545
DEBUG:root:[ Iteration 30 ] Training loss: 0.0202175
DEBUG:root:[ Iteration 33 ] Training loss: 0.0302544
DEBUG:root:[ Iteration 36 ] Training loss: 0.0269293
DEBUG:root:[ Iteration 39 ] Training loss: 0.0157561
DEBUG:root:[ Iteration 40 ] Test loss: 0.0214102
DEBUG:root:[ Iteration 42 ] Training loss: 0.0215714
DEBUG:root:[ Iteration 45 ] Training loss: 0.0208625
DEBUG:root:[ Iteration 48 ] Training loss: 0.0238228
DEBUG:root:[ Iteration 51 ] Training loss: 0.0179663
DEBUG:root:[ Iteration 54 ] Training loss: 0.0156944
DEBUG:root:[ Iteration 57 ] Training loss: 0.0105545
DEBUG:root:[ Iteration 60 ] Training loss: 0.0129288
DEBUG:root:[ Iteration 60 ] Test loss: 0.0134741
DEBUG:root:[ Iteration 63 ] Training loss: 0.0152124
DEBUG:root:[ Iteration 66 ] Training loss: 0.0220556
DEBUG:root:[ Iteration 69 ] Training loss: 0.012788
DEBUG:root:[ Iteration 72 ] Training loss: 0.007716
DEBUG:root:[ Iteration 75 ] Training loss: 0.00777118
DEBUG:root:[ Iteration 78 ] Training loss: 0.0182865
DEBUG:root:[ Iteration 80 ] Test loss: 0.0133059
DEBUG:root:[ Iteration 81 ] Training loss: 0.0169941
DEBUG:root:[ Iteration 84 ] Training loss: 0.0168169
DEBUG:root:[ Iteration 87 ] Training loss: 0.0114406
DEBUG:root:[ Iteration 90 ] Training loss: 0.013995
DEBUG:root:[ Iteration 93 ] Training loss: 0.0108958
DEBUG:root:[ Iteration 96 ] Training loss: 0.0105465
DEBUG:root:[ Iteration 99 ] Training loss: 0.0146311
DEBUG:root:[ Iteration 100 ] Test loss: 0.00876461
DEBUG:root:[ Iteration 102 ] Training loss: 0.0109267
DEBUG:root:[ Iteration 105 ] Training loss: 0.0066018
DEBUG:root:[ Iteration 108 ] Training loss: 0.00651888
DEBUG:root:[ Iteration 111 ] Training loss: 0.0106437
DEBUG:root:[ Iteration 114 ] Training loss: 0.0094993
DEBUG:root:[ Iteration 117 ] Training loss: 0.00726592
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-23-2016_14h01m15s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0415127
DEBUG:root:[ Iteration 0 ] Test loss: 0.0472741
DEBUG:root:[ Iteration 3 ] Training loss: 0.0424677
DEBUG:root:[ Iteration 6 ] Training loss: 0.0459825
DEBUG:root:[ Iteration 9 ] Training loss: 0.0448419
DEBUG:root:[ Iteration 12 ] Training loss: 0.0373922
DEBUG:root:[ Iteration 15 ] Training loss: 0.0480012
DEBUG:root:[ Iteration 18 ] Training loss: 0.0243616
DEBUG:root:[ Iteration 20 ] Test loss: 0.0249452
DEBUG:root:[ Iteration 21 ] Training loss: 0.0276288
DEBUG:root:[ Iteration 24 ] Training loss: 0.0275296
DEBUG:root:[ Iteration 27 ] Training loss: 0.0327358
DEBUG:root:[ Iteration 30 ] Training loss: 0.0360626
DEBUG:root:[ Iteration 33 ] Training loss: 0.0216061
DEBUG:root:[ Iteration 36 ] Training loss: 0.01539
DEBUG:root:Saving...
DEBUG:root:[ Iteration 0 ] Training loss: 0.0823778
DEBUG:root:[ Iteration 0 ] Test loss: 0.0752351
DEBUG:root:[ Iteration 0 ] Training loss: 0.0748821
DEBUG:root:[ Iteration 0 ] Test loss: 0.0752351
DEBUG:root:[ Iteration 3 ] Training loss: 0.0653602
DEBUG:root:[ Iteration 6 ] Training loss: 0.0510621
DEBUG:root:[ Iteration 9 ] Training loss: 0.0465514
DEBUG:root:[ Iteration 12 ] Training loss: 0.0500631
DEBUG:root:[ Iteration 15 ] Training loss: 0.0478239
DEBUG:root:[ Iteration 18 ] Training loss: 0.0461302
DEBUG:root:[ Iteration 20 ] Test loss: 0.0592032
DEBUG:root:[ Iteration 21 ] Training loss: 0.0644894
DEBUG:root:[ Iteration 24 ] Training loss: 0.0616452
DEBUG:root:[ Iteration 27 ] Training loss: 0.0510025
DEBUG:root:[ Iteration 30 ] Training loss: 0.0457269
DEBUG:root:[ Iteration 33 ] Training loss: 0.0521721
DEBUG:root:[ Iteration 36 ] Training loss: 0.0541882
DEBUG:root:[ Iteration 39 ] Training loss: 0.0370161
DEBUG:root:[ Iteration 40 ] Test loss: 0.0488394
DEBUG:root:[ Iteration 42 ] Training loss: 0.0472937
DEBUG:root:[ Iteration 45 ] Training loss: 0.0445338
DEBUG:root:[ Iteration 48 ] Training loss: 0.0462064
DEBUG:root:[ Iteration 51 ] Training loss: 0.0497937
DEBUG:root:[ Iteration 54 ] Training loss: 0.0288592
DEBUG:root:[ Iteration 57 ] Training loss: 0.0326089
DEBUG:root:[ Iteration 60 ] Training loss: 0.0489368
DEBUG:root:[ Iteration 60 ] Test loss: 0.0392289
DEBUG:root:[ Iteration 63 ] Training loss: 0.0384207
DEBUG:root:[ Iteration 66 ] Training loss: 0.0501063
DEBUG:root:[ Iteration 69 ] Training loss: 0.0331612
DEBUG:root:[ Iteration 72 ] Training loss: 0.0468264
DEBUG:root:[ Iteration 75 ] Training loss: 0.0321133
DEBUG:root:[ Iteration 78 ] Training loss: 0.0398911
DEBUG:root:[ Iteration 80 ] Test loss: 0.03469
DEBUG:root:[ Iteration 81 ] Training loss: 0.0406555
DEBUG:root:[ Iteration 84 ] Training loss: 0.0288939
DEBUG:root:[ Iteration 87 ] Training loss: 0.035526
DEBUG:root:[ Iteration 90 ] Training loss: 0.0282673
DEBUG:root:[ Iteration 93 ] Training loss: 0.0289945
DEBUG:root:[ Iteration 96 ] Training loss: 0.0292415
DEBUG:root:[ Iteration 99 ] Training loss: 0.021814
DEBUG:root:[ Iteration 100 ] Test loss: 0.0316782
DEBUG:root:[ Iteration 102 ] Training loss: 0.0236215
DEBUG:root:[ Iteration 105 ] Training loss: 0.02823
DEBUG:root:[ Iteration 108 ] Training loss: 0.0215207
DEBUG:root:[ Iteration 111 ] Training loss: 0.0227473
DEBUG:root:[ Iteration 114 ] Training loss: 0.0306396
DEBUG:root:[ Iteration 117 ] Training loss: 0.0220301
DEBUG:root:[ Iteration 120 ] Training loss: 0.0208017
DEBUG:root:[ Iteration 120 ] Test loss: 0.0274536
DEBUG:root:[ Iteration 123 ] Training loss: 0.0315345
DEBUG:root:[ Iteration 126 ] Training loss: 0.0169195
DEBUG:root:[ Iteration 129 ] Training loss: 0.0259542
DEBUG:root:[ Iteration 132 ] Training loss: 0.0204052
DEBUG:root:[ Iteration 135 ] Training loss: 0.0216927
DEBUG:root:[ Iteration 138 ] Training loss: 0.0158997
DEBUG:root:[ Iteration 140 ] Test loss: 0.0252886
DEBUG:root:[ Iteration 141 ] Training loss: 0.0199065
DEBUG:root:[ Iteration 144 ] Training loss: 0.0211029
DEBUG:root:[ Iteration 147 ] Training loss: 0.018428
DEBUG:root:[ Iteration 150 ] Training loss: 0.0227751
DEBUG:root:[ Iteration 153 ] Training loss: 0.0306478
DEBUG:root:[ Iteration 156 ] Training loss: 0.0207089
DEBUG:root:[ Iteration 159 ] Training loss: 0.023526
DEBUG:root:[ Iteration 160 ] Test loss: 0.0354018
DEBUG:root:[ Iteration 162 ] Training loss: 0.0258943
DEBUG:root:[ Iteration 165 ] Training loss: 0.0223185
DEBUG:root:[ Iteration 168 ] Training loss: 0.0298461
DEBUG:root:[ Iteration 171 ] Training loss: 0.0262554
DEBUG:root:[ Iteration 174 ] Training loss: 0.0155176
DEBUG:root:[ Iteration 177 ] Training loss: 0.0176548
DEBUG:root:[ Iteration 180 ] Training loss: 0.0172961
DEBUG:root:[ Iteration 180 ] Test loss: 0.0218416
DEBUG:root:[ Iteration 183 ] Training loss: 0.0241199
DEBUG:root:[ Iteration 186 ] Training loss: 0.0228162
DEBUG:root:[ Iteration 189 ] Training loss: 0.0207572
DEBUG:root:[ Iteration 192 ] Training loss: 0.0184321
DEBUG:root:[ Iteration 195 ] Training loss: 0.0203825
DEBUG:root:[ Iteration 198 ] Training loss: 0.0212549
DEBUG:root:[ Iteration 200 ] Test loss: 0.0265561
DEBUG:root:[ Iteration 201 ] Training loss: 0.0164174
DEBUG:root:[ Iteration 204 ] Training loss: 0.011224
DEBUG:root:[ Iteration 207 ] Training loss: 0.0315895
DEBUG:root:[ Iteration 210 ] Training loss: 0.0246178
DEBUG:root:[ Iteration 213 ] Training loss: 0.0166434
DEBUG:root:[ Iteration 216 ] Training loss: 0.0186199
DEBUG:root:[ Iteration 219 ] Training loss: 0.0215084
DEBUG:root:[ Iteration 220 ] Test loss: 0.0209994
DEBUG:root:[ Iteration 222 ] Training loss: 0.0178549
DEBUG:root:[ Iteration 225 ] Training loss: 0.0173759
DEBUG:root:[ Iteration 228 ] Training loss: 0.0202035
DEBUG:root:[ Iteration 231 ] Training loss: 0.0151323
DEBUG:root:[ Iteration 234 ] Training loss: 0.0225858
DEBUG:root:[ Iteration 237 ] Training loss: 0.0192452
DEBUG:root:[ Iteration 240 ] Training loss: 0.0285488
DEBUG:root:[ Iteration 240 ] Test loss: 0.0197658
DEBUG:root:[ Iteration 243 ] Training loss: 0.0158902
DEBUG:root:[ Iteration 246 ] Training loss: 0.0165791
DEBUG:root:[ Iteration 249 ] Training loss: 0.0246317
DEBUG:root:[ Iteration 252 ] Training loss: 0.0129975
DEBUG:root:[ Iteration 255 ] Training loss: 0.0152503
DEBUG:root:[ Iteration 258 ] Training loss: 0.016121
DEBUG:root:[ Iteration 260 ] Test loss: 0.0182097
DEBUG:root:[ Iteration 261 ] Training loss: 0.011961
DEBUG:root:[ Iteration 264 ] Training loss: 0.011933
DEBUG:root:[ Iteration 267 ] Training loss: 0.0187717
DEBUG:root:[ Iteration 270 ] Training loss: 0.0166211
DEBUG:root:[ Iteration 273 ] Training loss: 0.0135242
DEBUG:root:[ Iteration 276 ] Training loss: 0.0128901
DEBUG:root:[ Iteration 279 ] Training loss: 0.0234409
DEBUG:root:[ Iteration 280 ] Test loss: 0.0182888
DEBUG:root:[ Iteration 282 ] Training loss: 0.0171342
DEBUG:root:[ Iteration 285 ] Training loss: 0.0276981
DEBUG:root:[ Iteration 288 ] Training loss: 0.0117832
DEBUG:root:[ Iteration 291 ] Training loss: 0.0133405
DEBUG:root:[ Iteration 294 ] Training loss: 0.00953062
DEBUG:root:[ Iteration 297 ] Training loss: 0.0129875
DEBUG:root:[ Iteration 300 ] Training loss: 0.0108809
DEBUG:root:[ Iteration 300 ] Test loss: 0.0163959
DEBUG:root:[ Iteration 303 ] Training loss: 0.01314
DEBUG:root:[ Iteration 306 ] Training loss: 0.0159343
DEBUG:root:[ Iteration 309 ] Training loss: 0.0209213
DEBUG:root:[ Iteration 312 ] Training loss: 0.0145901
DEBUG:root:[ Iteration 315 ] Training loss: 0.00841139
DEBUG:root:[ Iteration 318 ] Training loss: 0.0144617
DEBUG:root:[ Iteration 320 ] Test loss: 0.0163166
DEBUG:root:[ Iteration 321 ] Training loss: 0.0113616
DEBUG:root:[ Iteration 324 ] Training loss: 0.00791945
DEBUG:root:[ Iteration 327 ] Training loss: 0.011022
DEBUG:root:[ Iteration 330 ] Training loss: 0.0121487
DEBUG:root:[ Iteration 333 ] Training loss: 0.0176294
DEBUG:root:[ Iteration 336 ] Training loss: 0.0202101
DEBUG:root:[ Iteration 339 ] Training loss: 0.0155738
DEBUG:root:[ Iteration 340 ] Test loss: 0.0180627
DEBUG:root:[ Iteration 342 ] Training loss: 0.0159036
DEBUG:root:[ Iteration 345 ] Training loss: 0.0164605
DEBUG:root:[ Iteration 348 ] Training loss: 0.0142617
DEBUG:root:[ Iteration 351 ] Training loss: 0.0127349
DEBUG:root:[ Iteration 354 ] Training loss: 0.0138081
DEBUG:root:[ Iteration 357 ] Training loss: 0.0106124
DEBUG:root:[ Iteration 360 ] Training loss: 0.0187511
DEBUG:root:[ Iteration 360 ] Test loss: 0.019879
DEBUG:root:[ Iteration 363 ] Training loss: 0.0142712
DEBUG:root:[ Iteration 366 ] Training loss: 0.00605294
DEBUG:root:[ Iteration 369 ] Training loss: 0.0171657
DEBUG:root:[ Iteration 372 ] Training loss: 0.0133562
DEBUG:root:[ Iteration 375 ] Training loss: 0.0133369
DEBUG:root:[ Iteration 378 ] Training loss: 0.0149624
DEBUG:root:[ Iteration 380 ] Test loss: 0.0161527
DEBUG:root:[ Iteration 381 ] Training loss: 0.0172203
DEBUG:root:[ Iteration 384 ] Training loss: 0.0124234
DEBUG:root:[ Iteration 387 ] Training loss: 0.017269
DEBUG:root:[ Iteration 390 ] Training loss: 0.0286486
DEBUG:root:[ Iteration 393 ] Training loss: 0.0226384
DEBUG:root:[ Iteration 396 ] Training loss: 0.0272
DEBUG:root:[ Iteration 399 ] Training loss: 0.00787747
DEBUG:root:[ Iteration 400 ] Test loss: 0.0169283
DEBUG:root:[ Iteration 402 ] Training loss: 0.0157787
DEBUG:root:[ Iteration 405 ] Training loss: 0.0124462
DEBUG:root:[ Iteration 408 ] Training loss: 0.0106071
DEBUG:root:[ Iteration 411 ] Training loss: 0.0112202
DEBUG:root:[ Iteration 414 ] Training loss: 0.0106601
DEBUG:root:[ Iteration 417 ] Training loss: 0.00968719
DEBUG:root:[ Iteration 420 ] Training loss: 0.0142834
DEBUG:root:[ Iteration 420 ] Test loss: 0.0173014
DEBUG:root:[ Iteration 423 ] Training loss: 0.0160371
DEBUG:root:[ Iteration 426 ] Training loss: 0.00933747
DEBUG:root:[ Iteration 429 ] Training loss: 0.00853514
DEBUG:root:[ Iteration 432 ] Training loss: 0.0139618
DEBUG:root:[ Iteration 435 ] Training loss: 0.0196621
DEBUG:root:[ Iteration 438 ] Training loss: 0.0161758
DEBUG:root:[ Iteration 440 ] Test loss: 0.0153732
DEBUG:root:[ Iteration 441 ] Training loss: 0.0104472
DEBUG:root:[ Iteration 444 ] Training loss: 0.0158343
DEBUG:root:[ Iteration 447 ] Training loss: 0.0145038
DEBUG:root:[ Iteration 450 ] Training loss: 0.00997002
DEBUG:root:[ Iteration 453 ] Training loss: 0.00887621
DEBUG:root:[ Iteration 456 ] Training loss: 0.0181862
DEBUG:root:[ Iteration 459 ] Training loss: 0.00735311
DEBUG:root:[ Iteration 460 ] Test loss: 0.0153715
DEBUG:root:[ Iteration 462 ] Training loss: 0.0127389
DEBUG:root:[ Iteration 465 ] Training loss: 0.0112868
DEBUG:root:[ Iteration 468 ] Training loss: 0.0139718
DEBUG:root:[ Iteration 471 ] Training loss: 0.00925267
DEBUG:root:[ Iteration 474 ] Training loss: 0.0141067
DEBUG:root:[ Iteration 477 ] Training loss: 0.0120285
DEBUG:root:[ Iteration 480 ] Training loss: 0.0173633
DEBUG:root:[ Iteration 480 ] Test loss: 0.0151932
DEBUG:root:[ Iteration 483 ] Training loss: 0.0169961
DEBUG:root:[ Iteration 486 ] Training loss: 0.015204
DEBUG:root:[ Iteration 489 ] Training loss: 0.0119447
DEBUG:root:[ Iteration 492 ] Training loss: 0.0111342
DEBUG:root:[ Iteration 495 ] Training loss: 0.0111069
DEBUG:root:[ Iteration 498 ] Training loss: 0.00849621
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-24-2016_17h39m10s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.174274
DEBUG:root:[ Iteration 0 ] Test loss: 0.170562
DEBUG:root:[ Iteration 3 ] Training loss: 0.171618
DEBUG:root:[ Iteration 6 ] Training loss: 0.151781
DEBUG:root:[ Iteration 9 ] Training loss: 0.12241
DEBUG:root:[ Iteration 12 ] Training loss: 0.0946663
DEBUG:root:[ Iteration 15 ] Training loss: 0.0947232
DEBUG:root:[ Iteration 18 ] Training loss: 0.0923898
DEBUG:root:[ Iteration 20 ] Test loss: 0.0887604
DEBUG:root:[ Iteration 21 ] Training loss: 0.0865417
DEBUG:root:[ Iteration 24 ] Training loss: 0.0719821
DEBUG:root:[ Iteration 27 ] Training loss: 0.0934366
DEBUG:root:[ Iteration 30 ] Training loss: 0.076572
DEBUG:root:[ Iteration 33 ] Training loss: 0.0769517
DEBUG:root:[ Iteration 36 ] Training loss: 0.0769996
DEBUG:root:[ Iteration 39 ] Training loss: 0.0698944
DEBUG:root:[ Iteration 40 ] Test loss: 0.0696266
DEBUG:root:[ Iteration 42 ] Training loss: 0.0528811
DEBUG:root:[ Iteration 45 ] Training loss: 0.0601119
DEBUG:root:[ Iteration 48 ] Training loss: 0.0573042
DEBUG:root:[ Iteration 51 ] Training loss: 0.0397072
DEBUG:root:[ Iteration 54 ] Training loss: 0.0494439
DEBUG:root:[ Iteration 57 ] Training loss: 0.0477119
DEBUG:root:[ Iteration 60 ] Training loss: 0.0686447
DEBUG:root:[ Iteration 60 ] Test loss: 0.05839
DEBUG:root:[ Iteration 63 ] Training loss: 0.0373858
DEBUG:root:[ Iteration 66 ] Training loss: 0.0487076
DEBUG:root:[ Iteration 69 ] Training loss: 0.0550836
DEBUG:root:[ Iteration 72 ] Training loss: 0.0570544
DEBUG:root:[ Iteration 75 ] Training loss: 0.0485731
DEBUG:root:[ Iteration 78 ] Training loss: 0.0874394
DEBUG:root:[ Iteration 80 ] Test loss: 0.0688994
DEBUG:root:[ Iteration 81 ] Training loss: 0.0336366
DEBUG:root:[ Iteration 84 ] Training loss: 0.079582
DEBUG:root:[ Iteration 87 ] Training loss: 0.0899438
DEBUG:root:[ Iteration 90 ] Training loss: 0.0522394
DEBUG:root:[ Iteration 93 ] Training loss: 0.0402525
DEBUG:root:[ Iteration 96 ] Training loss: 0.0473457
DEBUG:root:[ Iteration 99 ] Training loss: 0.0363972
DEBUG:root:[ Iteration 100 ] Test loss: 0.0502894
DEBUG:root:[ Iteration 102 ] Training loss: 0.0323029
DEBUG:root:[ Iteration 105 ] Training loss: 0.0379156
DEBUG:root:[ Iteration 108 ] Training loss: 0.0383557
DEBUG:root:[ Iteration 111 ] Training loss: 0.0402224
DEBUG:root:[ Iteration 114 ] Training loss: 0.0305697
DEBUG:root:[ Iteration 117 ] Training loss: 0.0334616
DEBUG:root:[ Iteration 120 ] Training loss: 0.0276085
DEBUG:root:[ Iteration 120 ] Test loss: 0.0361526
DEBUG:root:[ Iteration 123 ] Training loss: 0.0326286
DEBUG:root:[ Iteration 126 ] Training loss: 0.0304666
DEBUG:root:[ Iteration 129 ] Training loss: 0.027405
DEBUG:root:[ Iteration 132 ] Training loss: 0.0366234
DEBUG:root:[ Iteration 135 ] Training loss: 0.0321515
DEBUG:root:[ Iteration 138 ] Training loss: 0.0323346
DEBUG:root:[ Iteration 140 ] Test loss: 0.0440407
DEBUG:root:[ Iteration 141 ] Training loss: 0.0369037
DEBUG:root:[ Iteration 144 ] Training loss: 0.0224599
DEBUG:root:[ Iteration 147 ] Training loss: 0.0300723
DEBUG:root:[ Iteration 150 ] Training loss: 0.030072
DEBUG:root:[ Iteration 153 ] Training loss: 0.0339091
DEBUG:root:[ Iteration 156 ] Training loss: 0.0329639
DEBUG:root:[ Iteration 159 ] Training loss: 0.0330386
DEBUG:root:[ Iteration 160 ] Test loss: 0.0332057
DEBUG:root:[ Iteration 162 ] Training loss: 0.0404916
DEBUG:root:[ Iteration 165 ] Training loss: 0.0384731
DEBUG:root:[ Iteration 168 ] Training loss: 0.039598
DEBUG:root:[ Iteration 171 ] Training loss: 0.0349528
DEBUG:root:[ Iteration 174 ] Training loss: 0.0242955
DEBUG:root:[ Iteration 177 ] Training loss: 0.0236499
DEBUG:root:[ Iteration 180 ] Training loss: 0.022413
DEBUG:root:[ Iteration 180 ] Test loss: 0.0327254
DEBUG:root:[ Iteration 183 ] Training loss: 0.0253885
DEBUG:root:[ Iteration 186 ] Training loss: 0.0332294
DEBUG:root:[ Iteration 189 ] Training loss: 0.019741
DEBUG:root:[ Iteration 192 ] Training loss: 0.0284097
DEBUG:root:[ Iteration 195 ] Training loss: 0.0216966
DEBUG:root:[ Iteration 198 ] Training loss: 0.0269713
DEBUG:root:[ Iteration 200 ] Test loss: 0.0347747
DEBUG:root:[ Iteration 201 ] Training loss: 0.0276304
DEBUG:root:[ Iteration 204 ] Training loss: 0.0258903
DEBUG:root:[ Iteration 207 ] Training loss: 0.030435
DEBUG:root:[ Iteration 210 ] Training loss: 0.0333804
DEBUG:root:[ Iteration 213 ] Training loss: 0.0279625
DEBUG:root:[ Iteration 216 ] Training loss: 0.0182767
DEBUG:root:[ Iteration 219 ] Training loss: 0.0225121
DEBUG:root:[ Iteration 220 ] Test loss: 0.0278513
DEBUG:root:[ Iteration 222 ] Training loss: 0.0236473
DEBUG:root:[ Iteration 225 ] Training loss: 0.023871
DEBUG:root:[ Iteration 228 ] Training loss: 0.0189577
DEBUG:root:[ Iteration 231 ] Training loss: 0.0276193
DEBUG:root:[ Iteration 234 ] Training loss: 0.0268544
DEBUG:root:[ Iteration 237 ] Training loss: 0.0297366
DEBUG:root:[ Iteration 240 ] Training loss: 0.0373025
DEBUG:root:[ Iteration 240 ] Test loss: 0.0350545
DEBUG:root:[ Iteration 243 ] Training loss: 0.0259223
DEBUG:root:[ Iteration 246 ] Training loss: 0.0364256
DEBUG:root:[ Iteration 249 ] Training loss: 0.0323737
DEBUG:root:[ Iteration 252 ] Training loss: 0.0284524
DEBUG:root:[ Iteration 255 ] Training loss: 0.0248917
DEBUG:root:[ Iteration 258 ] Training loss: 0.0203873
DEBUG:root:[ Iteration 260 ] Test loss: 0.0272412
DEBUG:root:[ Iteration 261 ] Training loss: 0.0208126
DEBUG:root:[ Iteration 264 ] Training loss: 0.0232433
DEBUG:root:[ Iteration 267 ] Training loss: 0.0216221
DEBUG:root:[ Iteration 270 ] Training loss: 0.0214318
DEBUG:root:[ Iteration 273 ] Training loss: 0.0282761
DEBUG:root:[ Iteration 276 ] Training loss: 0.0219717
DEBUG:root:[ Iteration 279 ] Training loss: 0.0290837
DEBUG:root:[ Iteration 280 ] Test loss: 0.0308255
DEBUG:root:[ Iteration 282 ] Training loss: 0.0239513
DEBUG:root:[ Iteration 285 ] Training loss: 0.0233331
DEBUG:root:[ Iteration 288 ] Training loss: 0.0238593
DEBUG:root:[ Iteration 291 ] Training loss: 0.0362824
DEBUG:root:[ Iteration 294 ] Training loss: 0.0203176
DEBUG:root:[ Iteration 297 ] Training loss: 0.0273846
DEBUG:root:[ Iteration 300 ] Training loss: 0.0202413
DEBUG:root:[ Iteration 300 ] Test loss: 0.0279436
DEBUG:root:[ Iteration 303 ] Training loss: 0.0248475
DEBUG:root:[ Iteration 306 ] Training loss: 0.0234268
DEBUG:root:[ Iteration 309 ] Training loss: 0.0103285
DEBUG:root:[ Iteration 312 ] Training loss: 0.0177641
DEBUG:root:[ Iteration 315 ] Training loss: 0.0140462
DEBUG:root:[ Iteration 318 ] Training loss: 0.0216503
DEBUG:root:[ Iteration 320 ] Test loss: 0.025688
DEBUG:root:[ Iteration 321 ] Training loss: 0.01424
DEBUG:root:[ Iteration 324 ] Training loss: 0.0228619
DEBUG:root:[ Iteration 327 ] Training loss: 0.0130078
DEBUG:root:[ Iteration 330 ] Training loss: 0.0167647
DEBUG:root:[ Iteration 333 ] Training loss: 0.0253466
DEBUG:root:[ Iteration 336 ] Training loss: 0.0180852
DEBUG:root:[ Iteration 339 ] Training loss: 0.0259322
DEBUG:root:[ Iteration 340 ] Test loss: 0.0289369
DEBUG:root:[ Iteration 342 ] Training loss: 0.0210136
DEBUG:root:[ Iteration 345 ] Training loss: 0.0201185
DEBUG:root:[ Iteration 348 ] Training loss: 0.0183612
DEBUG:root:[ Iteration 351 ] Training loss: 0.0215912
DEBUG:root:[ Iteration 354 ] Training loss: 0.0227264
DEBUG:root:[ Iteration 357 ] Training loss: 0.0189289
DEBUG:root:[ Iteration 360 ] Training loss: 0.0129209
DEBUG:root:[ Iteration 360 ] Test loss: 0.0281464
DEBUG:root:[ Iteration 363 ] Training loss: 0.0331992
DEBUG:root:[ Iteration 366 ] Training loss: 0.0249074
DEBUG:root:[ Iteration 369 ] Training loss: 0.0187955
DEBUG:root:[ Iteration 372 ] Training loss: 0.0196223
DEBUG:root:[ Iteration 375 ] Training loss: 0.0151572
DEBUG:root:[ Iteration 378 ] Training loss: 0.0116816
DEBUG:root:[ Iteration 380 ] Test loss: 0.0246411
DEBUG:root:[ Iteration 381 ] Training loss: 0.0317854
DEBUG:root:[ Iteration 384 ] Training loss: 0.0161772
DEBUG:root:[ Iteration 387 ] Training loss: 0.0167715
DEBUG:root:[ Iteration 390 ] Training loss: 0.0182406
DEBUG:root:[ Iteration 393 ] Training loss: 0.0195769
DEBUG:root:[ Iteration 396 ] Training loss: 0.0161256
DEBUG:root:[ Iteration 399 ] Training loss: 0.0142544
DEBUG:root:[ Iteration 400 ] Test loss: 0.0266123
DEBUG:root:[ Iteration 402 ] Training loss: 0.0241357
DEBUG:root:[ Iteration 405 ] Training loss: 0.0184931
DEBUG:root:[ Iteration 408 ] Training loss: 0.0181468
DEBUG:root:[ Iteration 411 ] Training loss: 0.0185008
DEBUG:root:[ Iteration 414 ] Training loss: 0.0175761
DEBUG:root:[ Iteration 417 ] Training loss: 0.0219457
DEBUG:root:[ Iteration 420 ] Training loss: 0.0170678
DEBUG:root:[ Iteration 420 ] Test loss: 0.0277218
DEBUG:root:[ Iteration 423 ] Training loss: 0.018971
DEBUG:root:[ Iteration 426 ] Training loss: 0.0187215
DEBUG:root:[ Iteration 429 ] Training loss: 0.0281542
DEBUG:root:[ Iteration 432 ] Training loss: 0.01464
DEBUG:root:[ Iteration 435 ] Training loss: 0.0193714
DEBUG:root:[ Iteration 438 ] Training loss: 0.0194604
DEBUG:root:[ Iteration 440 ] Test loss: 0.0236071
DEBUG:root:[ Iteration 441 ] Training loss: 0.0260746
DEBUG:root:[ Iteration 444 ] Training loss: 0.0163033
DEBUG:root:[ Iteration 447 ] Training loss: 0.0338373
DEBUG:root:[ Iteration 450 ] Training loss: 0.0150051
DEBUG:root:[ Iteration 453 ] Training loss: 0.0301117
DEBUG:root:[ Iteration 456 ] Training loss: 0.0186633
DEBUG:root:[ Iteration 459 ] Training loss: 0.0151948
DEBUG:root:[ Iteration 460 ] Test loss: 0.0235724
DEBUG:root:[ Iteration 462 ] Training loss: 0.0211693
DEBUG:root:[ Iteration 465 ] Training loss: 0.0170432
DEBUG:root:[ Iteration 468 ] Training loss: 0.0100854
DEBUG:root:[ Iteration 471 ] Training loss: 0.0149242
DEBUG:root:[ Iteration 474 ] Training loss: 0.022103
DEBUG:root:[ Iteration 477 ] Training loss: 0.0166315
DEBUG:root:[ Iteration 480 ] Training loss: 0.0184769
DEBUG:root:[ Iteration 480 ] Test loss: 0.0261946
DEBUG:root:[ Iteration 483 ] Training loss: 0.0198776
DEBUG:root:[ Iteration 486 ] Training loss: 0.0235039
DEBUG:root:[ Iteration 489 ] Training loss: 0.0260632
DEBUG:root:[ Iteration 492 ] Training loss: 0.023324
DEBUG:root:[ Iteration 495 ] Training loss: 0.0276869
DEBUG:root:[ Iteration 498 ] Training loss: 0.0168265
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-24-2016_18h50m20s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.119814
DEBUG:root:[ Iteration 0 ] Test loss: 0.118357
DEBUG:root:[ Iteration 3 ] Training loss: 0.0947763
DEBUG:root:[ Iteration 6 ] Training loss: 0.113399
DEBUG:root:[ Iteration 9 ] Training loss: 0.0765843
DEBUG:root:[ Iteration 12 ] Training loss: 0.108963
DEBUG:root:[ Iteration 15 ] Training loss: 0.0688915
DEBUG:root:[ Iteration 18 ] Training loss: 0.0655422
DEBUG:root:[ Iteration 20 ] Test loss: 0.071195
DEBUG:root:[ Iteration 21 ] Training loss: 0.087474
DEBUG:root:[ Iteration 24 ] Training loss: 0.0520066
DEBUG:root:[ Iteration 27 ] Training loss: 0.0722857
DEBUG:root:[ Iteration 30 ] Training loss: 0.0733398
DEBUG:root:[ Iteration 33 ] Training loss: 0.0692737
DEBUG:root:[ Iteration 36 ] Training loss: 0.0714153
DEBUG:root:[ Iteration 39 ] Training loss: 0.0684087
DEBUG:root:[ Iteration 40 ] Test loss: 0.0629337
DEBUG:root:[ Iteration 42 ] Training loss: 0.058686
DEBUG:root:[ Iteration 45 ] Training loss: 0.0632608
DEBUG:root:[ Iteration 48 ] Training loss: 0.0760002
DEBUG:root:[ Iteration 51 ] Training loss: 0.0505602
DEBUG:root:[ Iteration 54 ] Training loss: 0.0565315
DEBUG:root:[ Iteration 57 ] Training loss: 0.0413642
DEBUG:root:[ Iteration 60 ] Training loss: 0.0469931
DEBUG:root:[ Iteration 60 ] Test loss: 0.0532305
DEBUG:root:[ Iteration 63 ] Training loss: 0.0478742
DEBUG:root:[ Iteration 66 ] Training loss: 0.0514407
DEBUG:root:[ Iteration 69 ] Training loss: 0.0371774
DEBUG:root:[ Iteration 72 ] Training loss: 0.0542698
DEBUG:root:[ Iteration 75 ] Training loss: 0.035137
DEBUG:root:[ Iteration 78 ] Training loss: 0.0338141
DEBUG:root:[ Iteration 80 ] Test loss: 0.0441359
DEBUG:root:[ Iteration 81 ] Training loss: 0.0396234
DEBUG:root:[ Iteration 84 ] Training loss: 0.0485853
DEBUG:root:[ Iteration 87 ] Training loss: 0.029431
DEBUG:root:[ Iteration 90 ] Training loss: 0.033153
DEBUG:root:[ Iteration 93 ] Training loss: 0.0406436
DEBUG:root:[ Iteration 96 ] Training loss: 0.0346743
DEBUG:root:[ Iteration 99 ] Training loss: 0.0264163
DEBUG:root:[ Iteration 100 ] Test loss: 0.0360178
DEBUG:root:[ Iteration 102 ] Training loss: 0.0243513
DEBUG:root:[ Iteration 105 ] Training loss: 0.0420137
DEBUG:root:[ Iteration 108 ] Training loss: 0.0282819
DEBUG:root:[ Iteration 111 ] Training loss: 0.0244092
DEBUG:root:[ Iteration 114 ] Training loss: 0.0398222
DEBUG:root:[ Iteration 117 ] Training loss: 0.0356269
DEBUG:root:[ Iteration 120 ] Training loss: 0.0204919
DEBUG:root:[ Iteration 120 ] Test loss: 0.0322985
DEBUG:root:[ Iteration 123 ] Training loss: 0.0277434
DEBUG:root:[ Iteration 126 ] Training loss: 0.026336
DEBUG:root:[ Iteration 129 ] Training loss: 0.0374576
DEBUG:root:[ Iteration 132 ] Training loss: 0.0351512
DEBUG:root:[ Iteration 135 ] Training loss: 0.0313155
DEBUG:root:[ Iteration 138 ] Training loss: 0.0389186
DEBUG:root:[ Iteration 140 ] Test loss: 0.0426598
DEBUG:root:[ Iteration 141 ] Training loss: 0.03541
DEBUG:root:[ Iteration 144 ] Training loss: 0.0270449
DEBUG:root:[ Iteration 147 ] Training loss: 0.016968
DEBUG:root:[ Iteration 150 ] Training loss: 0.0286736
DEBUG:root:[ Iteration 153 ] Training loss: 0.0316624
DEBUG:root:[ Iteration 156 ] Training loss: 0.0298301
DEBUG:root:[ Iteration 159 ] Training loss: 0.0269027
DEBUG:root:[ Iteration 160 ] Test loss: 0.0296838
DEBUG:root:[ Iteration 162 ] Training loss: 0.0308841
DEBUG:root:[ Iteration 165 ] Training loss: 0.0320021
DEBUG:root:[ Iteration 168 ] Training loss: 0.0337779
DEBUG:root:[ Iteration 171 ] Training loss: 0.0277596
DEBUG:root:[ Iteration 174 ] Training loss: 0.0201762
DEBUG:root:[ Iteration 177 ] Training loss: 0.0219525
DEBUG:root:[ Iteration 180 ] Training loss: 0.0187352
DEBUG:root:[ Iteration 180 ] Test loss: 0.0261686
DEBUG:root:[ Iteration 183 ] Training loss: 0.0208478
DEBUG:root:[ Iteration 186 ] Training loss: 0.0209424
DEBUG:root:[ Iteration 189 ] Training loss: 0.0284423
DEBUG:root:[ Iteration 192 ] Training loss: 0.0198035
DEBUG:root:[ Iteration 195 ] Training loss: 0.024058
DEBUG:root:[ Iteration 198 ] Training loss: 0.0177209
DEBUG:root:[ Iteration 200 ] Test loss: 0.0267042
DEBUG:root:[ Iteration 201 ] Training loss: 0.0219611
DEBUG:root:[ Iteration 204 ] Training loss: 0.0308881
DEBUG:root:[ Iteration 207 ] Training loss: 0.0139645
DEBUG:root:[ Iteration 210 ] Training loss: 0.0244761
DEBUG:root:[ Iteration 213 ] Training loss: 0.0187205
DEBUG:root:[ Iteration 216 ] Training loss: 0.0221726
DEBUG:root:[ Iteration 219 ] Training loss: 0.0244345
DEBUG:root:[ Iteration 220 ] Test loss: 0.0274548
DEBUG:root:[ Iteration 222 ] Training loss: 0.0225517
DEBUG:root:[ Iteration 225 ] Training loss: 0.013269
DEBUG:root:[ Iteration 228 ] Training loss: 0.0238381
DEBUG:root:[ Iteration 231 ] Training loss: 0.014852
DEBUG:root:[ Iteration 234 ] Training loss: 0.0138438
DEBUG:root:[ Iteration 237 ] Training loss: 0.0182389
DEBUG:root:[ Iteration 240 ] Training loss: 0.0251359
DEBUG:root:[ Iteration 240 ] Test loss: 0.0271638
DEBUG:root:[ Iteration 243 ] Training loss: 0.0183428
DEBUG:root:[ Iteration 246 ] Training loss: 0.0230611
DEBUG:root:[ Iteration 249 ] Training loss: 0.0149009
DEBUG:root:[ Iteration 252 ] Training loss: 0.0238181
DEBUG:root:[ Iteration 255 ] Training loss: 0.0188396
DEBUG:root:[ Iteration 258 ] Training loss: 0.0148005
DEBUG:root:[ Iteration 260 ] Test loss: 0.0268198
DEBUG:root:[ Iteration 261 ] Training loss: 0.019778
DEBUG:root:[ Iteration 264 ] Training loss: 0.0240855
DEBUG:root:[ Iteration 267 ] Training loss: 0.0172844
DEBUG:root:[ Iteration 270 ] Training loss: 0.0163827
DEBUG:root:[ Iteration 273 ] Training loss: 0.021146
DEBUG:root:[ Iteration 276 ] Training loss: 0.0202477
DEBUG:root:[ Iteration 279 ] Training loss: 0.0185723
DEBUG:root:[ Iteration 280 ] Test loss: 0.0245947
DEBUG:root:[ Iteration 282 ] Training loss: 0.0186209
DEBUG:root:[ Iteration 285 ] Training loss: 0.0202571
DEBUG:root:[ Iteration 288 ] Training loss: 0.0126512
DEBUG:root:[ Iteration 291 ] Training loss: 0.0166703
DEBUG:root:[ Iteration 294 ] Training loss: 0.0161561
DEBUG:root:[ Iteration 297 ] Training loss: 0.0130471
DEBUG:root:[ Iteration 300 ] Training loss: 0.0184178
DEBUG:root:[ Iteration 300 ] Test loss: 0.0222156
DEBUG:root:[ Iteration 303 ] Training loss: 0.0204427
DEBUG:root:[ Iteration 306 ] Training loss: 0.0213932
DEBUG:root:[ Iteration 309 ] Training loss: 0.0235982
DEBUG:root:[ Iteration 312 ] Training loss: 0.016697
DEBUG:root:[ Iteration 315 ] Training loss: 0.0165668
DEBUG:root:[ Iteration 318 ] Training loss: 0.0238602
DEBUG:root:[ Iteration 320 ] Test loss: 0.0239974
DEBUG:root:[ Iteration 321 ] Training loss: 0.0158609
DEBUG:root:[ Iteration 324 ] Training loss: 0.0178366
DEBUG:root:[ Iteration 327 ] Training loss: 0.0251699
DEBUG:root:[ Iteration 330 ] Training loss: 0.0138714
DEBUG:root:[ Iteration 333 ] Training loss: 0.0168719
DEBUG:root:[ Iteration 336 ] Training loss: 0.0200767
DEBUG:root:[ Iteration 339 ] Training loss: 0.0155395
DEBUG:root:[ Iteration 340 ] Test loss: 0.0225223
DEBUG:root:[ Iteration 342 ] Training loss: 0.0193373
DEBUG:root:[ Iteration 345 ] Training loss: 0.0162115
DEBUG:root:[ Iteration 348 ] Training loss: 0.0166552
DEBUG:root:[ Iteration 351 ] Training loss: 0.0173751
DEBUG:root:[ Iteration 354 ] Training loss: 0.0262223
DEBUG:root:[ Iteration 357 ] Training loss: 0.0170682
DEBUG:root:[ Iteration 360 ] Training loss: 0.0155515
DEBUG:root:[ Iteration 360 ] Test loss: 0.0227352
DEBUG:root:[ Iteration 363 ] Training loss: 0.0143634
DEBUG:root:[ Iteration 366 ] Training loss: 0.0130232
DEBUG:root:[ Iteration 369 ] Training loss: 0.0118128
DEBUG:root:[ Iteration 372 ] Training loss: 0.0199624
DEBUG:root:[ Iteration 375 ] Training loss: 0.0208249
DEBUG:root:[ Iteration 378 ] Training loss: 0.0227342
DEBUG:root:[ Iteration 380 ] Test loss: 0.0201131
DEBUG:root:[ Iteration 381 ] Training loss: 0.024042
DEBUG:root:[ Iteration 384 ] Training loss: 0.0215979
DEBUG:root:[ Iteration 387 ] Training loss: 0.01355
DEBUG:root:[ Iteration 390 ] Training loss: 0.0106741
DEBUG:root:[ Iteration 393 ] Training loss: 0.0170554
DEBUG:root:[ Iteration 396 ] Training loss: 0.023398
DEBUG:root:[ Iteration 399 ] Training loss: 0.0196398
DEBUG:root:[ Iteration 400 ] Test loss: 0.022887
DEBUG:root:[ Iteration 402 ] Training loss: 0.0154133
DEBUG:root:[ Iteration 405 ] Training loss: 0.0191079
DEBUG:root:[ Iteration 408 ] Training loss: 0.0218515
DEBUG:root:[ Iteration 411 ] Training loss: 0.0200064
DEBUG:root:[ Iteration 414 ] Training loss: 0.0145062
DEBUG:root:[ Iteration 417 ] Training loss: 0.0145421
DEBUG:root:[ Iteration 420 ] Training loss: 0.013434
DEBUG:root:[ Iteration 420 ] Test loss: 0.0196187
DEBUG:root:[ Iteration 423 ] Training loss: 0.0164056
DEBUG:root:[ Iteration 426 ] Training loss: 0.0112844
DEBUG:root:[ Iteration 429 ] Training loss: 0.0142239
DEBUG:root:[ Iteration 432 ] Training loss: 0.0183829
DEBUG:root:[ Iteration 435 ] Training loss: 0.017159
DEBUG:root:[ Iteration 438 ] Training loss: 0.0144555
DEBUG:root:[ Iteration 440 ] Test loss: 0.0191679
DEBUG:root:[ Iteration 441 ] Training loss: 0.0121491
DEBUG:root:[ Iteration 444 ] Training loss: 0.013875
DEBUG:root:[ Iteration 447 ] Training loss: 0.0155215
DEBUG:root:[ Iteration 450 ] Training loss: 0.0144938
DEBUG:root:[ Iteration 453 ] Training loss: 0.0174403
DEBUG:root:[ Iteration 456 ] Training loss: 0.0165538
DEBUG:root:[ Iteration 459 ] Training loss: 0.0171515
DEBUG:root:[ Iteration 460 ] Test loss: 0.0185818
DEBUG:root:[ Iteration 462 ] Training loss: 0.0151316
DEBUG:root:[ Iteration 465 ] Training loss: 0.0145094
DEBUG:root:[ Iteration 468 ] Training loss: 0.0134713
DEBUG:root:[ Iteration 471 ] Training loss: 0.0140205
DEBUG:root:[ Iteration 474 ] Training loss: 0.0072382
DEBUG:root:[ Iteration 477 ] Training loss: 0.0139255
DEBUG:root:[ Iteration 480 ] Training loss: 0.00741304
DEBUG:root:[ Iteration 480 ] Test loss: 0.0180883
DEBUG:root:[ Iteration 483 ] Training loss: 0.0202328
DEBUG:root:[ Iteration 486 ] Training loss: 0.0145213
DEBUG:root:[ Iteration 489 ] Training loss: 0.0187509
DEBUG:root:[ Iteration 492 ] Training loss: 0.0259067
DEBUG:root:[ Iteration 495 ] Training loss: 0.0190983
DEBUG:root:[ Iteration 498 ] Training loss: 0.013173
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-24-2016_19h32m42s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.111572
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-24-2016_20h18m17s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.190947
DEBUG:root:[ Iteration 0 ] Test loss: 0.186202
DEBUG:root:[ Iteration 3 ] Training loss: 0.187886
DEBUG:root:[ Iteration 6 ] Training loss: 0.179263
DEBUG:root:[ Iteration 9 ] Training loss: 0.158565
DEBUG:root:[ Iteration 12 ] Training loss: 0.131518
DEBUG:root:[ Iteration 15 ] Training loss: 0.13602
DEBUG:root:[ Iteration 18 ] Training loss: 0.16352
DEBUG:root:[ Iteration 20 ] Test loss: 0.149667
DEBUG:root:[ Iteration 21 ] Training loss: 0.150737
DEBUG:root:[ Iteration 24 ] Training loss: 0.153951
DEBUG:root:[ Iteration 27 ] Training loss: 0.129768
DEBUG:root:[ Iteration 30 ] Training loss: 0.135108
DEBUG:root:[ Iteration 33 ] Training loss: 0.142738
DEBUG:root:[ Iteration 36 ] Training loss: 0.12299
DEBUG:root:[ Iteration 39 ] Training loss: 0.130231
DEBUG:root:[ Iteration 40 ] Test loss: 0.126916
DEBUG:root:[ Iteration 42 ] Training loss: 0.121756
DEBUG:root:[ Iteration 45 ] Training loss: 0.134879
DEBUG:root:[ Iteration 48 ] Training loss: 0.121883
DEBUG:root:[ Iteration 51 ] Training loss: 0.10334
DEBUG:root:[ Iteration 54 ] Training loss: 0.1103
DEBUG:root:[ Iteration 57 ] Training loss: 0.112531
DEBUG:root:[ Iteration 60 ] Training loss: 0.114472
DEBUG:root:[ Iteration 60 ] Test loss: 0.10245
DEBUG:root:[ Iteration 63 ] Training loss: 0.0936162
DEBUG:root:[ Iteration 66 ] Training loss: 0.082879
DEBUG:root:[ Iteration 69 ] Training loss: 0.109671
DEBUG:root:[ Iteration 72 ] Training loss: 0.098685
DEBUG:root:[ Iteration 75 ] Training loss: 0.0810841
DEBUG:root:[ Iteration 78 ] Training loss: 0.0852487
DEBUG:root:[ Iteration 80 ] Test loss: 0.0850653
DEBUG:root:[ Iteration 81 ] Training loss: 0.0847289
DEBUG:root:[ Iteration 84 ] Training loss: 0.065714
DEBUG:root:[ Iteration 87 ] Training loss: 0.0805255
DEBUG:root:[ Iteration 90 ] Training loss: 0.0699199
DEBUG:root:[ Iteration 93 ] Training loss: 0.0689057
DEBUG:root:[ Iteration 96 ] Training loss: 0.072139
DEBUG:root:[ Iteration 99 ] Training loss: 0.0695292
DEBUG:root:[ Iteration 100 ] Test loss: 0.0705034
DEBUG:root:[ Iteration 102 ] Training loss: 0.0635397
DEBUG:root:[ Iteration 105 ] Training loss: 0.0517193
DEBUG:root:[ Iteration 108 ] Training loss: 0.0625582
DEBUG:root:[ Iteration 111 ] Training loss: 0.0545529
DEBUG:root:[ Iteration 114 ] Training loss: 0.0568451
DEBUG:root:[ Iteration 117 ] Training loss: 0.0504555
DEBUG:root:[ Iteration 120 ] Training loss: 0.0504076
DEBUG:root:[ Iteration 120 ] Test loss: 0.0568174
DEBUG:root:[ Iteration 123 ] Training loss: 0.0478315
DEBUG:root:[ Iteration 126 ] Training loss: 0.0776335
DEBUG:root:[ Iteration 129 ] Training loss: 0.061756
DEBUG:root:[ Iteration 132 ] Training loss: 0.0517977
DEBUG:root:[ Iteration 135 ] Training loss: 0.0711219
DEBUG:root:[ Iteration 138 ] Training loss: 0.0887291
DEBUG:root:[ Iteration 140 ] Test loss: 0.0707594
DEBUG:root:[ Iteration 141 ] Training loss: 0.0495594
DEBUG:root:[ Iteration 144 ] Training loss: 0.061663
DEBUG:root:[ Iteration 147 ] Training loss: 0.0481368
DEBUG:root:[ Iteration 150 ] Training loss: 0.0408209
DEBUG:root:[ Iteration 153 ] Training loss: 0.0482426
DEBUG:root:[ Iteration 156 ] Training loss: 0.0511302
DEBUG:root:[ Iteration 159 ] Training loss: 0.0460872
DEBUG:root:[ Iteration 160 ] Test loss: 0.0462488
DEBUG:root:[ Iteration 162 ] Training loss: 0.0607398
DEBUG:root:[ Iteration 165 ] Training loss: 0.0460934
DEBUG:root:[ Iteration 168 ] Training loss: 0.0393713
DEBUG:root:[ Iteration 171 ] Training loss: 0.0455376
DEBUG:root:[ Iteration 174 ] Training loss: 0.0439235
DEBUG:root:[ Iteration 177 ] Training loss: 0.0440873
DEBUG:root:[ Iteration 180 ] Training loss: 0.0328144
DEBUG:root:[ Iteration 180 ] Test loss: 0.0425659
DEBUG:root:[ Iteration 183 ] Training loss: 0.0335207
DEBUG:root:[ Iteration 186 ] Training loss: 0.031501
DEBUG:root:[ Iteration 189 ] Training loss: 0.0477511
DEBUG:root:[ Iteration 192 ] Training loss: 0.038192
DEBUG:root:[ Iteration 195 ] Training loss: 0.0326524
DEBUG:root:[ Iteration 198 ] Training loss: 0.0393278
DEBUG:root:[ Iteration 200 ] Test loss: 0.0466656
DEBUG:root:[ Iteration 201 ] Training loss: 0.0479858
DEBUG:root:[ Iteration 204 ] Training loss: 0.035069
DEBUG:root:[ Iteration 207 ] Training loss: 0.0283289
DEBUG:root:[ Iteration 210 ] Training loss: 0.0293892
DEBUG:root:[ Iteration 213 ] Training loss: 0.0566048
DEBUG:root:[ Iteration 216 ] Training loss: 0.0325843
DEBUG:root:[ Iteration 219 ] Training loss: 0.0335109
DEBUG:root:[ Iteration 220 ] Test loss: 0.0329071
DEBUG:root:[ Iteration 222 ] Training loss: 0.0399015
DEBUG:root:[ Iteration 225 ] Training loss: 0.0198455
DEBUG:root:[ Iteration 228 ] Training loss: 0.0174804
DEBUG:root:[ Iteration 231 ] Training loss: 0.0258716
DEBUG:root:[ Iteration 234 ] Training loss: 0.0323075
DEBUG:root:[ Iteration 237 ] Training loss: 0.0249865
DEBUG:root:[ Iteration 240 ] Training loss: 0.0309448
DEBUG:root:[ Iteration 240 ] Test loss: 0.0334155
DEBUG:root:[ Iteration 243 ] Training loss: 0.0218413
DEBUG:root:[ Iteration 246 ] Training loss: 0.0248323
DEBUG:root:[ Iteration 249 ] Training loss: 0.027907
DEBUG:root:[ Iteration 252 ] Training loss: 0.0240497
DEBUG:root:[ Iteration 255 ] Training loss: 0.0193944
DEBUG:root:[ Iteration 258 ] Training loss: 0.0303744
DEBUG:root:[ Iteration 260 ] Test loss: 0.0301941
DEBUG:root:[ Iteration 261 ] Training loss: 0.0329688
DEBUG:root:[ Iteration 264 ] Training loss: 0.0279536
DEBUG:root:[ Iteration 267 ] Training loss: 0.0209247
DEBUG:root:[ Iteration 270 ] Training loss: 0.0260057
DEBUG:root:[ Iteration 273 ] Training loss: 0.0287774
DEBUG:root:[ Iteration 276 ] Training loss: 0.0274412
DEBUG:root:[ Iteration 279 ] Training loss: 0.0293288
DEBUG:root:[ Iteration 280 ] Test loss: 0.0384475
DEBUG:root:[ Iteration 282 ] Training loss: 0.0338869
DEBUG:root:[ Iteration 285 ] Training loss: 0.0239147
DEBUG:root:[ Iteration 288 ] Training loss: 0.0315047
DEBUG:root:[ Iteration 291 ] Training loss: 0.0230068
DEBUG:root:[ Iteration 294 ] Training loss: 0.0334898
DEBUG:root:[ Iteration 297 ] Training loss: 0.0320999
DEBUG:root:[ Iteration 300 ] Training loss: 0.0294876
DEBUG:root:[ Iteration 300 ] Test loss: 0.0285717
DEBUG:root:[ Iteration 303 ] Training loss: 0.0309945
DEBUG:root:[ Iteration 306 ] Training loss: 0.0305083
DEBUG:root:[ Iteration 309 ] Training loss: 0.0322229
DEBUG:root:[ Iteration 312 ] Training loss: 0.0281424
DEBUG:root:[ Iteration 315 ] Training loss: 0.0262912
DEBUG:root:[ Iteration 318 ] Training loss: 0.0168687
DEBUG:root:[ Iteration 320 ] Test loss: 0.0307706
DEBUG:root:[ Iteration 321 ] Training loss: 0.0413713
DEBUG:root:[ Iteration 324 ] Training loss: 0.0220732
DEBUG:root:[ Iteration 327 ] Training loss: 0.0286857
DEBUG:root:[ Iteration 330 ] Training loss: 0.0441488
DEBUG:root:[ Iteration 333 ] Training loss: 0.0252076
DEBUG:root:[ Iteration 336 ] Training loss: 0.0268144
DEBUG:root:[ Iteration 339 ] Training loss: 0.0251045
DEBUG:root:[ Iteration 340 ] Test loss: 0.0277365
DEBUG:root:[ Iteration 342 ] Training loss: 0.0227613
DEBUG:root:[ Iteration 345 ] Training loss: 0.0233297
DEBUG:root:[ Iteration 348 ] Training loss: 0.0291934
DEBUG:root:[ Iteration 351 ] Training loss: 0.0259976
DEBUG:root:[ Iteration 354 ] Training loss: 0.0222444
DEBUG:root:[ Iteration 357 ] Training loss: 0.0255455
DEBUG:root:[ Iteration 360 ] Training loss: 0.0229468
DEBUG:root:[ Iteration 360 ] Test loss: 0.0258575
DEBUG:root:[ Iteration 363 ] Training loss: 0.0281362
DEBUG:root:[ Iteration 366 ] Training loss: 0.0283952
DEBUG:root:[ Iteration 369 ] Training loss: 0.0240724
DEBUG:root:[ Iteration 372 ] Training loss: 0.018041
DEBUG:root:[ Iteration 375 ] Training loss: 0.0203645
DEBUG:root:[ Iteration 378 ] Training loss: 0.0175349
DEBUG:root:[ Iteration 380 ] Test loss: 0.0266911
DEBUG:root:[ Iteration 381 ] Training loss: 0.0297456
DEBUG:root:[ Iteration 384 ] Training loss: 0.0298791
DEBUG:root:[ Iteration 387 ] Training loss: 0.0324732
DEBUG:root:[ Iteration 390 ] Training loss: 0.0316213
DEBUG:root:[ Iteration 393 ] Training loss: 0.0292636
DEBUG:root:[ Iteration 396 ] Training loss: 0.0252126
DEBUG:root:[ Iteration 399 ] Training loss: 0.0270371
DEBUG:root:[ Iteration 400 ] Test loss: 0.0322844
DEBUG:root:[ Iteration 402 ] Training loss: 0.0195823
DEBUG:root:[ Iteration 405 ] Training loss: 0.014286
DEBUG:root:[ Iteration 408 ] Training loss: 0.0271895
DEBUG:root:[ Iteration 411 ] Training loss: 0.039146
DEBUG:root:[ Iteration 414 ] Training loss: 0.0390347
DEBUG:root:[ Iteration 417 ] Training loss: 0.0310734
DEBUG:root:[ Iteration 420 ] Training loss: 0.0250333
DEBUG:root:[ Iteration 420 ] Test loss: 0.0302345
DEBUG:root:[ Iteration 423 ] Training loss: 0.0266005
DEBUG:root:[ Iteration 426 ] Training loss: 0.0207861
DEBUG:root:[ Iteration 429 ] Training loss: 0.0258284
DEBUG:root:[ Iteration 432 ] Training loss: 0.0322121
DEBUG:root:[ Iteration 435 ] Training loss: 0.0228818
DEBUG:root:[ Iteration 438 ] Training loss: 0.0408412
DEBUG:root:[ Iteration 440 ] Test loss: 0.0264063
DEBUG:root:[ Iteration 441 ] Training loss: 0.0260121
DEBUG:root:[ Iteration 444 ] Training loss: 0.0253173
DEBUG:root:[ Iteration 447 ] Training loss: 0.0209982
DEBUG:root:[ Iteration 450 ] Training loss: 0.0279963
DEBUG:root:[ Iteration 453 ] Training loss: 0.0246322
DEBUG:root:[ Iteration 456 ] Training loss: 0.0231875
DEBUG:root:[ Iteration 459 ] Training loss: 0.0161599
DEBUG:root:[ Iteration 460 ] Test loss: 0.0241774
DEBUG:root:[ Iteration 462 ] Training loss: 0.0206655
DEBUG:root:[ Iteration 465 ] Training loss: 0.0263788
DEBUG:root:[ Iteration 468 ] Training loss: 0.0242328
DEBUG:root:[ Iteration 471 ] Training loss: 0.0164805
DEBUG:root:[ Iteration 474 ] Training loss: 0.0226484
DEBUG:root:[ Iteration 477 ] Training loss: 0.0238253
DEBUG:root:[ Iteration 480 ] Training loss: 0.0180986
DEBUG:root:[ Iteration 480 ] Test loss: 0.0252592
DEBUG:root:[ Iteration 483 ] Training loss: 0.0191151
DEBUG:root:[ Iteration 486 ] Training loss: 0.015249
DEBUG:root:[ Iteration 489 ] Training loss: 0.0276087
DEBUG:root:[ Iteration 492 ] Training loss: 0.0200005
DEBUG:root:[ Iteration 495 ] Training loss: 0.0194663
DEBUG:root:[ Iteration 498 ] Training loss: 0.0153116
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-26-2016_17h40m20s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.190548
DEBUG:root:[ Iteration 0 ] Test loss: 0.195021
DEBUG:root:[ Iteration 0 ] Training loss: 0.19576
DEBUG:root:[ Iteration 0 ] Test loss: 0.192101
DEBUG:root:[ Iteration 3 ] Training loss: 0.188104
DEBUG:root:[ Iteration 6 ] Training loss: 0.18679
DEBUG:root:[ Iteration 9 ] Training loss: 0.159852
DEBUG:root:[ Iteration 12 ] Training loss: 0.103247
DEBUG:root:[ Iteration 15 ] Training loss: 0.103308
DEBUG:root:[ Iteration 18 ] Training loss: 0.0846903
DEBUG:root:[ Iteration 20 ] Test loss: 0.112413
DEBUG:root:[ Iteration 21 ] Training loss: 0.096644
DEBUG:root:[ Iteration 24 ] Training loss: 0.0826352
DEBUG:root:[ Iteration 27 ] Training loss: 0.111435
DEBUG:root:[ Iteration 30 ] Training loss: 0.0938661
DEBUG:root:[ Iteration 33 ] Training loss: 0.106176
DEBUG:root:[ Iteration 36 ] Training loss: 0.0905686
DEBUG:root:[ Iteration 39 ] Training loss: 0.0948699
DEBUG:root:[ Iteration 40 ] Test loss: 0.0958132
DEBUG:root:[ Iteration 42 ] Training loss: 0.0865473
DEBUG:root:[ Iteration 45 ] Training loss: 0.064206
DEBUG:root:[ Iteration 48 ] Training loss: 0.0721484
DEBUG:root:[ Iteration 51 ] Training loss: 0.0940173
DEBUG:root:[ Iteration 54 ] Training loss: 0.0612186
DEBUG:root:[ Iteration 57 ] Training loss: 0.124887
DEBUG:root:[ Iteration 60 ] Training loss: 0.102044
DEBUG:root:[ Iteration 60 ] Test loss: 0.0886006
DEBUG:root:[ Iteration 63 ] Training loss: 0.0816128
DEBUG:root:[ Iteration 66 ] Training loss: 0.0875887
DEBUG:root:[ Iteration 69 ] Training loss: 0.0914781
DEBUG:root:[ Iteration 72 ] Training loss: 0.0605695
DEBUG:root:[ Iteration 75 ] Training loss: 0.06496
DEBUG:root:[ Iteration 78 ] Training loss: 0.0709501
DEBUG:root:[ Iteration 80 ] Test loss: 0.0805985
DEBUG:root:[ Iteration 81 ] Training loss: 0.0658099
DEBUG:root:[ Iteration 84 ] Training loss: 0.0741077
DEBUG:root:[ Iteration 87 ] Training loss: 0.0560582
DEBUG:root:[ Iteration 90 ] Training loss: 0.072273
DEBUG:root:[ Iteration 93 ] Training loss: 0.0681604
DEBUG:root:[ Iteration 96 ] Training loss: 0.06562
DEBUG:root:[ Iteration 99 ] Training loss: 0.0570883
DEBUG:root:[ Iteration 100 ] Test loss: 0.0636696
DEBUG:root:[ Iteration 102 ] Training loss: 0.0611815
DEBUG:root:[ Iteration 105 ] Training loss: 0.076567
DEBUG:root:[ Iteration 108 ] Training loss: 0.0634574
DEBUG:root:[ Iteration 111 ] Training loss: 0.0356394
DEBUG:root:[ Iteration 114 ] Training loss: 0.0646989
DEBUG:root:[ Iteration 117 ] Training loss: 0.0576584
DEBUG:root:[ Iteration 120 ] Training loss: 0.0760773
DEBUG:root:[ Iteration 120 ] Test loss: 0.0544105
DEBUG:root:[ Iteration 123 ] Training loss: 0.0569043
DEBUG:root:[ Iteration 126 ] Training loss: 0.0439256
DEBUG:root:[ Iteration 129 ] Training loss: 0.0612912
DEBUG:root:[ Iteration 132 ] Training loss: 0.037147
DEBUG:root:[ Iteration 135 ] Training loss: 0.0473421
DEBUG:root:[ Iteration 138 ] Training loss: 0.0512117
DEBUG:root:[ Iteration 140 ] Test loss: 0.0486392
DEBUG:root:[ Iteration 141 ] Training loss: 0.0306653
DEBUG:root:[ Iteration 144 ] Training loss: 0.0496742
DEBUG:root:[ Iteration 147 ] Training loss: 0.0460915
DEBUG:root:[ Iteration 150 ] Training loss: 0.0351806
DEBUG:root:[ Iteration 153 ] Training loss: 0.0321042
DEBUG:root:[ Iteration 156 ] Training loss: 0.0443484
DEBUG:root:[ Iteration 159 ] Training loss: 0.0490867
DEBUG:root:[ Iteration 160 ] Test loss: 0.0426806
DEBUG:root:[ Iteration 162 ] Training loss: 0.0378146
DEBUG:root:[ Iteration 165 ] Training loss: 0.0318293
DEBUG:root:[ Iteration 168 ] Training loss: 0.0376107
DEBUG:root:[ Iteration 171 ] Training loss: 0.0358282
DEBUG:root:[ Iteration 174 ] Training loss: 0.0324386
DEBUG:root:[ Iteration 177 ] Training loss: 0.0343443
DEBUG:root:[ Iteration 180 ] Training loss: 0.0257152
DEBUG:root:[ Iteration 180 ] Test loss: 0.0314327
DEBUG:root:[ Iteration 183 ] Training loss: 0.0343348
DEBUG:root:[ Iteration 186 ] Training loss: 0.0231477
DEBUG:root:[ Iteration 189 ] Training loss: 0.0356343
DEBUG:root:[ Iteration 192 ] Training loss: 0.029457
DEBUG:root:[ Iteration 195 ] Training loss: 0.0257577
DEBUG:root:[ Iteration 198 ] Training loss: 0.0366239
DEBUG:root:[ Iteration 200 ] Test loss: 0.0257589
DEBUG:root:[ Iteration 201 ] Training loss: 0.0279998
DEBUG:root:[ Iteration 204 ] Training loss: 0.0372669
DEBUG:root:[ Iteration 207 ] Training loss: 0.0237437
DEBUG:root:[ Iteration 210 ] Training loss: 0.0236715
DEBUG:root:[ Iteration 213 ] Training loss: 0.0164766
DEBUG:root:[ Iteration 216 ] Training loss: 0.0173549
DEBUG:root:[ Iteration 219 ] Training loss: 0.0164954
DEBUG:root:[ Iteration 220 ] Test loss: 0.0190027
DEBUG:root:[ Iteration 222 ] Training loss: 0.0198089
DEBUG:root:[ Iteration 225 ] Training loss: 0.0230361
DEBUG:root:[ Iteration 228 ] Training loss: 0.0189079
DEBUG:root:[ Iteration 231 ] Training loss: 0.0183528
DEBUG:root:[ Iteration 234 ] Training loss: 0.0161142
DEBUG:root:[ Iteration 237 ] Training loss: 0.0240026
DEBUG:root:[ Iteration 240 ] Training loss: 0.0205594
DEBUG:root:[ Iteration 240 ] Test loss: 0.0220086
DEBUG:root:[ Iteration 243 ] Training loss: 0.0136576
DEBUG:root:[ Iteration 246 ] Training loss: 0.017722
DEBUG:root:[ Iteration 249 ] Training loss: 0.0149137
DEBUG:root:[ Iteration 252 ] Training loss: 0.0128067
DEBUG:root:[ Iteration 255 ] Training loss: 0.0247911
DEBUG:root:[ Iteration 258 ] Training loss: 0.0107948
DEBUG:root:[ Iteration 260 ] Test loss: 0.0123889
DEBUG:root:[ Iteration 261 ] Training loss: 0.00842133
DEBUG:root:[ Iteration 264 ] Training loss: 0.0125022
DEBUG:root:[ Iteration 267 ] Training loss: 0.0152746
DEBUG:root:[ Iteration 270 ] Training loss: 0.00896913
DEBUG:root:[ Iteration 273 ] Training loss: 0.00974462
DEBUG:root:[ Iteration 276 ] Training loss: 0.0130907
DEBUG:root:[ Iteration 279 ] Training loss: 0.0132084
DEBUG:root:[ Iteration 280 ] Test loss: 0.0113
DEBUG:root:[ Iteration 282 ] Training loss: 0.0103305
DEBUG:root:[ Iteration 285 ] Training loss: 0.00855718
DEBUG:root:[ Iteration 288 ] Training loss: 0.013516
DEBUG:root:[ Iteration 291 ] Training loss: 0.015288
DEBUG:root:[ Iteration 294 ] Training loss: 0.0105752
DEBUG:root:[ Iteration 297 ] Training loss: 0.0129885
DEBUG:root:[ Iteration 300 ] Training loss: 0.013554
DEBUG:root:[ Iteration 300 ] Test loss: 0.0135686
DEBUG:root:[ Iteration 303 ] Training loss: 0.0090275
DEBUG:root:[ Iteration 306 ] Training loss: 0.0202038
DEBUG:root:[ Iteration 309 ] Training loss: 0.0104383
DEBUG:root:[ Iteration 312 ] Training loss: 0.0208198
DEBUG:root:[ Iteration 315 ] Training loss: 0.0127038
DEBUG:root:[ Iteration 318 ] Training loss: 0.0105571
DEBUG:root:[ Iteration 320 ] Test loss: 0.00955541
DEBUG:root:[ Iteration 321 ] Training loss: 0.00871224
DEBUG:root:[ Iteration 324 ] Training loss: 0.00700008
DEBUG:root:[ Iteration 327 ] Training loss: 0.00481762
DEBUG:root:[ Iteration 330 ] Training loss: 0.00876188
DEBUG:root:[ Iteration 333 ] Training loss: 0.0156079
DEBUG:root:[ Iteration 336 ] Training loss: 0.00946309
DEBUG:root:[ Iteration 339 ] Training loss: 0.00971402
DEBUG:root:[ Iteration 340 ] Test loss: 0.00929439
DEBUG:root:[ Iteration 342 ] Training loss: 0.00811219
DEBUG:root:[ Iteration 345 ] Training loss: 0.0117513
DEBUG:root:[ Iteration 348 ] Training loss: 0.00559436
DEBUG:root:[ Iteration 351 ] Training loss: 0.00944735
DEBUG:root:[ Iteration 354 ] Training loss: 0.00506766
DEBUG:root:[ Iteration 357 ] Training loss: 0.00591955
DEBUG:root:[ Iteration 360 ] Training loss: 0.0173723
DEBUG:root:[ Iteration 360 ] Test loss: 0.00857606
DEBUG:root:[ Iteration 363 ] Training loss: 0.0148009
DEBUG:root:[ Iteration 366 ] Training loss: 0.0049979
DEBUG:root:[ Iteration 369 ] Training loss: 0.0116809
DEBUG:root:[ Iteration 372 ] Training loss: 0.00521547
DEBUG:root:[ Iteration 375 ] Training loss: 0.0159464
DEBUG:root:[ Iteration 378 ] Training loss: 0.0136461
DEBUG:root:[ Iteration 380 ] Test loss: 0.0106003
DEBUG:root:[ Iteration 381 ] Training loss: 0.00710027
DEBUG:root:[ Iteration 384 ] Training loss: 0.0099996
DEBUG:root:[ Iteration 387 ] Training loss: 0.00736942
DEBUG:root:[ Iteration 390 ] Training loss: 0.00891009
DEBUG:root:[ Iteration 393 ] Training loss: 0.00881186
DEBUG:root:[ Iteration 396 ] Training loss: 0.0114372
DEBUG:root:[ Iteration 399 ] Training loss: 0.00957413
DEBUG:root:[ Iteration 400 ] Test loss: 0.00975413
DEBUG:root:[ Iteration 402 ] Training loss: 0.00742517
DEBUG:root:[ Iteration 405 ] Training loss: 0.0108593
DEBUG:root:[ Iteration 408 ] Training loss: 0.00641898
DEBUG:root:[ Iteration 411 ] Training loss: 0.00516221
DEBUG:root:[ Iteration 414 ] Training loss: 0.012005
DEBUG:root:[ Iteration 417 ] Training loss: 0.00493316
DEBUG:root:[ Iteration 420 ] Training loss: 0.00722859
DEBUG:root:[ Iteration 420 ] Test loss: 0.00754176
DEBUG:root:[ Iteration 423 ] Training loss: 0.00736654
DEBUG:root:[ Iteration 426 ] Training loss: 0.00711169
DEBUG:root:[ Iteration 429 ] Training loss: 0.00410612
DEBUG:root:[ Iteration 432 ] Training loss: 0.00814848
DEBUG:root:[ Iteration 435 ] Training loss: 0.00623049
DEBUG:root:[ Iteration 438 ] Training loss: 0.0121639
DEBUG:root:[ Iteration 440 ] Test loss: 0.00813579
DEBUG:root:[ Iteration 441 ] Training loss: 0.00423852
DEBUG:root:[ Iteration 444 ] Training loss: 0.0104152
DEBUG:root:[ Iteration 447 ] Training loss: 0.0132024
DEBUG:root:[ Iteration 450 ] Training loss: 0.0074769
DEBUG:root:[ Iteration 453 ] Training loss: 0.0184661
DEBUG:root:[ Iteration 456 ] Training loss: 0.0147637
DEBUG:root:[ Iteration 459 ] Training loss: 0.00882288
DEBUG:root:[ Iteration 460 ] Test loss: 0.00729616
DEBUG:root:[ Iteration 462 ] Training loss: 0.00695077
DEBUG:root:[ Iteration 465 ] Training loss: 0.00695202
DEBUG:root:[ Iteration 468 ] Training loss: 0.00591762
DEBUG:root:[ Iteration 471 ] Training loss: 0.0113513
DEBUG:root:[ Iteration 474 ] Training loss: 0.00823832
DEBUG:root:[ Iteration 477 ] Training loss: 0.0150103
DEBUG:root:[ Iteration 480 ] Training loss: 0.00704745
DEBUG:root:[ Iteration 480 ] Test loss: 0.00927408
DEBUG:root:[ Iteration 483 ] Training loss: 0.00494169
DEBUG:root:[ Iteration 486 ] Training loss: 0.00849595
DEBUG:root:[ Iteration 489 ] Training loss: 0.00569708
DEBUG:root:[ Iteration 492 ] Training loss: 0.00447162
DEBUG:root:[ Iteration 495 ] Training loss: 0.00446653
DEBUG:root:[ Iteration 498 ] Training loss: 0.00762198
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-26-2016_17h58m15s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.218238
DEBUG:root:[ Iteration 0 ] Test loss: 0.205865
DEBUG:root:[ Iteration 3 ] Training loss: 0.183639
DEBUG:root:[ Iteration 6 ] Training loss: 0.158901
DEBUG:root:[ Iteration 9 ] Training loss: 0.123474
DEBUG:root:[ Iteration 12 ] Training loss: 0.136179
DEBUG:root:[ Iteration 15 ] Training loss: 0.173194
DEBUG:root:[ Iteration 18 ] Training loss: 0.201471
DEBUG:root:[ Iteration 20 ] Test loss: 0.184516
DEBUG:root:[ Iteration 21 ] Training loss: 0.184171
DEBUG:root:[ Iteration 24 ] Training loss: 0.201824
DEBUG:root:[ Iteration 27 ] Training loss: 0.228328
DEBUG:root:[ Iteration 30 ] Training loss: 0.176343
DEBUG:root:[ Iteration 33 ] Training loss: 0.211356
DEBUG:root:[ Iteration 36 ] Training loss: 0.224149
DEBUG:root:[ Iteration 39 ] Training loss: 0.239845
DEBUG:root:[ Iteration 40 ] Test loss: 0.17905
DEBUG:root:[ Iteration 42 ] Training loss: 0.190738
DEBUG:root:[ Iteration 45 ] Training loss: 0.173975
DEBUG:root:[ Iteration 48 ] Training loss: 0.143702
DEBUG:root:[ Iteration 51 ] Training loss: 0.113286
DEBUG:root:[ Iteration 54 ] Training loss: 0.138791
DEBUG:root:[ Iteration 57 ] Training loss: 0.11883
DEBUG:root:[ Iteration 60 ] Training loss: 0.103188
DEBUG:root:[ Iteration 60 ] Test loss: 0.11509
DEBUG:root:[ Iteration 63 ] Training loss: 0.116078
DEBUG:root:[ Iteration 66 ] Training loss: 0.105003
DEBUG:root:[ Iteration 69 ] Training loss: 0.104364
DEBUG:root:[ Iteration 72 ] Training loss: 0.115529
DEBUG:root:[ Iteration 75 ] Training loss: 0.102106
DEBUG:root:[ Iteration 78 ] Training loss: 0.108122
DEBUG:root:[ Iteration 80 ] Test loss: 0.104473
DEBUG:root:[ Iteration 81 ] Training loss: 0.108504
DEBUG:root:[ Iteration 84 ] Training loss: 0.110535
DEBUG:root:[ Iteration 87 ] Training loss: 0.104227
DEBUG:root:[ Iteration 90 ] Training loss: 0.104369
DEBUG:root:[ Iteration 93 ] Training loss: 0.100362
DEBUG:root:[ Iteration 96 ] Training loss: 0.0950214
DEBUG:root:[ Iteration 99 ] Training loss: 0.107021
DEBUG:root:[ Iteration 100 ] Test loss: 0.0964365
DEBUG:root:[ Iteration 102 ] Training loss: 0.0866241
DEBUG:root:[ Iteration 105 ] Training loss: 0.0961256
DEBUG:root:[ Iteration 108 ] Training loss: 0.0958751
DEBUG:root:[ Iteration 111 ] Training loss: 0.107073
DEBUG:root:[ Iteration 114 ] Training loss: 0.0919859
DEBUG:root:[ Iteration 117 ] Training loss: 0.0915296
DEBUG:root:[ Iteration 120 ] Training loss: 0.0956583
DEBUG:root:[ Iteration 120 ] Test loss: 0.0905343
DEBUG:root:[ Iteration 123 ] Training loss: 0.0882947
DEBUG:root:[ Iteration 126 ] Training loss: 0.0900151
DEBUG:root:[ Iteration 129 ] Training loss: 0.087271
DEBUG:root:[ Iteration 132 ] Training loss: 0.0855403
DEBUG:root:[ Iteration 135 ] Training loss: 0.101132
DEBUG:root:[ Iteration 138 ] Training loss: 0.0901149
DEBUG:root:[ Iteration 140 ] Test loss: 0.0893784
DEBUG:root:[ Iteration 141 ] Training loss: 0.0854613
DEBUG:root:[ Iteration 144 ] Training loss: 0.0938134
DEBUG:root:[ Iteration 147 ] Training loss: 0.0801969
DEBUG:root:[ Iteration 150 ] Training loss: 0.0925581
DEBUG:root:[ Iteration 153 ] Training loss: 0.0963181
DEBUG:root:[ Iteration 156 ] Training loss: 0.0877853
DEBUG:root:[ Iteration 159 ] Training loss: 0.0736176
DEBUG:root:[ Iteration 160 ] Test loss: 0.0827266
DEBUG:root:[ Iteration 162 ] Training loss: 0.0782004
DEBUG:root:[ Iteration 165 ] Training loss: 0.0805869
DEBUG:root:[ Iteration 168 ] Training loss: 0.0888722
DEBUG:root:[ Iteration 171 ] Training loss: 0.0874658
DEBUG:root:[ Iteration 174 ] Training loss: 0.0746857
DEBUG:root:[ Iteration 177 ] Training loss: 0.0777435
DEBUG:root:[ Iteration 180 ] Training loss: 0.0832885
DEBUG:root:[ Iteration 180 ] Test loss: 0.0820572
DEBUG:root:[ Iteration 183 ] Training loss: 0.0803089
DEBUG:root:[ Iteration 186 ] Training loss: 0.0833018
DEBUG:root:[ Iteration 189 ] Training loss: 0.0750754
DEBUG:root:[ Iteration 192 ] Training loss: 0.0687025
DEBUG:root:[ Iteration 195 ] Training loss: 0.0737234
DEBUG:root:[ Iteration 198 ] Training loss: 0.0716735
DEBUG:root:[ Iteration 200 ] Test loss: 0.0792545
DEBUG:root:[ Iteration 201 ] Training loss: 0.0723326
DEBUG:root:[ Iteration 204 ] Training loss: 0.0700112
DEBUG:root:[ Iteration 207 ] Training loss: 0.076948
DEBUG:root:[ Iteration 210 ] Training loss: 0.0740392
DEBUG:root:[ Iteration 213 ] Training loss: 0.0833089
DEBUG:root:[ Iteration 216 ] Training loss: 0.0765453
DEBUG:root:[ Iteration 219 ] Training loss: 0.0718576
DEBUG:root:[ Iteration 220 ] Test loss: 0.0743113
DEBUG:root:[ Iteration 222 ] Training loss: 0.0717209
DEBUG:root:[ Iteration 225 ] Training loss: 0.0727556
DEBUG:root:[ Iteration 228 ] Training loss: 0.0690018
DEBUG:root:[ Iteration 231 ] Training loss: 0.0725491
DEBUG:root:[ Iteration 234 ] Training loss: 0.0639409
DEBUG:root:[ Iteration 237 ] Training loss: 0.0687039
DEBUG:root:[ Iteration 240 ] Training loss: 0.0657248
DEBUG:root:[ Iteration 240 ] Test loss: 0.0714499
DEBUG:root:[ Iteration 243 ] Training loss: 0.0636643
DEBUG:root:[ Iteration 246 ] Training loss: 0.0722723
DEBUG:root:[ Iteration 249 ] Training loss: 0.0646555
DEBUG:root:[ Iteration 252 ] Training loss: 0.0711956
DEBUG:root:[ Iteration 255 ] Training loss: 0.0841549
DEBUG:root:[ Iteration 258 ] Training loss: 0.0618602
DEBUG:root:[ Iteration 260 ] Test loss: 0.0687979
DEBUG:root:[ Iteration 261 ] Training loss: 0.0579011
DEBUG:root:[ Iteration 264 ] Training loss: 0.0711861
DEBUG:root:[ Iteration 267 ] Training loss: 0.0724868
DEBUG:root:[ Iteration 270 ] Training loss: 0.0644908
DEBUG:root:[ Iteration 273 ] Training loss: 0.061488
DEBUG:root:[ Iteration 276 ] Training loss: 0.0620807
DEBUG:root:[ Iteration 279 ] Training loss: 0.0655611
DEBUG:root:[ Iteration 280 ] Test loss: 0.0853149
DEBUG:root:[ Iteration 282 ] Training loss: 0.0666506
DEBUG:root:[ Iteration 285 ] Training loss: 0.0681181
DEBUG:root:[ Iteration 288 ] Training loss: 0.0744049
DEBUG:root:[ Iteration 291 ] Training loss: 0.0671348
DEBUG:root:[ Iteration 294 ] Training loss: 0.0749754
DEBUG:root:[ Iteration 297 ] Training loss: 0.0730683
DEBUG:root:[ Iteration 300 ] Training loss: 0.0677045
DEBUG:root:[ Iteration 300 ] Test loss: 0.0698802
DEBUG:root:[ Iteration 303 ] Training loss: 0.0825507
DEBUG:root:[ Iteration 306 ] Training loss: 0.0599546
DEBUG:root:[ Iteration 309 ] Training loss: 0.0735831
DEBUG:root:[ Iteration 312 ] Training loss: 0.0748293
DEBUG:root:[ Iteration 315 ] Training loss: 0.0581509
DEBUG:root:[ Iteration 318 ] Training loss: 0.0524198
DEBUG:root:[ Iteration 320 ] Test loss: 0.0676614
DEBUG:root:[ Iteration 321 ] Training loss: 0.0464562
DEBUG:root:[ Iteration 324 ] Training loss: 0.068048
DEBUG:root:[ Iteration 327 ] Training loss: 0.0589941
DEBUG:root:[ Iteration 330 ] Training loss: 0.0606474
DEBUG:root:[ Iteration 333 ] Training loss: 0.057974
DEBUG:root:[ Iteration 336 ] Training loss: 0.0672888
DEBUG:root:[ Iteration 339 ] Training loss: 0.052068
DEBUG:root:[ Iteration 340 ] Test loss: 0.0673823
DEBUG:root:[ Iteration 342 ] Training loss: 0.0840344
DEBUG:root:[ Iteration 345 ] Training loss: 0.0979009
DEBUG:root:[ Iteration 348 ] Training loss: 0.0623229
DEBUG:root:[ Iteration 351 ] Training loss: 0.0855876
DEBUG:root:[ Iteration 354 ] Training loss: 0.0591988
DEBUG:root:[ Iteration 357 ] Training loss: 0.0729733
DEBUG:root:[ Iteration 360 ] Training loss: 0.0599284
DEBUG:root:[ Iteration 360 ] Test loss: 0.0649783
DEBUG:root:[ Iteration 363 ] Training loss: 0.0657911
DEBUG:root:[ Iteration 366 ] Training loss: 0.0535169
DEBUG:root:[ Iteration 369 ] Training loss: 0.0532502
DEBUG:root:[ Iteration 372 ] Training loss: 0.0660791
DEBUG:root:[ Iteration 375 ] Training loss: 0.0601448
DEBUG:root:[ Iteration 378 ] Training loss: 0.049976
DEBUG:root:[ Iteration 380 ] Test loss: 0.0623764
DEBUG:root:[ Iteration 381 ] Training loss: 0.0624852
DEBUG:root:[ Iteration 384 ] Training loss: 0.0589686
DEBUG:root:[ Iteration 387 ] Training loss: 0.0600493
DEBUG:root:[ Iteration 390 ] Training loss: 0.058676
DEBUG:root:[ Iteration 393 ] Training loss: 0.0601332
DEBUG:root:[ Iteration 396 ] Training loss: 0.0570414
DEBUG:root:[ Iteration 399 ] Training loss: 0.0660034
DEBUG:root:[ Iteration 400 ] Test loss: 0.0643381
DEBUG:root:[ Iteration 402 ] Training loss: 0.0457082
DEBUG:root:[ Iteration 405 ] Training loss: 0.0546875
DEBUG:root:[ Iteration 408 ] Training loss: 0.0534086
DEBUG:root:[ Iteration 411 ] Training loss: 0.0582568
DEBUG:root:[ Iteration 414 ] Training loss: 0.0489736
DEBUG:root:[ Iteration 417 ] Training loss: 0.0698385
DEBUG:root:[ Iteration 420 ] Training loss: 0.057398
DEBUG:root:[ Iteration 420 ] Test loss: 0.0625294
DEBUG:root:[ Iteration 423 ] Training loss: 0.0549724
DEBUG:root:[ Iteration 426 ] Training loss: 0.0535591
DEBUG:root:[ Iteration 429 ] Training loss: 0.0587769
DEBUG:root:[ Iteration 432 ] Training loss: 0.0544746
DEBUG:root:[ Iteration 435 ] Training loss: 0.0736531
DEBUG:root:[ Iteration 438 ] Training loss: 0.0455389
DEBUG:root:[ Iteration 440 ] Test loss: 0.0611639
DEBUG:root:[ Iteration 441 ] Training loss: 0.0522633
DEBUG:root:[ Iteration 444 ] Training loss: 0.049796
DEBUG:root:[ Iteration 447 ] Training loss: 0.0590012
DEBUG:root:[ Iteration 450 ] Training loss: 0.050787
DEBUG:root:[ Iteration 453 ] Training loss: 0.0590822
DEBUG:root:[ Iteration 456 ] Training loss: 0.0702862
DEBUG:root:[ Iteration 459 ] Training loss: 0.060695
DEBUG:root:[ Iteration 460 ] Test loss: 0.062451
DEBUG:root:[ Iteration 462 ] Training loss: 0.0632199
DEBUG:root:[ Iteration 465 ] Training loss: 0.0668376
DEBUG:root:[ Iteration 468 ] Training loss: 0.0468946
DEBUG:root:[ Iteration 471 ] Training loss: 0.0387231
DEBUG:root:[ Iteration 474 ] Training loss: 0.0637932
DEBUG:root:[ Iteration 477 ] Training loss: 0.0659008
DEBUG:root:[ Iteration 480 ] Training loss: 0.0598747
DEBUG:root:[ Iteration 480 ] Test loss: 0.0640702
DEBUG:root:[ Iteration 483 ] Training loss: 0.0592922
DEBUG:root:[ Iteration 486 ] Training loss: 0.0502446
DEBUG:root:[ Iteration 489 ] Training loss: 0.0544373
DEBUG:root:[ Iteration 492 ] Training loss: 0.0381614
DEBUG:root:[ Iteration 495 ] Training loss: 0.0661398
DEBUG:root:[ Iteration 498 ] Training loss: 0.056549
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-26-2016_18h08m10s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.057945
DEBUG:root:[ Iteration 0 ] Test loss: 0.0530236
DEBUG:root:[ Iteration 3 ] Training loss: 0.0493688
DEBUG:root:[ Iteration 6 ] Training loss: 0.0570179
DEBUG:root:[ Iteration 9 ] Training loss: 0.0471654
DEBUG:root:[ Iteration 12 ] Training loss: 0.045514
DEBUG:root:[ Iteration 15 ] Training loss: 0.0569786
DEBUG:root:[ Iteration 18 ] Training loss: 0.0433518
DEBUG:root:[ Iteration 20 ] Test loss: 0.0474707
DEBUG:root:[ Iteration 21 ] Training loss: 0.0497173
DEBUG:root:[ Iteration 24 ] Training loss: 0.0475623
DEBUG:root:[ Iteration 27 ] Training loss: 0.0476456
DEBUG:root:[ Iteration 30 ] Training loss: 0.0504171
DEBUG:root:[ Iteration 33 ] Training loss: 0.0387201
DEBUG:root:[ Iteration 36 ] Training loss: 0.0433622
DEBUG:root:[ Iteration 39 ] Training loss: 0.049119
DEBUG:root:[ Iteration 40 ] Test loss: 0.0448097
DEBUG:root:[ Iteration 42 ] Training loss: 0.0517803
DEBUG:root:[ Iteration 45 ] Training loss: 0.0494417
DEBUG:root:[ Iteration 48 ] Training loss: 0.0467292
DEBUG:root:[ Iteration 51 ] Training loss: 0.0501919
DEBUG:root:[ Iteration 54 ] Training loss: 0.047197
DEBUG:root:[ Iteration 57 ] Training loss: 0.0419563
DEBUG:root:[ Iteration 60 ] Training loss: 0.0432846
DEBUG:root:[ Iteration 60 ] Test loss: 0.0539209
DEBUG:root:[ Iteration 63 ] Training loss: 0.0485972
DEBUG:root:[ Iteration 66 ] Training loss: 0.0327416
DEBUG:root:[ Iteration 69 ] Training loss: 0.037404
DEBUG:root:[ Iteration 72 ] Training loss: 0.0453598
DEBUG:root:[ Iteration 75 ] Training loss: 0.0489787
DEBUG:root:[ Iteration 78 ] Training loss: 0.0504331
DEBUG:root:[ Iteration 80 ] Test loss: 0.0489395
DEBUG:root:[ Iteration 81 ] Training loss: 0.0423769
DEBUG:root:[ Iteration 84 ] Training loss: 0.0432569
DEBUG:root:[ Iteration 87 ] Training loss: 0.0490616
DEBUG:root:[ Iteration 90 ] Training loss: 0.0392112
DEBUG:root:[ Iteration 93 ] Training loss: 0.0372966
DEBUG:root:[ Iteration 96 ] Training loss: 0.0495445
DEBUG:root:[ Iteration 99 ] Training loss: 0.0542622
DEBUG:root:[ Iteration 100 ] Test loss: 0.0502335
DEBUG:root:[ Iteration 102 ] Training loss: 0.0481703
DEBUG:root:[ Iteration 105 ] Training loss: 0.0468384
DEBUG:root:[ Iteration 108 ] Training loss: 0.0418066
DEBUG:root:[ Iteration 111 ] Training loss: 0.0425955
DEBUG:root:[ Iteration 114 ] Training loss: 0.0433603
DEBUG:root:[ Iteration 117 ] Training loss: 0.0403315
DEBUG:root:[ Iteration 120 ] Training loss: 0.0443831
DEBUG:root:[ Iteration 120 ] Test loss: 0.046726
DEBUG:root:[ Iteration 123 ] Training loss: 0.0383685
DEBUG:root:[ Iteration 126 ] Training loss: 0.0461227
DEBUG:root:[ Iteration 129 ] Training loss: 0.0374709
DEBUG:root:[ Iteration 132 ] Training loss: 0.0411963
DEBUG:root:[ Iteration 135 ] Training loss: 0.04208
DEBUG:root:[ Iteration 138 ] Training loss: 0.0366633
DEBUG:root:[ Iteration 140 ] Test loss: 0.0442566
DEBUG:root:[ Iteration 141 ] Training loss: 0.0407295
DEBUG:root:[ Iteration 144 ] Training loss: 0.0478729
DEBUG:root:[ Iteration 147 ] Training loss: 0.0470406
DEBUG:root:[ Iteration 150 ] Training loss: 0.0441368
DEBUG:root:[ Iteration 153 ] Training loss: 0.0500464
DEBUG:root:[ Iteration 156 ] Training loss: 0.0468395
DEBUG:root:[ Iteration 159 ] Training loss: 0.0520083
DEBUG:root:[ Iteration 160 ] Test loss: 0.0492958
DEBUG:root:[ Iteration 162 ] Training loss: 0.0387972
DEBUG:root:[ Iteration 165 ] Training loss: 0.0422901
DEBUG:root:[ Iteration 168 ] Training loss: 0.0420113
DEBUG:root:[ Iteration 171 ] Training loss: 0.0413712
DEBUG:root:[ Iteration 174 ] Training loss: 0.0391526
DEBUG:root:[ Iteration 177 ] Training loss: 0.0363675
DEBUG:root:[ Iteration 180 ] Training loss: 0.0386038
DEBUG:root:[ Iteration 180 ] Test loss: 0.0475868
DEBUG:root:[ Iteration 183 ] Training loss: 0.0418258
DEBUG:root:[ Iteration 186 ] Training loss: 0.045012
DEBUG:root:[ Iteration 189 ] Training loss: 0.0396224
DEBUG:root:[ Iteration 192 ] Training loss: 0.037108
DEBUG:root:[ Iteration 195 ] Training loss: 0.0400002
DEBUG:root:[ Iteration 198 ] Training loss: 0.0354925
DEBUG:root:[ Iteration 200 ] Test loss: 0.0411585
DEBUG:root:[ Iteration 201 ] Training loss: 0.0451268
DEBUG:root:[ Iteration 204 ] Training loss: 0.0424582
DEBUG:root:[ Iteration 207 ] Training loss: 0.0441182
DEBUG:root:[ Iteration 210 ] Training loss: 0.0336311
DEBUG:root:[ Iteration 213 ] Training loss: 0.0361724
DEBUG:root:[ Iteration 216 ] Training loss: 0.0383481
DEBUG:root:[ Iteration 219 ] Training loss: 0.0411235
DEBUG:root:[ Iteration 220 ] Test loss: 0.0474915
DEBUG:root:[ Iteration 222 ] Training loss: 0.0344008
DEBUG:root:[ Iteration 225 ] Training loss: 0.0406147
DEBUG:root:[ Iteration 228 ] Training loss: 0.0400334
DEBUG:root:[ Iteration 231 ] Training loss: 0.0384074
DEBUG:root:[ Iteration 234 ] Training loss: 0.0414844
DEBUG:root:[ Iteration 237 ] Training loss: 0.0447592
DEBUG:root:[ Iteration 240 ] Training loss: 0.0375835
DEBUG:root:[ Iteration 240 ] Test loss: 0.0430445
DEBUG:root:[ Iteration 243 ] Training loss: 0.0398271
DEBUG:root:[ Iteration 246 ] Training loss: 0.036817
DEBUG:root:[ Iteration 249 ] Training loss: 0.0461012
DEBUG:root:[ Iteration 252 ] Training loss: 0.0336033
DEBUG:root:[ Iteration 255 ] Training loss: 0.0376835
DEBUG:root:[ Iteration 258 ] Training loss: 0.0349455
DEBUG:root:[ Iteration 260 ] Test loss: 0.0481755
DEBUG:root:[ Iteration 261 ] Training loss: 0.047733
DEBUG:root:[ Iteration 264 ] Training loss: 0.0457835
DEBUG:root:[ Iteration 267 ] Training loss: 0.0353313
DEBUG:root:[ Iteration 270 ] Training loss: 0.0465572
DEBUG:root:[ Iteration 273 ] Training loss: 0.0312078
DEBUG:root:[ Iteration 276 ] Training loss: 0.0404882
DEBUG:root:[ Iteration 279 ] Training loss: 0.0407236
DEBUG:root:[ Iteration 280 ] Test loss: 0.0505029
DEBUG:root:[ Iteration 282 ] Training loss: 0.0393242
DEBUG:root:[ Iteration 285 ] Training loss: 0.0358576
DEBUG:root:[ Iteration 288 ] Training loss: 0.0311618
DEBUG:root:[ Iteration 291 ] Training loss: 0.0397941
DEBUG:root:[ Iteration 294 ] Training loss: 0.0362948
DEBUG:root:[ Iteration 297 ] Training loss: 0.0328924
DEBUG:root:[ Iteration 300 ] Training loss: 0.0363339
DEBUG:root:[ Iteration 300 ] Test loss: 0.0401859
DEBUG:root:[ Iteration 303 ] Training loss: 0.0426071
DEBUG:root:[ Iteration 306 ] Training loss: 0.031353
DEBUG:root:[ Iteration 309 ] Training loss: 0.0321374
DEBUG:root:[ Iteration 312 ] Training loss: 0.0391816
DEBUG:root:[ Iteration 315 ] Training loss: 0.0349318
DEBUG:root:[ Iteration 318 ] Training loss: 0.0372892
DEBUG:root:[ Iteration 320 ] Test loss: 0.0370523
DEBUG:root:[ Iteration 321 ] Training loss: 0.0312773
DEBUG:root:[ Iteration 324 ] Training loss: 0.0425017
DEBUG:root:[ Iteration 327 ] Training loss: 0.037939
DEBUG:root:[ Iteration 330 ] Training loss: 0.0606413
DEBUG:root:[ Iteration 333 ] Training loss: 0.0438841
DEBUG:root:[ Iteration 336 ] Training loss: 0.0478014
DEBUG:root:[ Iteration 339 ] Training loss: 0.0297686
DEBUG:root:[ Iteration 340 ] Test loss: 0.0471135
DEBUG:root:[ Iteration 342 ] Training loss: 0.0310629
DEBUG:root:[ Iteration 345 ] Training loss: 0.0369243
DEBUG:root:[ Iteration 348 ] Training loss: 0.0383231
DEBUG:root:[ Iteration 351 ] Training loss: 0.0352723
DEBUG:root:[ Iteration 354 ] Training loss: 0.0347919
DEBUG:root:[ Iteration 357 ] Training loss: 0.0415259
DEBUG:root:[ Iteration 360 ] Training loss: 0.039954
DEBUG:root:[ Iteration 360 ] Test loss: 0.0369149
DEBUG:root:[ Iteration 363 ] Training loss: 0.0321111
DEBUG:root:[ Iteration 366 ] Training loss: 0.0386909
DEBUG:root:[ Iteration 369 ] Training loss: 0.0289592
DEBUG:root:[ Iteration 372 ] Training loss: 0.0407368
DEBUG:root:[ Iteration 375 ] Training loss: 0.0365543
DEBUG:root:[ Iteration 378 ] Training loss: 0.0352345
DEBUG:root:[ Iteration 380 ] Test loss: 0.0465813
DEBUG:root:[ Iteration 381 ] Training loss: 0.0387478
DEBUG:root:[ Iteration 384 ] Training loss: 0.0294373
DEBUG:root:[ Iteration 387 ] Training loss: 0.0393661
DEBUG:root:[ Iteration 390 ] Training loss: 0.0326934
DEBUG:root:[ Iteration 393 ] Training loss: 0.0303512
DEBUG:root:[ Iteration 396 ] Training loss: 0.0411066
DEBUG:root:[ Iteration 399 ] Training loss: 0.035578
DEBUG:root:[ Iteration 400 ] Test loss: 0.0415361
DEBUG:root:[ Iteration 402 ] Training loss: 0.0456975
DEBUG:root:[ Iteration 405 ] Training loss: 0.0352717
DEBUG:root:[ Iteration 408 ] Training loss: 0.0380667
DEBUG:root:[ Iteration 411 ] Training loss: 0.0383743
DEBUG:root:[ Iteration 414 ] Training loss: 0.0349551
DEBUG:root:[ Iteration 417 ] Training loss: 0.032639
DEBUG:root:[ Iteration 420 ] Training loss: 0.0341729
DEBUG:root:[ Iteration 420 ] Test loss: 0.0352103
DEBUG:root:[ Iteration 423 ] Training loss: 0.03789
DEBUG:root:[ Iteration 426 ] Training loss: 0.0336779
DEBUG:root:[ Iteration 429 ] Training loss: 0.0313864
DEBUG:root:[ Iteration 432 ] Training loss: 0.0349535
DEBUG:root:[ Iteration 435 ] Training loss: 0.0336169
DEBUG:root:[ Iteration 438 ] Training loss: 0.0397282
DEBUG:root:[ Iteration 440 ] Test loss: 0.0469333
DEBUG:root:[ Iteration 441 ] Training loss: 0.0408858
DEBUG:root:[ Iteration 444 ] Training loss: 0.0346715
DEBUG:root:[ Iteration 447 ] Training loss: 0.0439071
DEBUG:root:[ Iteration 450 ] Training loss: 0.0418195
DEBUG:root:[ Iteration 453 ] Training loss: 0.0344892
DEBUG:root:[ Iteration 456 ] Training loss: 0.0379842
DEBUG:root:[ Iteration 459 ] Training loss: 0.0378653
DEBUG:root:[ Iteration 460 ] Test loss: 0.0463122
DEBUG:root:[ Iteration 462 ] Training loss: 0.0372246
DEBUG:root:[ Iteration 465 ] Training loss: 0.0449456
DEBUG:root:[ Iteration 468 ] Training loss: 0.030316
DEBUG:root:[ Iteration 471 ] Training loss: 0.0392914
DEBUG:root:[ Iteration 474 ] Training loss: 0.0407312
DEBUG:root:[ Iteration 477 ] Training loss: 0.0389135
DEBUG:root:[ Iteration 480 ] Training loss: 0.0335715
DEBUG:root:[ Iteration 480 ] Test loss: 0.0405088
DEBUG:root:[ Iteration 483 ] Training loss: 0.0308984
DEBUG:root:[ Iteration 486 ] Training loss: 0.036324
DEBUG:root:[ Iteration 489 ] Training loss: 0.0392231
DEBUG:root:[ Iteration 492 ] Training loss: 0.040814
DEBUG:root:[ Iteration 495 ] Training loss: 0.0405401
DEBUG:root:[ Iteration 498 ] Training loss: 0.0299206
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-27-2016_12h06m19s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.145324
DEBUG:root:[ Iteration 0 ] Test loss: 0.145383
DEBUG:root:[ Iteration 3 ] Training loss: 0.132269
DEBUG:root:[ Iteration 6 ] Training loss: 0.10138
DEBUG:root:[ Iteration 9 ] Training loss: 0.0725761
DEBUG:root:[ Iteration 12 ] Training loss: 0.0709006
DEBUG:root:[ Iteration 15 ] Training loss: 0.0645622
DEBUG:root:[ Iteration 18 ] Training loss: 0.0538337
DEBUG:root:[ Iteration 20 ] Test loss: 0.0629369
DEBUG:root:[ Iteration 21 ] Training loss: 0.0567282
DEBUG:root:[ Iteration 24 ] Training loss: 0.055568
DEBUG:root:[ Iteration 27 ] Training loss: 0.0479183
DEBUG:root:[ Iteration 30 ] Training loss: 0.0430864
DEBUG:root:[ Iteration 33 ] Training loss: 0.0336128
DEBUG:root:[ Iteration 36 ] Training loss: 0.0366784
DEBUG:root:[ Iteration 39 ] Training loss: 0.031238
DEBUG:root:[ Iteration 40 ] Test loss: 0.0398727
DEBUG:root:[ Iteration 42 ] Training loss: 0.0279767
DEBUG:root:[ Iteration 45 ] Training loss: 0.0293905
DEBUG:root:[ Iteration 48 ] Training loss: 0.0261465
DEBUG:root:[ Iteration 51 ] Training loss: 0.0230032
DEBUG:root:[ Iteration 54 ] Training loss: 0.018029
DEBUG:root:[ Iteration 57 ] Training loss: 0.0188368
DEBUG:root:[ Iteration 60 ] Training loss: 0.0202019
DEBUG:root:[ Iteration 60 ] Test loss: 0.0256379
DEBUG:root:[ Iteration 63 ] Training loss: 0.018551
DEBUG:root:[ Iteration 66 ] Training loss: 0.0168542
DEBUG:root:[ Iteration 69 ] Training loss: 0.0158669
DEBUG:root:[ Iteration 72 ] Training loss: 0.0136412
DEBUG:root:[ Iteration 75 ] Training loss: 0.0197317
DEBUG:root:[ Iteration 78 ] Training loss: 0.0163406
DEBUG:root:[ Iteration 80 ] Test loss: 0.0206386
DEBUG:root:[ Iteration 81 ] Training loss: 0.0141477
DEBUG:root:[ Iteration 84 ] Training loss: 0.00937366
DEBUG:root:[ Iteration 87 ] Training loss: 0.0121447
DEBUG:root:[ Iteration 90 ] Training loss: 0.0139704
DEBUG:root:[ Iteration 93 ] Training loss: 0.0122603
DEBUG:root:[ Iteration 96 ] Training loss: 0.0100407
DEBUG:root:[ Iteration 99 ] Training loss: 0.0141851
DEBUG:root:[ Iteration 100 ] Test loss: 0.0183667
DEBUG:root:[ Iteration 102 ] Training loss: 0.010977
DEBUG:root:[ Iteration 105 ] Training loss: 0.0105845
DEBUG:root:[ Iteration 108 ] Training loss: 0.0113523
DEBUG:root:[ Iteration 111 ] Training loss: 0.0112092
DEBUG:root:[ Iteration 114 ] Training loss: 0.00941269
DEBUG:root:[ Iteration 117 ] Training loss: 0.011463
DEBUG:root:[ Iteration 120 ] Training loss: 0.0118492
DEBUG:root:[ Iteration 120 ] Test loss: 0.0157923
DEBUG:root:[ Iteration 123 ] Training loss: 0.00918902
DEBUG:root:[ Iteration 126 ] Training loss: 0.00958742
DEBUG:root:[ Iteration 129 ] Training loss: 0.00909889
DEBUG:root:[ Iteration 132 ] Training loss: 0.0102585
DEBUG:root:[ Iteration 135 ] Training loss: 0.00856025
DEBUG:root:[ Iteration 138 ] Training loss: 0.00897996
DEBUG:root:[ Iteration 140 ] Test loss: 0.0143192
DEBUG:root:[ Iteration 141 ] Training loss: 0.00643489
DEBUG:root:[ Iteration 144 ] Training loss: 0.00944126
DEBUG:root:[ Iteration 147 ] Training loss: 0.00923579
DEBUG:root:[ Iteration 150 ] Training loss: 0.00943849
DEBUG:root:[ Iteration 153 ] Training loss: 0.00940516
DEBUG:root:[ Iteration 156 ] Training loss: 0.00773225
DEBUG:root:[ Iteration 159 ] Training loss: 0.00744138
DEBUG:root:[ Iteration 160 ] Test loss: 0.0124411
DEBUG:root:[ Iteration 162 ] Training loss: 0.00797019
DEBUG:root:[ Iteration 165 ] Training loss: 0.00903052
DEBUG:root:[ Iteration 168 ] Training loss: 0.00959457
DEBUG:root:[ Iteration 171 ] Training loss: 0.00851513
DEBUG:root:[ Iteration 174 ] Training loss: 0.00679462
DEBUG:root:[ Iteration 177 ] Training loss: 0.0060104
DEBUG:root:[ Iteration 180 ] Training loss: 0.0120845
DEBUG:root:[ Iteration 180 ] Test loss: 0.0147937
DEBUG:root:[ Iteration 183 ] Training loss: 0.0075645
DEBUG:root:[ Iteration 186 ] Training loss: 0.00869816
DEBUG:root:[ Iteration 189 ] Training loss: 0.00638967
DEBUG:root:[ Iteration 192 ] Training loss: 0.00720338
DEBUG:root:[ Iteration 195 ] Training loss: 0.00710892
DEBUG:root:[ Iteration 198 ] Training loss: 0.00563762
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-27-2016_14h27m35s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.135872
DEBUG:root:[ Iteration 0 ] Test loss: 0.109921
DEBUG:root:[ Iteration 3 ] Training loss: 0.118803
DEBUG:root:[ Iteration 6 ] Training loss: 0.0974384
DEBUG:root:[ Iteration 9 ] Training loss: 0.0793706
DEBUG:root:[ Iteration 12 ] Training loss: 0.0829507
DEBUG:root:[ Iteration 15 ] Training loss: 0.0640314
DEBUG:root:[ Iteration 18 ] Training loss: 0.0572622
DEBUG:root:[ Iteration 20 ] Test loss: 0.0528262
DEBUG:root:[ Iteration 21 ] Training loss: 0.0573761
DEBUG:root:[ Iteration 24 ] Training loss: 0.0559985
DEBUG:root:[ Iteration 27 ] Training loss: 0.0575652
DEBUG:root:[ Iteration 30 ] Training loss: 0.0511035
DEBUG:root:[ Iteration 33 ] Training loss: 0.0444592
DEBUG:root:[ Iteration 36 ] Training loss: 0.0406761
DEBUG:root:[ Iteration 39 ] Training loss: 0.0316635
DEBUG:root:[ Iteration 40 ] Test loss: 0.027045
DEBUG:root:[ Iteration 42 ] Training loss: 0.0305441
DEBUG:root:[ Iteration 45 ] Training loss: 0.030626
DEBUG:root:[ Iteration 48 ] Training loss: 0.0255553
DEBUG:root:[ Iteration 51 ] Training loss: 0.0283536
DEBUG:root:[ Iteration 54 ] Training loss: 0.0251647
DEBUG:root:[ Iteration 57 ] Training loss: 0.0245401
DEBUG:root:[ Iteration 60 ] Training loss: 0.0184639
DEBUG:root:[ Iteration 60 ] Test loss: 0.0220407
DEBUG:root:[ Iteration 63 ] Training loss: 0.0230307
DEBUG:root:[ Iteration 66 ] Training loss: 0.0196787
DEBUG:root:[ Iteration 69 ] Training loss: 0.0231225
DEBUG:root:[ Iteration 72 ] Training loss: 0.0188831
DEBUG:root:[ Iteration 75 ] Training loss: 0.0162794
DEBUG:root:[ Iteration 78 ] Training loss: 0.0165557
DEBUG:root:[ Iteration 80 ] Test loss: 0.0119594
DEBUG:root:[ Iteration 81 ] Training loss: 0.0162574
DEBUG:root:[ Iteration 84 ] Training loss: 0.0215824
DEBUG:root:[ Iteration 87 ] Training loss: 0.0161561
DEBUG:root:[ Iteration 90 ] Training loss: 0.0223877
DEBUG:root:[ Iteration 93 ] Training loss: 0.0203899
DEBUG:root:[ Iteration 96 ] Training loss: 0.0171477
DEBUG:root:[ Iteration 99 ] Training loss: 0.0143175
DEBUG:root:[ Iteration 100 ] Test loss: 0.0178198
DEBUG:root:[ Iteration 102 ] Training loss: 0.0159825
DEBUG:root:[ Iteration 105 ] Training loss: 0.0184587
DEBUG:root:[ Iteration 108 ] Training loss: 0.0139586
DEBUG:root:[ Iteration 111 ] Training loss: 0.0118942
DEBUG:root:[ Iteration 114 ] Training loss: 0.00998137
DEBUG:root:[ Iteration 117 ] Training loss: 0.0130577
DEBUG:root:[ Iteration 120 ] Training loss: 0.0119137
DEBUG:root:[ Iteration 120 ] Test loss: 0.0102643
DEBUG:root:[ Iteration 123 ] Training loss: 0.012207
DEBUG:root:[ Iteration 126 ] Training loss: 0.00920021
DEBUG:root:[ Iteration 129 ] Training loss: 0.0116399
DEBUG:root:[ Iteration 132 ] Training loss: 0.00958822
DEBUG:root:[ Iteration 135 ] Training loss: 0.0112466
DEBUG:root:[ Iteration 138 ] Training loss: 0.00858734
DEBUG:root:[ Iteration 140 ] Test loss: 0.0103198
DEBUG:root:[ Iteration 141 ] Training loss: 0.00944949
DEBUG:root:[ Iteration 144 ] Training loss: 0.00992119
DEBUG:root:[ Iteration 147 ] Training loss: 0.00763682
DEBUG:root:[ Iteration 150 ] Training loss: 0.01
DEBUG:root:[ Iteration 153 ] Training loss: 0.00597819
DEBUG:root:[ Iteration 156 ] Training loss: 0.0096032
DEBUG:root:[ Iteration 159 ] Training loss: 0.0093804
DEBUG:root:[ Iteration 160 ] Test loss: 0.00873472
DEBUG:root:[ Iteration 162 ] Training loss: 0.0101987
DEBUG:root:[ Iteration 165 ] Training loss: 0.0099221
DEBUG:root:[ Iteration 168 ] Training loss: 0.00909236
DEBUG:root:[ Iteration 171 ] Training loss: 0.00841397
DEBUG:root:[ Iteration 174 ] Training loss: 0.0103276
DEBUG:root:[ Iteration 177 ] Training loss: 0.00551137
DEBUG:root:[ Iteration 180 ] Training loss: 0.0120747
DEBUG:root:[ Iteration 180 ] Test loss: 0.00970817
DEBUG:root:[ Iteration 183 ] Training loss: 0.00812059
DEBUG:root:[ Iteration 186 ] Training loss: 0.00611838
DEBUG:root:[ Iteration 189 ] Training loss: 0.005868
DEBUG:root:[ Iteration 192 ] Training loss: 0.00599127
DEBUG:root:[ Iteration 195 ] Training loss: 0.00570342
DEBUG:root:[ Iteration 198 ] Training loss: 0.00621704
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-27-2016_15h30m01s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.137565
DEBUG:root:[ Iteration 0 ] Test loss: 0.119392
DEBUG:root:[ Iteration 3 ] Training loss: 0.111971
DEBUG:root:[ Iteration 6 ] Training loss: 0.0989706
DEBUG:root:[ Iteration 9 ] Training loss: 0.0846478
DEBUG:root:[ Iteration 12 ] Training loss: 0.0700202
DEBUG:root:[ Iteration 15 ] Training loss: 0.0651526
DEBUG:root:[ Iteration 18 ] Training loss: 0.0607773
DEBUG:root:[ Iteration 20 ] Test loss: 0.0557568
DEBUG:root:[ Iteration 21 ] Training loss: 0.0505611
DEBUG:root:[ Iteration 24 ] Training loss: 0.0488368
DEBUG:root:[ Iteration 27 ] Training loss: 0.0513182
DEBUG:root:[ Iteration 30 ] Training loss: 0.0452079
DEBUG:root:[ Iteration 33 ] Training loss: 0.0417616
DEBUG:root:[ Iteration 36 ] Training loss: 0.0393467
DEBUG:root:[ Iteration 39 ] Training loss: 0.0346682
DEBUG:root:[ Iteration 40 ] Test loss: 0.0323566
DEBUG:root:[ Iteration 42 ] Training loss: 0.0368153
DEBUG:root:[ Iteration 45 ] Training loss: 0.0363255
DEBUG:root:[ Iteration 48 ] Training loss: 0.026735
DEBUG:root:[ Iteration 51 ] Training loss: 0.0297045
DEBUG:root:[ Iteration 54 ] Training loss: 0.0239968
DEBUG:root:[ Iteration 57 ] Training loss: 0.0274016
DEBUG:root:[ Iteration 60 ] Training loss: 0.0215688
DEBUG:root:[ Iteration 60 ] Test loss: 0.022742
DEBUG:root:[ Iteration 63 ] Training loss: 0.023427
DEBUG:root:[ Iteration 66 ] Training loss: 0.0184614
DEBUG:root:[ Iteration 69 ] Training loss: 0.0180418
DEBUG:root:[ Iteration 72 ] Training loss: 0.0188646
DEBUG:root:[ Iteration 75 ] Training loss: 0.0187388
DEBUG:root:[ Iteration 78 ] Training loss: 0.018081
DEBUG:root:[ Iteration 80 ] Test loss: 0.0118137
DEBUG:root:[ Iteration 81 ] Training loss: 0.0144679
DEBUG:root:[ Iteration 84 ] Training loss: 0.0157398
DEBUG:root:[ Iteration 87 ] Training loss: 0.0179147
DEBUG:root:[ Iteration 90 ] Training loss: 0.0124807
DEBUG:root:[ Iteration 93 ] Training loss: 0.01456
DEBUG:root:[ Iteration 96 ] Training loss: 0.0131196
DEBUG:root:[ Iteration 99 ] Training loss: 0.0154821
DEBUG:root:[ Iteration 100 ] Test loss: 0.013052
DEBUG:root:[ Iteration 102 ] Training loss: 0.0145299
DEBUG:root:[ Iteration 105 ] Training loss: 0.0145438
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-27-2016_16h09m20s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0710406
DEBUG:root:[ Iteration 0 ] Test loss: 0.0819122
DEBUG:root:[ Iteration 3 ] Training loss: 0.0757393
DEBUG:root:[ Iteration 6 ] Training loss: 0.0824374
DEBUG:root:[ Iteration 9 ] Training loss: 0.0510314
DEBUG:root:[ Iteration 12 ] Training loss: 0.066369
DEBUG:root:[ Iteration 15 ] Training loss: 0.061211
DEBUG:root:[ Iteration 18 ] Training loss: 0.0501622
DEBUG:root:[ Iteration 20 ] Test loss: 0.047644
DEBUG:root:[ Iteration 21 ] Training loss: 0.0577791
DEBUG:root:[ Iteration 24 ] Training loss: 0.0535259
DEBUG:root:[ Iteration 27 ] Training loss: 0.0450313
DEBUG:root:[ Iteration 30 ] Training loss: 0.0453095
DEBUG:root:[ Iteration 33 ] Training loss: 0.047802
DEBUG:root:[ Iteration 36 ] Training loss: 0.0400634
DEBUG:root:[ Iteration 39 ] Training loss: 0.0379586
DEBUG:root:[ Iteration 40 ] Test loss: 0.0361267
DEBUG:root:[ Iteration 42 ] Training loss: 0.0422204
DEBUG:root:[ Iteration 45 ] Training loss: 0.0401924
DEBUG:root:[ Iteration 48 ] Training loss: 0.0487459
DEBUG:root:[ Iteration 51 ] Training loss: 0.0386591
DEBUG:root:[ Iteration 54 ] Training loss: 0.0395959
DEBUG:root:[ Iteration 57 ] Training loss: 0.0322319
DEBUG:root:[ Iteration 60 ] Training loss: 0.0419852
DEBUG:root:[ Iteration 60 ] Test loss: 0.0394815
DEBUG:root:[ Iteration 63 ] Training loss: 0.0341276
DEBUG:root:[ Iteration 66 ] Training loss: 0.033553
DEBUG:root:[ Iteration 69 ] Training loss: 0.039207
DEBUG:root:[ Iteration 72 ] Training loss: 0.0382311
DEBUG:root:[ Iteration 75 ] Training loss: 0.0343675
DEBUG:root:[ Iteration 78 ] Training loss: 0.0335612
DEBUG:root:[ Iteration 80 ] Test loss: 0.0365389
DEBUG:root:[ Iteration 81 ] Training loss: 0.0356107
DEBUG:root:[ Iteration 84 ] Training loss: 0.0313732
DEBUG:root:[ Iteration 87 ] Training loss: 0.0297823
DEBUG:root:[ Iteration 90 ] Training loss: 0.0298455
DEBUG:root:[ Iteration 93 ] Training loss: 0.0316155
DEBUG:root:[ Iteration 96 ] Training loss: 0.0297843
DEBUG:root:[ Iteration 99 ] Training loss: 0.0359791
DEBUG:root:[ Iteration 100 ] Test loss: 0.0309309
DEBUG:root:[ Iteration 102 ] Training loss: 0.0297629
DEBUG:root:[ Iteration 105 ] Training loss: 0.0272115
DEBUG:root:[ Iteration 108 ] Training loss: 0.028665
DEBUG:root:[ Iteration 111 ] Training loss: 0.0277568
DEBUG:root:[ Iteration 114 ] Training loss: 0.030144
DEBUG:root:[ Iteration 117 ] Training loss: 0.0259168
DEBUG:root:[ Iteration 120 ] Training loss: 0.0293579
DEBUG:root:[ Iteration 120 ] Test loss: 0.0296487
DEBUG:root:[ Iteration 123 ] Training loss: 0.0349673
DEBUG:root:[ Iteration 126 ] Training loss: 0.0294181
DEBUG:root:[ Iteration 129 ] Training loss: 0.0318323
DEBUG:root:[ Iteration 132 ] Training loss: 0.0260253
DEBUG:root:[ Iteration 135 ] Training loss: 0.0362684
DEBUG:root:[ Iteration 138 ] Training loss: 0.029167
DEBUG:root:[ Iteration 140 ] Test loss: 0.0276554
DEBUG:root:[ Iteration 141 ] Training loss: 0.0323924
DEBUG:root:[ Iteration 144 ] Training loss: 0.0251247
DEBUG:root:[ Iteration 147 ] Training loss: 0.0276234
DEBUG:root:[ Iteration 150 ] Training loss: 0.0308279
DEBUG:root:[ Iteration 153 ] Training loss: 0.0232597
DEBUG:root:[ Iteration 156 ] Training loss: 0.0383902
DEBUG:root:[ Iteration 159 ] Training loss: 0.0245727
DEBUG:root:[ Iteration 160 ] Test loss: 0.026544
DEBUG:root:[ Iteration 162 ] Training loss: 0.0330004
DEBUG:root:[ Iteration 165 ] Training loss: 0.0284123
DEBUG:root:[ Iteration 168 ] Training loss: 0.0297752
DEBUG:root:[ Iteration 171 ] Training loss: 0.028907
DEBUG:root:[ Iteration 174 ] Training loss: 0.0305388
DEBUG:root:[ Iteration 177 ] Training loss: 0.0269843
DEBUG:root:[ Iteration 180 ] Training loss: 0.0242389
DEBUG:root:[ Iteration 180 ] Test loss: 0.0272112
DEBUG:root:[ Iteration 183 ] Training loss: 0.0267282
DEBUG:root:[ Iteration 186 ] Training loss: 0.0263008
DEBUG:root:[ Iteration 189 ] Training loss: 0.0266801
DEBUG:root:[ Iteration 192 ] Training loss: 0.0286309
DEBUG:root:[ Iteration 195 ] Training loss: 0.0250267
DEBUG:root:[ Iteration 198 ] Training loss: 0.0267541
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-27-2016_16h16m02s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0258224
DEBUG:root:[ Iteration 0 ] Test loss: 0.0207389
DEBUG:root:[ Iteration 3 ] Training loss: 0.0225778
DEBUG:root:[ Iteration 6 ] Training loss: 0.0248504
DEBUG:root:[ Iteration 9 ] Training loss: 0.0298401
DEBUG:root:[ Iteration 12 ] Training loss: 0.0233382
DEBUG:root:[ Iteration 15 ] Training loss: 0.0208628
DEBUG:root:[ Iteration 18 ] Training loss: 0.0232084
DEBUG:root:[ Iteration 20 ] Test loss: 0.0233655
DEBUG:root:[ Iteration 21 ] Training loss: 0.0283115
DEBUG:root:[ Iteration 24 ] Training loss: 0.0206788
DEBUG:root:[ Iteration 27 ] Training loss: 0.0224817
DEBUG:root:[ Iteration 30 ] Training loss: 0.0223423
DEBUG:root:[ Iteration 33 ] Training loss: 0.0259307
DEBUG:root:[ Iteration 36 ] Training loss: 0.0256062
DEBUG:root:[ Iteration 39 ] Training loss: 0.01957
DEBUG:root:[ Iteration 40 ] Test loss: 0.021394
DEBUG:root:[ Iteration 42 ] Training loss: 0.0204115
DEBUG:root:[ Iteration 45 ] Training loss: 0.0175056
DEBUG:root:[ Iteration 48 ] Training loss: 0.0241174
DEBUG:root:[ Iteration 51 ] Training loss: 0.0260569
DEBUG:root:[ Iteration 54 ] Training loss: 0.0219911
DEBUG:root:[ Iteration 57 ] Training loss: 0.0224868
DEBUG:root:[ Iteration 60 ] Training loss: 0.0217116
DEBUG:root:[ Iteration 60 ] Test loss: 0.0232491
DEBUG:root:[ Iteration 63 ] Training loss: 0.0193628
DEBUG:root:[ Iteration 66 ] Training loss: 0.023766
DEBUG:root:[ Iteration 69 ] Training loss: 0.0193984
DEBUG:root:[ Iteration 72 ] Training loss: 0.022324
DEBUG:root:[ Iteration 75 ] Training loss: 0.0221265
DEBUG:root:[ Iteration 78 ] Training loss: 0.0191013
DEBUG:root:[ Iteration 80 ] Test loss: 0.0189637
DEBUG:root:[ Iteration 81 ] Training loss: 0.0232711
DEBUG:root:[ Iteration 84 ] Training loss: 0.0236819
DEBUG:root:[ Iteration 87 ] Training loss: 0.025431
DEBUG:root:[ Iteration 90 ] Training loss: 0.0239108
DEBUG:root:[ Iteration 93 ] Training loss: 0.0170145
DEBUG:root:[ Iteration 96 ] Training loss: 0.0201
DEBUG:root:[ Iteration 99 ] Training loss: 0.019247
DEBUG:root:[ Iteration 100 ] Test loss: 0.0167453
DEBUG:root:[ Iteration 102 ] Training loss: 0.0165706
DEBUG:root:[ Iteration 105 ] Training loss: 0.02423
DEBUG:root:[ Iteration 108 ] Training loss: 0.0153349
DEBUG:root:[ Iteration 111 ] Training loss: 0.0152776
DEBUG:root:[ Iteration 114 ] Training loss: 0.0193108
DEBUG:root:[ Iteration 117 ] Training loss: 0.0226929
DEBUG:root:[ Iteration 120 ] Training loss: 0.0187626
DEBUG:root:[ Iteration 120 ] Test loss: 0.0174802
DEBUG:root:[ Iteration 123 ] Training loss: 0.0139722
DEBUG:root:[ Iteration 126 ] Training loss: 0.0237948
DEBUG:root:[ Iteration 129 ] Training loss: 0.018231
DEBUG:root:[ Iteration 132 ] Training loss: 0.0183859
DEBUG:root:[ Iteration 135 ] Training loss: 0.0167849
DEBUG:root:[ Iteration 138 ] Training loss: 0.0166049
DEBUG:root:[ Iteration 140 ] Test loss: 0.0184522
DEBUG:root:[ Iteration 141 ] Training loss: 0.0168402
DEBUG:root:[ Iteration 144 ] Training loss: 0.026575
DEBUG:root:[ Iteration 147 ] Training loss: 0.0163016
DEBUG:root:[ Iteration 150 ] Training loss: 0.0170531
DEBUG:root:[ Iteration 153 ] Training loss: 0.0167926
DEBUG:root:[ Iteration 156 ] Training loss: 0.0237217
DEBUG:root:[ Iteration 159 ] Training loss: 0.0205589
DEBUG:root:[ Iteration 160 ] Test loss: 0.0189214
DEBUG:root:[ Iteration 162 ] Training loss: 0.0178385
DEBUG:root:[ Iteration 165 ] Training loss: 0.0167236
DEBUG:root:[ Iteration 168 ] Training loss: 0.0132729
DEBUG:root:[ Iteration 171 ] Training loss: 0.0162788
DEBUG:root:[ Iteration 174 ] Training loss: 0.0146948
DEBUG:root:[ Iteration 177 ] Training loss: 0.0154507
DEBUG:root:[ Iteration 180 ] Training loss: 0.0163296
DEBUG:root:[ Iteration 180 ] Test loss: 0.0190572
DEBUG:root:[ Iteration 183 ] Training loss: 0.020596
DEBUG:root:[ Iteration 186 ] Training loss: 0.0161082
DEBUG:root:[ Iteration 189 ] Training loss: 0.0160654
DEBUG:root:[ Iteration 192 ] Training loss: 0.0176865
DEBUG:root:[ Iteration 195 ] Training loss: 0.0204524
DEBUG:root:[ Iteration 198 ] Training loss: 0.0148676
DEBUG:root:[ Iteration 200 ] Test loss: 0.0176993
DEBUG:root:[ Iteration 201 ] Training loss: 0.0159056
DEBUG:root:[ Iteration 204 ] Training loss: 0.0176859
DEBUG:root:[ Iteration 207 ] Training loss: 0.0192578
DEBUG:root:[ Iteration 210 ] Training loss: 0.0154983
DEBUG:root:[ Iteration 213 ] Training loss: 0.0139502
DEBUG:root:[ Iteration 216 ] Training loss: 0.0185883
DEBUG:root:[ Iteration 219 ] Training loss: 0.0155138
DEBUG:root:[ Iteration 220 ] Test loss: 0.0197
DEBUG:root:[ Iteration 222 ] Training loss: 0.0207517
DEBUG:root:[ Iteration 225 ] Training loss: 0.0184921
DEBUG:root:[ Iteration 228 ] Training loss: 0.0149598
DEBUG:root:[ Iteration 231 ] Training loss: 0.0164178
DEBUG:root:[ Iteration 234 ] Training loss: 0.0190963
DEBUG:root:[ Iteration 237 ] Training loss: 0.0162516
DEBUG:root:[ Iteration 240 ] Training loss: 0.0204471
DEBUG:root:[ Iteration 240 ] Test loss: 0.0249276
DEBUG:root:[ Iteration 243 ] Training loss: 0.0138099
DEBUG:root:[ Iteration 246 ] Training loss: 0.0178183
DEBUG:root:[ Iteration 249 ] Training loss: 0.0155275
DEBUG:root:[ Iteration 252 ] Training loss: 0.0153226
DEBUG:root:[ Iteration 255 ] Training loss: 0.0192884
DEBUG:root:[ Iteration 258 ] Training loss: 0.0210332
DEBUG:root:[ Iteration 260 ] Test loss: 0.0162814
DEBUG:root:[ Iteration 261 ] Training loss: 0.0176945
DEBUG:root:[ Iteration 264 ] Training loss: 0.0166211
DEBUG:root:[ Iteration 267 ] Training loss: 0.0146668
DEBUG:root:[ Iteration 270 ] Training loss: 0.0181038
DEBUG:root:[ Iteration 273 ] Training loss: 0.0150848
DEBUG:root:[ Iteration 276 ] Training loss: 0.0154577
DEBUG:root:[ Iteration 279 ] Training loss: 0.0129859
DEBUG:root:[ Iteration 280 ] Test loss: 0.0151077
DEBUG:root:[ Iteration 282 ] Training loss: 0.0129649
DEBUG:root:[ Iteration 285 ] Training loss: 0.0161648
DEBUG:root:[ Iteration 288 ] Training loss: 0.0130392
DEBUG:root:[ Iteration 291 ] Training loss: 0.0139671
DEBUG:root:[ Iteration 294 ] Training loss: 0.0148856
DEBUG:root:[ Iteration 297 ] Training loss: 0.0155534
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-27-2016_16h23m55s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.19094
DEBUG:root:[ Iteration 0 ] Test loss: 0.190359
DEBUG:root:[ Iteration 3 ] Training loss: 0.193623
DEBUG:root:[ Iteration 6 ] Training loss: 0.190103
DEBUG:root:[ Iteration 9 ] Training loss: 0.184255
DEBUG:root:[ Iteration 12 ] Training loss: 0.179976
DEBUG:root:[ Iteration 15 ] Training loss: 0.181588
DEBUG:root:[ Iteration 18 ] Training loss: 0.173982
DEBUG:root:[ Iteration 20 ] Test loss: 0.166412
DEBUG:root:[ Iteration 21 ] Training loss: 0.157122
DEBUG:root:[ Iteration 24 ] Training loss: 0.124214
DEBUG:root:[ Iteration 27 ] Training loss: 0.0908677
DEBUG:root:[ Iteration 30 ] Training loss: 0.0863205
DEBUG:root:[ Iteration 33 ] Training loss: 0.104173
DEBUG:root:[ Iteration 36 ] Training loss: 0.0917369
DEBUG:root:[ Iteration 39 ] Training loss: 0.0691521
DEBUG:root:[ Iteration 40 ] Test loss: 0.0788921
DEBUG:root:[ Iteration 42 ] Training loss: 0.083868
DEBUG:root:[ Iteration 45 ] Training loss: 0.0766178
DEBUG:root:[ Iteration 48 ] Training loss: 0.059246
DEBUG:root:[ Iteration 51 ] Training loss: 0.0720508
DEBUG:root:[ Iteration 54 ] Training loss: 0.0770464
DEBUG:root:[ Iteration 57 ] Training loss: 0.0824575
DEBUG:root:[ Iteration 60 ] Training loss: 0.0817674
DEBUG:root:[ Iteration 60 ] Test loss: 0.0917225
DEBUG:root:[ Iteration 63 ] Training loss: 0.0728328
DEBUG:root:[ Iteration 66 ] Training loss: 0.0633921
DEBUG:root:[ Iteration 69 ] Training loss: 0.0607435
DEBUG:root:[ Iteration 72 ] Training loss: 0.0613389
DEBUG:root:[ Iteration 75 ] Training loss: 0.0646404
DEBUG:root:[ Iteration 78 ] Training loss: 0.0562672
DEBUG:root:[ Iteration 80 ] Test loss: 0.0741207
DEBUG:root:[ Iteration 81 ] Training loss: 0.0780896
DEBUG:root:[ Iteration 84 ] Training loss: 0.0567227
DEBUG:root:[ Iteration 87 ] Training loss: 0.0607909
DEBUG:root:[ Iteration 90 ] Training loss: 0.0601487
DEBUG:root:[ Iteration 93 ] Training loss: 0.0591621
DEBUG:root:[ Iteration 96 ] Training loss: 0.0666145
DEBUG:root:[ Iteration 99 ] Training loss: 0.0669556
DEBUG:root:[ Iteration 100 ] Test loss: 0.0566902
DEBUG:root:[ Iteration 102 ] Training loss: 0.0562197
DEBUG:root:[ Iteration 105 ] Training loss: 0.0704421
DEBUG:root:[ Iteration 108 ] Training loss: 0.052096
DEBUG:root:[ Iteration 111 ] Training loss: 0.056269
DEBUG:root:[ Iteration 114 ] Training loss: 0.0608123
DEBUG:root:[ Iteration 117 ] Training loss: 0.0801912
DEBUG:root:[ Iteration 120 ] Training loss: 0.0588106
DEBUG:root:[ Iteration 120 ] Test loss: 0.063276
DEBUG:root:[ Iteration 123 ] Training loss: 0.0707839
DEBUG:root:[ Iteration 126 ] Training loss: 0.0436058
DEBUG:root:[ Iteration 129 ] Training loss: 0.0530427
DEBUG:root:[ Iteration 132 ] Training loss: 0.0587902
DEBUG:root:[ Iteration 135 ] Training loss: 0.0519148
DEBUG:root:[ Iteration 138 ] Training loss: 0.0474879
DEBUG:root:[ Iteration 140 ] Test loss: 0.0350313
DEBUG:root:[ Iteration 141 ] Training loss: 0.0658635
DEBUG:root:[ Iteration 144 ] Training loss: 0.040401
DEBUG:root:[ Iteration 147 ] Training loss: 0.0569693
DEBUG:root:[ Iteration 150 ] Training loss: 0.0487519
DEBUG:root:[ Iteration 153 ] Training loss: 0.0389473
DEBUG:root:[ Iteration 156 ] Training loss: 0.0664834
DEBUG:root:[ Iteration 159 ] Training loss: 0.0595529
DEBUG:root:[ Iteration 160 ] Test loss: 0.0513225
DEBUG:root:[ Iteration 162 ] Training loss: 0.0491038
DEBUG:root:[ Iteration 165 ] Training loss: 0.0468287
DEBUG:root:[ Iteration 168 ] Training loss: 0.0521418
DEBUG:root:[ Iteration 171 ] Training loss: 0.0451249
DEBUG:root:[ Iteration 174 ] Training loss: 0.0513512
DEBUG:root:[ Iteration 177 ] Training loss: 0.0417252
DEBUG:root:[ Iteration 180 ] Training loss: 0.0552855
DEBUG:root:[ Iteration 180 ] Test loss: 0.0432459
DEBUG:root:[ Iteration 183 ] Training loss: 0.0366209
DEBUG:root:[ Iteration 186 ] Training loss: 0.0442075
DEBUG:root:[ Iteration 189 ] Training loss: 0.0491258
DEBUG:root:[ Iteration 192 ] Training loss: 0.0383902
DEBUG:root:[ Iteration 195 ] Training loss: 0.0404742
DEBUG:root:[ Iteration 198 ] Training loss: 0.0403054
DEBUG:root:[ Iteration 200 ] Test loss: 0.038527
DEBUG:root:[ Iteration 201 ] Training loss: 0.0351791
DEBUG:root:[ Iteration 204 ] Training loss: 0.0357334
DEBUG:root:[ Iteration 207 ] Training loss: 0.0332563
DEBUG:root:[ Iteration 210 ] Training loss: 0.0409334
DEBUG:root:[ Iteration 213 ] Training loss: 0.0369082
DEBUG:root:[ Iteration 216 ] Training loss: 0.0333406
DEBUG:root:[ Iteration 219 ] Training loss: 0.034548
DEBUG:root:[ Iteration 220 ] Test loss: 0.0520515
DEBUG:root:[ Iteration 222 ] Training loss: 0.047104
DEBUG:root:[ Iteration 225 ] Training loss: 0.0289563
DEBUG:root:[ Iteration 228 ] Training loss: 0.0345654
DEBUG:root:[ Iteration 231 ] Training loss: 0.028181
DEBUG:root:[ Iteration 234 ] Training loss: 0.0335365
DEBUG:root:[ Iteration 237 ] Training loss: 0.0501044
DEBUG:root:[ Iteration 240 ] Training loss: 0.0292053
DEBUG:root:[ Iteration 240 ] Test loss: 0.0367223
DEBUG:root:[ Iteration 243 ] Training loss: 0.0326912
DEBUG:root:[ Iteration 246 ] Training loss: 0.0373661
DEBUG:root:[ Iteration 249 ] Training loss: 0.0416496
DEBUG:root:[ Iteration 252 ] Training loss: 0.0301898
DEBUG:root:[ Iteration 255 ] Training loss: 0.033979
DEBUG:root:[ Iteration 258 ] Training loss: 0.0333023
DEBUG:root:[ Iteration 260 ] Test loss: 0.0360943
DEBUG:root:[ Iteration 261 ] Training loss: 0.0247093
DEBUG:root:[ Iteration 264 ] Training loss: 0.0290357
DEBUG:root:[ Iteration 267 ] Training loss: 0.0327707
DEBUG:root:[ Iteration 270 ] Training loss: 0.0398457
DEBUG:root:[ Iteration 273 ] Training loss: 0.0321159
DEBUG:root:[ Iteration 276 ] Training loss: 0.0402308
DEBUG:root:[ Iteration 279 ] Training loss: 0.0389345
DEBUG:root:[ Iteration 280 ] Test loss: 0.038067
DEBUG:root:[ Iteration 282 ] Training loss: 0.0319362
DEBUG:root:[ Iteration 285 ] Training loss: 0.0288917
DEBUG:root:[ Iteration 288 ] Training loss: 0.0314227
DEBUG:root:[ Iteration 291 ] Training loss: 0.0444028
DEBUG:root:[ Iteration 294 ] Training loss: 0.0284135
DEBUG:root:[ Iteration 297 ] Training loss: 0.0270718
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-27-2016_17h28m48s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0131695
DEBUG:root:[ Iteration 0 ] Test loss: 0.0205296
DEBUG:root:[ Iteration 3 ] Training loss: 0.0149998
DEBUG:root:[ Iteration 6 ] Training loss: 0.0152573
DEBUG:root:[ Iteration 9 ] Training loss: 0.0158843
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-28-2016_12h14m27s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0546677
DEBUG:root:[ Iteration 0 ] Test loss: 0.0615343
DEBUG:root:[ Iteration 3 ] Training loss: 0.0298627
DEBUG:root:[ Iteration 6 ] Training loss: 0.0499861
DEBUG:root:[ Iteration 9 ] Training loss: 0.0326184
DEBUG:root:[ Iteration 12 ] Training loss: 0.0295753
DEBUG:root:[ Iteration 15 ] Training loss: 0.0284456
DEBUG:root:[ Iteration 18 ] Training loss: 0.0324675
DEBUG:root:[ Iteration 20 ] Test loss: 0.0355379
DEBUG:root:[ Iteration 21 ] Training loss: 0.0331678
DEBUG:root:[ Iteration 24 ] Training loss: 0.0290082
DEBUG:root:[ Iteration 27 ] Training loss: 0.0270501
DEBUG:root:[ Iteration 30 ] Training loss: 0.02362
DEBUG:root:[ Iteration 33 ] Training loss: 0.0267994
DEBUG:root:[ Iteration 36 ] Training loss: 0.0314531
DEBUG:root:[ Iteration 39 ] Training loss: 0.0308181
DEBUG:root:[ Iteration 40 ] Test loss: 0.0281243
DEBUG:root:[ Iteration 42 ] Training loss: 0.0245541
DEBUG:root:[ Iteration 45 ] Training loss: 0.0229941
DEBUG:root:[ Iteration 48 ] Training loss: 0.0204825
DEBUG:root:[ Iteration 51 ] Training loss: 0.025022
DEBUG:root:[ Iteration 54 ] Training loss: 0.0243337
DEBUG:root:[ Iteration 57 ] Training loss: 0.0209944
DEBUG:root:[ Iteration 60 ] Training loss: 0.0215729
DEBUG:root:[ Iteration 60 ] Test loss: 0.0268793
DEBUG:root:[ Iteration 63 ] Training loss: 0.0208597
DEBUG:root:[ Iteration 66 ] Training loss: 0.019589
DEBUG:root:[ Iteration 69 ] Training loss: 0.0195133
DEBUG:root:[ Iteration 72 ] Training loss: 0.0223371
DEBUG:root:[ Iteration 75 ] Training loss: 0.0209346
DEBUG:root:[ Iteration 78 ] Training loss: 0.0211646
DEBUG:root:[ Iteration 80 ] Test loss: 0.0235954
DEBUG:root:[ Iteration 81 ] Training loss: 0.024149
DEBUG:root:[ Iteration 84 ] Training loss: 0.0234441
DEBUG:root:[ Iteration 87 ] Training loss: 0.0202384
DEBUG:root:[ Iteration 90 ] Training loss: 0.0198821
DEBUG:root:[ Iteration 93 ] Training loss: 0.019401
DEBUG:root:[ Iteration 96 ] Training loss: 0.0181421
DEBUG:root:[ Iteration 99 ] Training loss: 0.0151169
DEBUG:root:[ Iteration 100 ] Test loss: 0.0195753
DEBUG:root:[ Iteration 102 ] Training loss: 0.0176552
DEBUG:root:[ Iteration 105 ] Training loss: 0.0192483
DEBUG:root:[ Iteration 108 ] Training loss: 0.0224062
DEBUG:root:[ Iteration 111 ] Training loss: 0.0139648
DEBUG:root:[ Iteration 114 ] Training loss: 0.0164901
DEBUG:root:[ Iteration 117 ] Training loss: 0.0187122
DEBUG:root:[ Iteration 120 ] Training loss: 0.0190263
DEBUG:root:[ Iteration 120 ] Test loss: 0.0198431
DEBUG:root:[ Iteration 123 ] Training loss: 0.0160818
DEBUG:root:[ Iteration 126 ] Training loss: 0.016186
DEBUG:root:[ Iteration 129 ] Training loss: 0.0160522
DEBUG:root:[ Iteration 132 ] Training loss: 0.0143184
DEBUG:root:[ Iteration 135 ] Training loss: 0.0187356
DEBUG:root:[ Iteration 138 ] Training loss: 0.0160767
DEBUG:root:[ Iteration 140 ] Test loss: 0.01732
DEBUG:root:[ Iteration 141 ] Training loss: 0.0183531
DEBUG:root:[ Iteration 144 ] Training loss: 0.0176846
DEBUG:root:[ Iteration 147 ] Training loss: 0.0139486
DEBUG:root:[ Iteration 150 ] Training loss: 0.0165246
DEBUG:root:[ Iteration 153 ] Training loss: 0.0174735
DEBUG:root:[ Iteration 156 ] Training loss: 0.0142586
DEBUG:root:[ Iteration 159 ] Training loss: 0.0177648
DEBUG:root:[ Iteration 160 ] Test loss: 0.0170321
DEBUG:root:[ Iteration 162 ] Training loss: 0.0166674
DEBUG:root:[ Iteration 165 ] Training loss: 0.0157518
DEBUG:root:[ Iteration 168 ] Training loss: 0.0162748
DEBUG:root:[ Iteration 171 ] Training loss: 0.0193185
DEBUG:root:[ Iteration 174 ] Training loss: 0.0180709
DEBUG:root:[ Iteration 177 ] Training loss: 0.0148115
DEBUG:root:[ Iteration 180 ] Training loss: 0.0138822
DEBUG:root:[ Iteration 180 ] Test loss: 0.0179308
DEBUG:root:[ Iteration 183 ] Training loss: 0.0158375
DEBUG:root:[ Iteration 186 ] Training loss: 0.018199
DEBUG:root:[ Iteration 189 ] Training loss: 0.0123266
DEBUG:root:[ Iteration 192 ] Training loss: 0.0117773
DEBUG:root:[ Iteration 195 ] Training loss: 0.0169405
DEBUG:root:[ Iteration 198 ] Training loss: 0.0132185
DEBUG:root:[ Iteration 200 ] Test loss: 0.0153247
DEBUG:root:[ Iteration 201 ] Training loss: 0.0100398
DEBUG:root:[ Iteration 204 ] Training loss: 0.0119622
DEBUG:root:[ Iteration 207 ] Training loss: 0.0167687
DEBUG:root:[ Iteration 210 ] Training loss: 0.0154672
DEBUG:root:[ Iteration 213 ] Training loss: 0.0169686
DEBUG:root:[ Iteration 216 ] Training loss: 0.0139269
DEBUG:root:[ Iteration 219 ] Training loss: 0.0214051
DEBUG:root:[ Iteration 220 ] Test loss: 0.0172257
DEBUG:root:[ Iteration 222 ] Training loss: 0.0152099
DEBUG:root:[ Iteration 225 ] Training loss: 0.0136143
DEBUG:root:[ Iteration 228 ] Training loss: 0.0212661
DEBUG:root:[ Iteration 231 ] Training loss: 0.0128042
DEBUG:root:[ Iteration 234 ] Training loss: 0.0125197
DEBUG:root:[ Iteration 237 ] Training loss: 0.0200794
DEBUG:root:[ Iteration 240 ] Training loss: 0.0138049
DEBUG:root:[ Iteration 240 ] Test loss: 0.0196128
DEBUG:root:[ Iteration 243 ] Training loss: 0.0144304
DEBUG:root:[ Iteration 246 ] Training loss: 0.011582
DEBUG:root:[ Iteration 249 ] Training loss: 0.0131626
DEBUG:root:[ Iteration 252 ] Training loss: 0.0113071
DEBUG:root:[ Iteration 255 ] Training loss: 0.0143151
DEBUG:root:[ Iteration 258 ] Training loss: 0.011035
DEBUG:root:[ Iteration 260 ] Test loss: 0.0170645
DEBUG:root:[ Iteration 261 ] Training loss: 0.0112502
DEBUG:root:[ Iteration 264 ] Training loss: 0.0112403
DEBUG:root:[ Iteration 267 ] Training loss: 0.0145536
DEBUG:root:[ Iteration 270 ] Training loss: 0.017662
DEBUG:root:[ Iteration 273 ] Training loss: 0.0136244
DEBUG:root:[ Iteration 276 ] Training loss: 0.0132548
DEBUG:root:[ Iteration 279 ] Training loss: 0.0154543
DEBUG:root:[ Iteration 280 ] Test loss: 0.0143592
DEBUG:root:[ Iteration 282 ] Training loss: 0.0123836
DEBUG:root:[ Iteration 285 ] Training loss: 0.0125982
DEBUG:root:[ Iteration 288 ] Training loss: 0.0111404
DEBUG:root:[ Iteration 291 ] Training loss: 0.0119754
DEBUG:root:[ Iteration 294 ] Training loss: 0.0141463
DEBUG:root:[ Iteration 297 ] Training loss: 0.0129044
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-28-2016_12h22m03s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0108321
DEBUG:root:[ Iteration 0 ] Test loss: 0.0181895
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-28-2016_14h08m30s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.022956
DEBUG:root:[ Iteration 0 ] Test loss: 0.0164656
DEBUG:root:[ Iteration 3 ] Training loss: 0.0279833
DEBUG:root:[ Iteration 6 ] Training loss: 0.0187193
DEBUG:root:[ Iteration 9 ] Training loss: 0.0166312
DEBUG:root:[ Iteration 12 ] Training loss: 0.0184106
DEBUG:root:[ Iteration 15 ] Training loss: 0.0240697
DEBUG:root:[ Iteration 18 ] Training loss: 0.0232534
DEBUG:root:[ Iteration 20 ] Test loss: 0.0185547
DEBUG:root:[ Iteration 21 ] Training loss: 0.0155215
DEBUG:root:[ Iteration 24 ] Training loss: 0.0173943
DEBUG:root:[ Iteration 27 ] Training loss: 0.0182478
DEBUG:root:[ Iteration 30 ] Training loss: 0.018198
DEBUG:root:[ Iteration 33 ] Training loss: 0.0161618
DEBUG:root:[ Iteration 36 ] Training loss: 0.0163284
DEBUG:root:[ Iteration 39 ] Training loss: 0.0172736
DEBUG:root:[ Iteration 40 ] Test loss: 0.0211207
DEBUG:root:[ Iteration 42 ] Training loss: 0.0129195
DEBUG:root:[ Iteration 45 ] Training loss: 0.0148493
DEBUG:root:[ Iteration 48 ] Training loss: 0.0182865
DEBUG:root:[ Iteration 51 ] Training loss: 0.0186306
DEBUG:root:[ Iteration 54 ] Training loss: 0.0188093
DEBUG:root:[ Iteration 57 ] Training loss: 0.0145855
DEBUG:root:[ Iteration 60 ] Training loss: 0.0197159
DEBUG:root:[ Iteration 60 ] Test loss: 0.0199622
DEBUG:root:[ Iteration 63 ] Training loss: 0.0191929
DEBUG:root:[ Iteration 66 ] Training loss: 0.0172494
DEBUG:root:[ Iteration 69 ] Training loss: 0.014509
DEBUG:root:[ Iteration 72 ] Training loss: 0.0187386
DEBUG:root:[ Iteration 75 ] Training loss: 0.0122197
DEBUG:root:[ Iteration 78 ] Training loss: 0.0160621
DEBUG:root:[ Iteration 80 ] Test loss: 0.0198341
DEBUG:root:[ Iteration 81 ] Training loss: 0.017208
DEBUG:root:[ Iteration 84 ] Training loss: 0.0137634
DEBUG:root:[ Iteration 87 ] Training loss: 0.0147493
DEBUG:root:[ Iteration 90 ] Training loss: 0.0155091
DEBUG:root:[ Iteration 93 ] Training loss: 0.0141121
DEBUG:root:[ Iteration 96 ] Training loss: 0.0167122
DEBUG:root:[ Iteration 99 ] Training loss: 0.0166458
DEBUG:root:[ Iteration 100 ] Test loss: 0.0181334
DEBUG:root:[ Iteration 102 ] Training loss: 0.0190712
DEBUG:root:[ Iteration 105 ] Training loss: 0.0173375
DEBUG:root:[ Iteration 108 ] Training loss: 0.0161952
DEBUG:root:[ Iteration 111 ] Training loss: 0.0227336
DEBUG:root:[ Iteration 114 ] Training loss: 0.0164165
DEBUG:root:[ Iteration 117 ] Training loss: 0.0182663
DEBUG:root:[ Iteration 120 ] Training loss: 0.0135992
DEBUG:root:[ Iteration 120 ] Test loss: 0.0194733
DEBUG:root:[ Iteration 123 ] Training loss: 0.0153881
DEBUG:root:[ Iteration 126 ] Training loss: 0.0176843
DEBUG:root:[ Iteration 129 ] Training loss: 0.0159612
DEBUG:root:[ Iteration 132 ] Training loss: 0.0221283
DEBUG:root:[ Iteration 135 ] Training loss: 0.0166974
DEBUG:root:[ Iteration 138 ] Training loss: 0.0191841
DEBUG:root:[ Iteration 140 ] Test loss: 0.0168235
DEBUG:root:[ Iteration 141 ] Training loss: 0.0169763
DEBUG:root:[ Iteration 144 ] Training loss: 0.0173157
DEBUG:root:[ Iteration 147 ] Training loss: 0.0160057
DEBUG:root:[ Iteration 150 ] Training loss: 0.0134767
DEBUG:root:[ Iteration 153 ] Training loss: 0.014284
DEBUG:root:[ Iteration 156 ] Training loss: 0.0112273
DEBUG:root:[ Iteration 159 ] Training loss: 0.0152084
DEBUG:root:[ Iteration 160 ] Test loss: 0.0164922
DEBUG:root:[ Iteration 162 ] Training loss: 0.00939901
DEBUG:root:[ Iteration 165 ] Training loss: 0.0133378
DEBUG:root:[ Iteration 168 ] Training loss: 0.0113258
DEBUG:root:[ Iteration 171 ] Training loss: 0.0133784
DEBUG:root:[ Iteration 174 ] Training loss: 0.0139496
DEBUG:root:[ Iteration 177 ] Training loss: 0.0148943
DEBUG:root:[ Iteration 180 ] Training loss: 0.0151117
DEBUG:root:[ Iteration 180 ] Test loss: 0.0184551
DEBUG:root:[ Iteration 183 ] Training loss: 0.0189994
DEBUG:root:[ Iteration 186 ] Training loss: 0.0163438
DEBUG:root:[ Iteration 189 ] Training loss: 0.0118632
DEBUG:root:[ Iteration 192 ] Training loss: 0.0146005
DEBUG:root:[ Iteration 195 ] Training loss: 0.0124791
DEBUG:root:[ Iteration 198 ] Training loss: 0.0112879
DEBUG:root:[ Iteration 200 ] Test loss: 0.0138006
DEBUG:root:[ Iteration 201 ] Training loss: 0.0168466
DEBUG:root:[ Iteration 204 ] Training loss: 0.0133095
DEBUG:root:[ Iteration 207 ] Training loss: 0.0108432
DEBUG:root:[ Iteration 210 ] Training loss: 0.0144228
DEBUG:root:[ Iteration 213 ] Training loss: 0.0127857
DEBUG:root:[ Iteration 216 ] Training loss: 0.0108295
DEBUG:root:[ Iteration 219 ] Training loss: 0.0128315
DEBUG:root:[ Iteration 220 ] Test loss: 0.0114164
DEBUG:root:[ Iteration 222 ] Training loss: 0.0176956
DEBUG:root:[ Iteration 225 ] Training loss: 0.0151119
DEBUG:root:[ Iteration 228 ] Training loss: 0.0167808
DEBUG:root:[ Iteration 231 ] Training loss: 0.0172432
DEBUG:root:[ Iteration 234 ] Training loss: 0.0107024
DEBUG:root:[ Iteration 237 ] Training loss: 0.011781
DEBUG:root:[ Iteration 240 ] Training loss: 0.0155147
DEBUG:root:[ Iteration 240 ] Test loss: 0.0167468
DEBUG:root:[ Iteration 243 ] Training loss: 0.0164105
DEBUG:root:[ Iteration 246 ] Training loss: 0.0150335
DEBUG:root:[ Iteration 249 ] Training loss: 0.0133039
DEBUG:root:[ Iteration 252 ] Training loss: 0.0133689
DEBUG:root:[ Iteration 255 ] Training loss: 0.00942993
DEBUG:root:[ Iteration 258 ] Training loss: 0.0119644
DEBUG:root:[ Iteration 260 ] Test loss: 0.0149935
DEBUG:root:[ Iteration 261 ] Training loss: 0.0112192
DEBUG:root:[ Iteration 264 ] Training loss: 0.0134438
DEBUG:root:[ Iteration 267 ] Training loss: 0.00980147
DEBUG:root:[ Iteration 270 ] Training loss: 0.0126483
DEBUG:root:[ Iteration 273 ] Training loss: 0.0157371
DEBUG:root:[ Iteration 276 ] Training loss: 0.0126115
DEBUG:root:[ Iteration 279 ] Training loss: 0.0122921
DEBUG:root:[ Iteration 280 ] Test loss: 0.0172129
DEBUG:root:[ Iteration 282 ] Training loss: 0.0122648
DEBUG:root:[ Iteration 285 ] Training loss: 0.0167047
DEBUG:root:[ Iteration 288 ] Training loss: 0.0164128
DEBUG:root:[ Iteration 291 ] Training loss: 0.0132495
DEBUG:root:[ Iteration 294 ] Training loss: 0.0132708
DEBUG:root:[ Iteration 297 ] Training loss: 0.0112814
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-28-2016_14h16m01s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.190123
DEBUG:root:[ Iteration 0 ] Test loss: 0.19015
DEBUG:root:[ Iteration 3 ] Training loss: 0.177425
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-29-2016_11h25m33s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.181004
DEBUG:root:[ Iteration 0 ] Test loss: 0.182612
DEBUG:root:[ Iteration 3 ] Training loss: 0.17949
DEBUG:root:[ Iteration 6 ] Training loss: 0.154341
DEBUG:root:[ Iteration 9 ] Training loss: 0.0646667
DEBUG:root:[ Iteration 12 ] Training loss: 0.0564318
DEBUG:root:[ Iteration 15 ] Training loss: 0.0364395
DEBUG:root:[ Iteration 18 ] Training loss: 0.0475107
DEBUG:root:[ Iteration 20 ] Test loss: 0.0514724
DEBUG:root:[ Iteration 21 ] Training loss: 0.0256859
DEBUG:root:[ Iteration 24 ] Training loss: 0.0318025
DEBUG:root:[ Iteration 27 ] Training loss: 0.0305943
DEBUG:root:[ Iteration 30 ] Training loss: 0.0174594
DEBUG:root:[ Iteration 33 ] Training loss: 0.0129946
DEBUG:root:[ Iteration 36 ] Training loss: 0.0118223
DEBUG:root:[ Iteration 39 ] Training loss: 0.010744
DEBUG:root:[ Iteration 40 ] Test loss: 0.0110133
DEBUG:root:[ Iteration 42 ] Training loss: 0.00864179
DEBUG:root:[ Iteration 45 ] Training loss: 0.00767312
DEBUG:root:[ Iteration 48 ] Training loss: 0.00852083
DEBUG:root:[ Iteration 51 ] Training loss: 0.00752235
DEBUG:root:[ Iteration 54 ] Training loss: 0.00754864
DEBUG:root:[ Iteration 57 ] Training loss: 0.00538166
DEBUG:root:[ Iteration 60 ] Training loss: 0.0068993
DEBUG:root:[ Iteration 60 ] Test loss: 0.00601261
DEBUG:root:[ Iteration 63 ] Training loss: 0.00582859
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-29-2016_12h23m01s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.186806
DEBUG:root:[ Iteration 0 ] Test loss: 0.18565
DEBUG:root:[ Iteration 3 ] Training loss: 0.186617
DEBUG:root:[ Iteration 6 ] Training loss: 0.183914
DEBUG:root:[ Iteration 9 ] Training loss: 0.186242
DEBUG:root:[ Iteration 12 ] Training loss: 0.180663
DEBUG:root:[ Iteration 15 ] Training loss: 0.162656
DEBUG:root:[ Iteration 18 ] Training loss: 0.164976
DEBUG:root:[ Iteration 20 ] Test loss: 0.175766
DEBUG:root:[ Iteration 21 ] Training loss: 0.163579
DEBUG:root:[ Iteration 24 ] Training loss: 0.156581
DEBUG:root:[ Iteration 27 ] Training loss: 0.157286
DEBUG:root:[ Iteration 30 ] Training loss: 0.158448
DEBUG:root:[ Iteration 33 ] Training loss: 0.140528
DEBUG:root:[ Iteration 36 ] Training loss: 0.139232
DEBUG:root:[ Iteration 39 ] Training loss: 0.121602
DEBUG:root:[ Iteration 40 ] Test loss: 0.120194
DEBUG:root:[ Iteration 42 ] Training loss: 0.105191
DEBUG:root:[ Iteration 45 ] Training loss: 0.100503
DEBUG:root:[ Iteration 48 ] Training loss: 0.0882536
DEBUG:root:[ Iteration 51 ] Training loss: 0.0738778
DEBUG:root:[ Iteration 54 ] Training loss: 0.0767789
DEBUG:root:[ Iteration 57 ] Training loss: 0.0792054
DEBUG:root:[ Iteration 60 ] Training loss: 0.0769035
DEBUG:root:[ Iteration 60 ] Test loss: 0.102672
DEBUG:root:[ Iteration 63 ] Training loss: 0.0796831
DEBUG:root:[ Iteration 66 ] Training loss: 0.0708117
DEBUG:root:[ Iteration 69 ] Training loss: 0.0630703
DEBUG:root:[ Iteration 72 ] Training loss: 0.114717
DEBUG:root:[ Iteration 75 ] Training loss: 0.0667498
DEBUG:root:[ Iteration 78 ] Training loss: 0.0709804
DEBUG:root:[ Iteration 80 ] Test loss: 0.0641676
DEBUG:root:[ Iteration 81 ] Training loss: 0.0579358
DEBUG:root:[ Iteration 84 ] Training loss: 0.0617543
DEBUG:root:[ Iteration 87 ] Training loss: 0.0568728
DEBUG:root:[ Iteration 90 ] Training loss: 0.0512206
DEBUG:root:[ Iteration 93 ] Training loss: 0.0600225
DEBUG:root:[ Iteration 96 ] Training loss: 0.0498682
DEBUG:root:[ Iteration 99 ] Training loss: 0.058053
DEBUG:root:[ Iteration 100 ] Test loss: 0.0531893
DEBUG:root:[ Iteration 102 ] Training loss: 0.0442589
DEBUG:root:[ Iteration 105 ] Training loss: 0.053259
DEBUG:root:[ Iteration 108 ] Training loss: 0.0621839
DEBUG:root:[ Iteration 111 ] Training loss: 0.0426146
DEBUG:root:[ Iteration 114 ] Training loss: 0.0495606
DEBUG:root:[ Iteration 117 ] Training loss: 0.0405897
DEBUG:root:[ Iteration 120 ] Training loss: 0.0472709
DEBUG:root:[ Iteration 120 ] Test loss: 0.0580637
DEBUG:root:[ Iteration 123 ] Training loss: 0.0508074
DEBUG:root:[ Iteration 126 ] Training loss: 0.051842
DEBUG:root:[ Iteration 129 ] Training loss: 0.0372948
DEBUG:root:[ Iteration 132 ] Training loss: 0.0551717
DEBUG:root:[ Iteration 135 ] Training loss: 0.035127
DEBUG:root:[ Iteration 138 ] Training loss: 0.0496263
DEBUG:root:[ Iteration 140 ] Test loss: 0.0639518
DEBUG:root:[ Iteration 141 ] Training loss: 0.0564045
DEBUG:root:[ Iteration 144 ] Training loss: 0.0443382
DEBUG:root:[ Iteration 147 ] Training loss: 0.0403258
DEBUG:root:[ Iteration 150 ] Training loss: 0.0354993
DEBUG:root:[ Iteration 153 ] Training loss: 0.0391996
DEBUG:root:[ Iteration 156 ] Training loss: 0.0353533
DEBUG:root:[ Iteration 159 ] Training loss: 0.0290132
DEBUG:root:[ Iteration 160 ] Test loss: 0.0468364
DEBUG:root:[ Iteration 162 ] Training loss: 0.0347609
DEBUG:root:[ Iteration 165 ] Training loss: 0.0403981
DEBUG:root:[ Iteration 168 ] Training loss: 0.037562
DEBUG:root:[ Iteration 171 ] Training loss: 0.0358124
DEBUG:root:[ Iteration 174 ] Training loss: 0.0347614
DEBUG:root:[ Iteration 177 ] Training loss: 0.0261244
DEBUG:root:[ Iteration 180 ] Training loss: 0.0309751
DEBUG:root:[ Iteration 180 ] Test loss: 0.0509777
DEBUG:root:[ Iteration 183 ] Training loss: 0.0378648
DEBUG:root:[ Iteration 186 ] Training loss: 0.0326045
DEBUG:root:[ Iteration 189 ] Training loss: 0.0399591
DEBUG:root:[ Iteration 192 ] Training loss: 0.0309474
DEBUG:root:[ Iteration 195 ] Training loss: 0.0447677
DEBUG:root:[ Iteration 198 ] Training loss: 0.0577897
DEBUG:root:[ Iteration 200 ] Test loss: 0.0560835
DEBUG:root:[ Iteration 201 ] Training loss: 0.0436473
DEBUG:root:[ Iteration 204 ] Training loss: 0.0472854
DEBUG:root:[ Iteration 207 ] Training loss: 0.0420459
DEBUG:root:[ Iteration 210 ] Training loss: 0.0323545
DEBUG:root:[ Iteration 213 ] Training loss: 0.0305198
DEBUG:root:[ Iteration 216 ] Training loss: 0.0283647
DEBUG:root:[ Iteration 219 ] Training loss: 0.0241106
DEBUG:root:[ Iteration 220 ] Test loss: 0.0475472
DEBUG:root:[ Iteration 222 ] Training loss: 0.04694
DEBUG:root:[ Iteration 225 ] Training loss: 0.0301939
DEBUG:root:[ Iteration 228 ] Training loss: 0.0349138
DEBUG:root:[ Iteration 231 ] Training loss: 0.0419096
DEBUG:root:[ Iteration 234 ] Training loss: 0.0311457
DEBUG:root:[ Iteration 237 ] Training loss: 0.0277223
DEBUG:root:[ Iteration 240 ] Training loss: 0.021528
DEBUG:root:[ Iteration 240 ] Test loss: 0.0439961
DEBUG:root:[ Iteration 243 ] Training loss: 0.0285079
DEBUG:root:[ Iteration 246 ] Training loss: 0.0271057
DEBUG:root:[ Iteration 249 ] Training loss: 0.0220355
DEBUG:root:[ Iteration 252 ] Training loss: 0.0249274
DEBUG:root:[ Iteration 255 ] Training loss: 0.0277073
DEBUG:root:[ Iteration 258 ] Training loss: 0.0286629
DEBUG:root:[ Iteration 260 ] Test loss: 0.0369202
DEBUG:root:[ Iteration 261 ] Training loss: 0.0234791
DEBUG:root:[ Iteration 264 ] Training loss: 0.0180108
DEBUG:root:[ Iteration 267 ] Training loss: 0.0236738
DEBUG:root:[ Iteration 270 ] Training loss: 0.0128975
DEBUG:root:[ Iteration 273 ] Training loss: 0.0302839
DEBUG:root:[ Iteration 276 ] Training loss: 0.0227912
DEBUG:root:[ Iteration 279 ] Training loss: 0.0252283
DEBUG:root:[ Iteration 280 ] Test loss: 0.0257474
DEBUG:root:[ Iteration 282 ] Training loss: 0.0271382
DEBUG:root:[ Iteration 285 ] Training loss: 0.0231103
DEBUG:root:[ Iteration 288 ] Training loss: 0.0313089
DEBUG:root:[ Iteration 291 ] Training loss: 0.0284883
DEBUG:root:[ Iteration 294 ] Training loss: 0.029762
DEBUG:root:[ Iteration 297 ] Training loss: 0.0288593
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-29-2016_14h27m56s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.195551
DEBUG:root:[ Iteration 0 ] Test loss: 0.200386
DEBUG:root:[ Iteration 3 ] Training loss: 0.192913
DEBUG:root:[ Iteration 6 ] Training loss: 0.203824
DEBUG:root:[ Iteration 9 ] Training loss: 0.196106
DEBUG:root:[ Iteration 12 ] Training loss: 0.204491
DEBUG:root:[ Iteration 15 ] Training loss: 0.194402
DEBUG:root:[ Iteration 18 ] Training loss: 0.194365
DEBUG:root:[ Iteration 20 ] Test loss: 0.19659
DEBUG:root:[ Iteration 21 ] Training loss: 0.197333
DEBUG:root:[ Iteration 24 ] Training loss: 0.188165
DEBUG:root:[ Iteration 27 ] Training loss: 0.201519
DEBUG:root:[ Iteration 30 ] Training loss: 0.19347
DEBUG:root:[ Iteration 33 ] Training loss: 0.19247
DEBUG:root:[ Iteration 36 ] Training loss: 0.191686
DEBUG:root:[ Iteration 39 ] Training loss: 0.194545
DEBUG:root:[ Iteration 40 ] Test loss: 0.196999
DEBUG:root:[ Iteration 42 ] Training loss: 0.184435
DEBUG:root:[ Iteration 45 ] Training loss: 0.174104
DEBUG:root:[ Iteration 48 ] Training loss: 0.176025
DEBUG:root:[ Iteration 51 ] Training loss: 0.164854
DEBUG:root:[ Iteration 54 ] Training loss: 0.155801
DEBUG:root:[ Iteration 57 ] Training loss: 0.160168
DEBUG:root:[ Iteration 60 ] Training loss: 0.144394
DEBUG:root:[ Iteration 60 ] Test loss: 0.148885
DEBUG:root:[ Iteration 63 ] Training loss: 0.131467
DEBUG:root:[ Iteration 66 ] Training loss: 0.132124
DEBUG:root:[ Iteration 69 ] Training loss: 0.12746
DEBUG:root:[ Iteration 72 ] Training loss: 0.109478
DEBUG:root:[ Iteration 75 ] Training loss: 0.100116
DEBUG:root:[ Iteration 78 ] Training loss: 0.0978699
DEBUG:root:[ Iteration 80 ] Test loss: 0.104703
DEBUG:root:[ Iteration 81 ] Training loss: 0.0952626
DEBUG:root:[ Iteration 84 ] Training loss: 0.102639
DEBUG:root:[ Iteration 87 ] Training loss: 0.0893779
DEBUG:root:[ Iteration 90 ] Training loss: 0.105738
DEBUG:root:[ Iteration 93 ] Training loss: 0.0972913
DEBUG:root:[ Iteration 96 ] Training loss: 0.0894862
DEBUG:root:[ Iteration 99 ] Training loss: 0.0896051
DEBUG:root:[ Iteration 100 ] Test loss: 0.0935939
DEBUG:root:[ Iteration 102 ] Training loss: 0.0827143
DEBUG:root:[ Iteration 105 ] Training loss: 0.100366
DEBUG:root:[ Iteration 108 ] Training loss: 0.079535
DEBUG:root:[ Iteration 111 ] Training loss: 0.075175
DEBUG:root:[ Iteration 114 ] Training loss: 0.0877624
DEBUG:root:[ Iteration 117 ] Training loss: 0.0742967
DEBUG:root:[ Iteration 120 ] Training loss: 0.0755171
DEBUG:root:[ Iteration 120 ] Test loss: 0.0667637
DEBUG:root:[ Iteration 123 ] Training loss: 0.0825844
DEBUG:root:[ Iteration 126 ] Training loss: 0.0969289
DEBUG:root:[ Iteration 129 ] Training loss: 0.0701617
DEBUG:root:[ Iteration 132 ] Training loss: 0.0688546
DEBUG:root:[ Iteration 135 ] Training loss: 0.0745622
DEBUG:root:[ Iteration 138 ] Training loss: 0.0598683
DEBUG:root:[ Iteration 140 ] Test loss: 0.0651723
DEBUG:root:[ Iteration 141 ] Training loss: 0.0606692
DEBUG:root:[ Iteration 144 ] Training loss: 0.0664224
DEBUG:root:[ Iteration 147 ] Training loss: 0.0715447
DEBUG:root:[ Iteration 150 ] Training loss: 0.0744499
DEBUG:root:[ Iteration 153 ] Training loss: 0.0639986
DEBUG:root:[ Iteration 156 ] Training loss: 0.0451133
DEBUG:root:[ Iteration 159 ] Training loss: 0.0506207
DEBUG:root:[ Iteration 160 ] Test loss: 0.062721
DEBUG:root:[ Iteration 162 ] Training loss: 0.0638353
DEBUG:root:[ Iteration 165 ] Training loss: 0.0610335
DEBUG:root:[ Iteration 168 ] Training loss: 0.0568071
DEBUG:root:[ Iteration 171 ] Training loss: 0.0704267
DEBUG:root:[ Iteration 174 ] Training loss: 0.043775
DEBUG:root:[ Iteration 177 ] Training loss: 0.0417295
DEBUG:root:[ Iteration 180 ] Training loss: 0.042915
DEBUG:root:[ Iteration 180 ] Test loss: 0.0428942
DEBUG:root:[ Iteration 183 ] Training loss: 0.0560837
DEBUG:root:[ Iteration 186 ] Training loss: 0.0503566
DEBUG:root:[ Iteration 189 ] Training loss: 0.0459234
DEBUG:root:[ Iteration 192 ] Training loss: 0.05314
DEBUG:root:[ Iteration 195 ] Training loss: 0.0452185
DEBUG:root:[ Iteration 198 ] Training loss: 0.0425732
DEBUG:root:[ Iteration 200 ] Test loss: 0.0839601
DEBUG:root:[ Iteration 201 ] Training loss: 0.0763391
DEBUG:root:[ Iteration 204 ] Training loss: 0.0604231
DEBUG:root:[ Iteration 207 ] Training loss: 0.0530354
DEBUG:root:[ Iteration 210 ] Training loss: 0.0467517
DEBUG:root:[ Iteration 213 ] Training loss: 0.0433581
DEBUG:root:[ Iteration 216 ] Training loss: 0.0398915
DEBUG:root:[ Iteration 219 ] Training loss: 0.0517148
DEBUG:root:[ Iteration 220 ] Test loss: 0.0438926
DEBUG:root:[ Iteration 222 ] Training loss: 0.0383441
DEBUG:root:[ Iteration 225 ] Training loss: 0.0487162
DEBUG:root:[ Iteration 228 ] Training loss: 0.0497184
DEBUG:root:[ Iteration 231 ] Training loss: 0.0417805
DEBUG:root:[ Iteration 234 ] Training loss: 0.0449644
DEBUG:root:[ Iteration 237 ] Training loss: 0.0511835
DEBUG:root:[ Iteration 240 ] Training loss: 0.0394007
DEBUG:root:[ Iteration 240 ] Test loss: 0.0481075
DEBUG:root:[ Iteration 243 ] Training loss: 0.038139
DEBUG:root:[ Iteration 246 ] Training loss: 0.0375517
DEBUG:root:[ Iteration 249 ] Training loss: 0.0450812
DEBUG:root:[ Iteration 252 ] Training loss: 0.0396052
DEBUG:root:[ Iteration 255 ] Training loss: 0.0338757
DEBUG:root:[ Iteration 258 ] Training loss: 0.0426878
DEBUG:root:[ Iteration 260 ] Test loss: 0.056616
DEBUG:root:[ Iteration 261 ] Training loss: 0.0349944
DEBUG:root:[ Iteration 264 ] Training loss: 0.038299
DEBUG:root:[ Iteration 267 ] Training loss: 0.030889
DEBUG:root:[ Iteration 270 ] Training loss: 0.0397262
DEBUG:root:[ Iteration 273 ] Training loss: 0.0318554
DEBUG:root:[ Iteration 276 ] Training loss: 0.0457207
DEBUG:root:[ Iteration 279 ] Training loss: 0.0421376
DEBUG:root:[ Iteration 280 ] Test loss: 0.0336539
DEBUG:root:[ Iteration 282 ] Training loss: 0.0338014
DEBUG:root:[ Iteration 285 ] Training loss: 0.0426707
DEBUG:root:[ Iteration 288 ] Training loss: 0.0363741
DEBUG:root:[ Iteration 291 ] Training loss: 0.0390086
DEBUG:root:[ Iteration 294 ] Training loss: 0.0614263
DEBUG:root:[ Iteration 297 ] Training loss: 0.0355698
DEBUG:root:[ Iteration 300 ] Training loss: 0.035104
DEBUG:root:[ Iteration 300 ] Test loss: 0.0389163
DEBUG:root:[ Iteration 303 ] Training loss: 0.0483815
DEBUG:root:[ Iteration 306 ] Training loss: 0.043076
DEBUG:root:[ Iteration 309 ] Training loss: 0.0329314
DEBUG:root:[ Iteration 312 ] Training loss: 0.0300436
DEBUG:root:[ Iteration 315 ] Training loss: 0.0332586
DEBUG:root:[ Iteration 318 ] Training loss: 0.0319651
DEBUG:root:[ Iteration 320 ] Test loss: 0.0354526
DEBUG:root:[ Iteration 321 ] Training loss: 0.0295205
DEBUG:root:[ Iteration 324 ] Training loss: 0.0333817
DEBUG:root:[ Iteration 327 ] Training loss: 0.0293077
DEBUG:root:[ Iteration 330 ] Training loss: 0.0267547
DEBUG:root:[ Iteration 333 ] Training loss: 0.0368778
DEBUG:root:[ Iteration 336 ] Training loss: 0.0315115
DEBUG:root:[ Iteration 339 ] Training loss: 0.0285326
DEBUG:root:[ Iteration 340 ] Test loss: 0.0378786
DEBUG:root:[ Iteration 342 ] Training loss: 0.043527
DEBUG:root:[ Iteration 345 ] Training loss: 0.0278618
DEBUG:root:[ Iteration 348 ] Training loss: 0.0475995
DEBUG:root:[ Iteration 351 ] Training loss: 0.0369414
DEBUG:root:[ Iteration 354 ] Training loss: 0.0362406
DEBUG:root:[ Iteration 357 ] Training loss: 0.0293231
DEBUG:root:[ Iteration 360 ] Training loss: 0.0305124
DEBUG:root:[ Iteration 360 ] Test loss: 0.0398515
DEBUG:root:[ Iteration 363 ] Training loss: 0.0261163
DEBUG:root:[ Iteration 366 ] Training loss: 0.0416178
DEBUG:root:[ Iteration 369 ] Training loss: 0.0243284
DEBUG:root:[ Iteration 372 ] Training loss: 0.0243691
DEBUG:root:[ Iteration 375 ] Training loss: 0.0241473
DEBUG:root:[ Iteration 378 ] Training loss: 0.023375
DEBUG:root:[ Iteration 380 ] Test loss: 0.0330924
DEBUG:root:[ Iteration 381 ] Training loss: 0.0381711
DEBUG:root:[ Iteration 384 ] Training loss: 0.0506214
DEBUG:root:[ Iteration 387 ] Training loss: 0.022294
DEBUG:root:[ Iteration 390 ] Training loss: 0.0285403
DEBUG:root:[ Iteration 393 ] Training loss: 0.0470634
DEBUG:root:[ Iteration 396 ] Training loss: 0.0283596
DEBUG:root:[ Iteration 399 ] Training loss: 0.0474858
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-29-2016_15h46m42s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.201711
DEBUG:root:[ Iteration 0 ] Test loss: 0.210477
DEBUG:root:[ Iteration 3 ] Training loss: 0.202988
DEBUG:root:[ Iteration 6 ] Training loss: 0.202043
DEBUG:root:[ Iteration 9 ] Training loss: 0.198911
DEBUG:root:[ Iteration 12 ] Training loss: 0.19492
DEBUG:root:[ Iteration 15 ] Training loss: 0.193646
DEBUG:root:[ Iteration 18 ] Training loss: 0.20693
DEBUG:root:[ Iteration 20 ] Test loss: 0.206867
DEBUG:root:[ Iteration 21 ] Training loss: 0.200785
DEBUG:root:[ Iteration 24 ] Training loss: 0.201304
DEBUG:root:[ Iteration 27 ] Training loss: 0.187985
DEBUG:root:[ Iteration 30 ] Training loss: 0.192008
DEBUG:root:[ Iteration 33 ] Training loss: 0.195018
DEBUG:root:[ Iteration 36 ] Training loss: 0.196113
DEBUG:root:[ Iteration 39 ] Training loss: 0.195795
DEBUG:root:[ Iteration 40 ] Test loss: 0.202013
DEBUG:root:[ Iteration 42 ] Training loss: 0.194519
DEBUG:root:[ Iteration 45 ] Training loss: 0.197812
DEBUG:root:[ Iteration 48 ] Training loss: 0.188444
DEBUG:root:[ Iteration 51 ] Training loss: 0.181706
DEBUG:root:[ Iteration 54 ] Training loss: 0.184363
DEBUG:root:[ Iteration 57 ] Training loss: 0.169859
DEBUG:root:[ Iteration 60 ] Training loss: 0.17508
DEBUG:root:[ Iteration 60 ] Test loss: 0.170325
DEBUG:root:[ Iteration 63 ] Training loss: 0.15537
DEBUG:root:[ Iteration 66 ] Training loss: 0.154539
DEBUG:root:[ Iteration 69 ] Training loss: 0.149067
DEBUG:root:[ Iteration 72 ] Training loss: 0.137292
DEBUG:root:[ Iteration 75 ] Training loss: 0.137427
DEBUG:root:[ Iteration 78 ] Training loss: 0.125263
DEBUG:root:[ Iteration 80 ] Test loss: 0.138498
DEBUG:root:[ Iteration 81 ] Training loss: 0.141255
DEBUG:root:[ Iteration 84 ] Training loss: 0.133559
DEBUG:root:[ Iteration 87 ] Training loss: 0.126123
DEBUG:root:[ Iteration 90 ] Training loss: 0.126146
DEBUG:root:[ Iteration 93 ] Training loss: 0.123103
DEBUG:root:[ Iteration 96 ] Training loss: 0.119548
DEBUG:root:[ Iteration 99 ] Training loss: 0.10903
DEBUG:root:[ Iteration 100 ] Test loss: 0.114604
DEBUG:root:[ Iteration 102 ] Training loss: 0.116451
DEBUG:root:[ Iteration 105 ] Training loss: 0.143661
DEBUG:root:[ Iteration 108 ] Training loss: 0.11483
DEBUG:root:[ Iteration 111 ] Training loss: 0.101427
DEBUG:root:[ Iteration 114 ] Training loss: 0.118645
DEBUG:root:[ Iteration 117 ] Training loss: 0.0999819
DEBUG:root:[ Iteration 120 ] Training loss: 0.091281
DEBUG:root:[ Iteration 120 ] Test loss: 0.0976713
DEBUG:root:[ Iteration 123 ] Training loss: 0.10609
DEBUG:root:[ Iteration 126 ] Training loss: 0.114453
DEBUG:root:[ Iteration 129 ] Training loss: 0.0816016
DEBUG:root:[ Iteration 132 ] Training loss: 0.102253
DEBUG:root:[ Iteration 135 ] Training loss: 0.0852477
DEBUG:root:[ Iteration 138 ] Training loss: 0.0931399
DEBUG:root:[ Iteration 140 ] Test loss: 0.0969181
DEBUG:root:[ Iteration 141 ] Training loss: 0.0946802
DEBUG:root:[ Iteration 144 ] Training loss: 0.0910857
DEBUG:root:[ Iteration 147 ] Training loss: 0.0731345
DEBUG:root:[ Iteration 150 ] Training loss: 0.0770634
DEBUG:root:[ Iteration 153 ] Training loss: 0.0768498
DEBUG:root:[ Iteration 156 ] Training loss: 0.0753469
DEBUG:root:[ Iteration 159 ] Training loss: 0.0705205
DEBUG:root:[ Iteration 160 ] Test loss: 0.0691123
DEBUG:root:[ Iteration 162 ] Training loss: 0.0728493
DEBUG:root:[ Iteration 165 ] Training loss: 0.0871857
DEBUG:root:[ Iteration 168 ] Training loss: 0.0759365
DEBUG:root:[ Iteration 171 ] Training loss: 0.0677432
DEBUG:root:[ Iteration 174 ] Training loss: 0.0660356
DEBUG:root:[ Iteration 177 ] Training loss: 0.0751595
DEBUG:root:[ Iteration 180 ] Training loss: 0.0623817
DEBUG:root:[ Iteration 180 ] Test loss: 0.0627825
DEBUG:root:[ Iteration 183 ] Training loss: 0.0840933
DEBUG:root:[ Iteration 186 ] Training loss: 0.0625989
DEBUG:root:[ Iteration 189 ] Training loss: 0.118894
DEBUG:root:[ Iteration 192 ] Training loss: 0.0891257
DEBUG:root:[ Iteration 195 ] Training loss: 0.0804386
DEBUG:root:[ Iteration 198 ] Training loss: 0.0724681
DEBUG:root:[ Iteration 200 ] Test loss: 0.0857386
DEBUG:root:[ Iteration 201 ] Training loss: 0.0707201
DEBUG:root:[ Iteration 204 ] Training loss: 0.0661697
DEBUG:root:[ Iteration 207 ] Training loss: 0.0642088
DEBUG:root:[ Iteration 210 ] Training loss: 0.0646752
DEBUG:root:[ Iteration 213 ] Training loss: 0.0688783
DEBUG:root:[ Iteration 216 ] Training loss: 0.0575001
DEBUG:root:[ Iteration 219 ] Training loss: 0.0642394
DEBUG:root:[ Iteration 220 ] Test loss: 0.0643723
DEBUG:root:[ Iteration 222 ] Training loss: 0.0655245
DEBUG:root:[ Iteration 225 ] Training loss: 0.0652357
DEBUG:root:[ Iteration 228 ] Training loss: 0.0589385
DEBUG:root:[ Iteration 231 ] Training loss: 0.0657151
DEBUG:root:[ Iteration 234 ] Training loss: 0.0519709
DEBUG:root:[ Iteration 237 ] Training loss: 0.0704227
DEBUG:root:[ Iteration 240 ] Training loss: 0.059637
DEBUG:root:[ Iteration 240 ] Test loss: 0.0565528
DEBUG:root:[ Iteration 243 ] Training loss: 0.0557582
DEBUG:root:[ Iteration 246 ] Training loss: 0.0610446
DEBUG:root:[ Iteration 249 ] Training loss: 0.055294
DEBUG:root:[ Iteration 252 ] Training loss: 0.0562296
DEBUG:root:[ Iteration 255 ] Training loss: 0.057211
DEBUG:root:[ Iteration 258 ] Training loss: 0.0503253
DEBUG:root:[ Iteration 260 ] Test loss: 0.0479121
DEBUG:root:[ Iteration 261 ] Training loss: 0.0414909
DEBUG:root:[ Iteration 264 ] Training loss: 0.0505487
DEBUG:root:[ Iteration 267 ] Training loss: 0.0516273
DEBUG:root:[ Iteration 270 ] Training loss: 0.0409183
DEBUG:root:[ Iteration 273 ] Training loss: 0.0552138
DEBUG:root:[ Iteration 276 ] Training loss: 0.0350583
DEBUG:root:[ Iteration 279 ] Training loss: 0.0448417
DEBUG:root:[ Iteration 280 ] Test loss: 0.0599619
DEBUG:root:[ Iteration 282 ] Training loss: 0.0526219
DEBUG:root:[ Iteration 285 ] Training loss: 0.0488064
DEBUG:root:[ Iteration 288 ] Training loss: 0.0549471
DEBUG:root:[ Iteration 291 ] Training loss: 0.0452767
DEBUG:root:[ Iteration 294 ] Training loss: 0.0573322
DEBUG:root:[ Iteration 297 ] Training loss: 0.0537417
DEBUG:root:[ Iteration 300 ] Training loss: 0.0425738
DEBUG:root:[ Iteration 300 ] Test loss: 0.0455513
DEBUG:root:[ Iteration 303 ] Training loss: 0.0421425
DEBUG:root:[ Iteration 306 ] Training loss: 0.0411607
DEBUG:root:[ Iteration 309 ] Training loss: 0.0414235
DEBUG:root:[ Iteration 312 ] Training loss: 0.0328203
DEBUG:root:[ Iteration 315 ] Training loss: 0.0390149
DEBUG:root:[ Iteration 318 ] Training loss: 0.0724545
DEBUG:root:[ Iteration 320 ] Test loss: 0.0555598
DEBUG:root:[ Iteration 321 ] Training loss: 0.0439131
DEBUG:root:[ Iteration 324 ] Training loss: 0.0393892
DEBUG:root:[ Iteration 327 ] Training loss: 0.0497189
DEBUG:root:[ Iteration 330 ] Training loss: 0.0417493
DEBUG:root:[ Iteration 333 ] Training loss: 0.0375688
DEBUG:root:[ Iteration 336 ] Training loss: 0.0522601
DEBUG:root:[ Iteration 339 ] Training loss: 0.0553938
DEBUG:root:[ Iteration 340 ] Test loss: 0.0563117
DEBUG:root:[ Iteration 342 ] Training loss: 0.0432391
DEBUG:root:[ Iteration 345 ] Training loss: 0.0328775
DEBUG:root:[ Iteration 348 ] Training loss: 0.0487248
DEBUG:root:[ Iteration 351 ] Training loss: 0.0464698
DEBUG:root:[ Iteration 354 ] Training loss: 0.0394262
DEBUG:root:[ Iteration 357 ] Training loss: 0.0312197
DEBUG:root:[ Iteration 360 ] Training loss: 0.0402903
DEBUG:root:[ Iteration 360 ] Test loss: 0.043028
DEBUG:root:[ Iteration 363 ] Training loss: 0.0365558
DEBUG:root:[ Iteration 366 ] Training loss: 0.0405869
DEBUG:root:[ Iteration 369 ] Training loss: 0.0434401
DEBUG:root:[ Iteration 372 ] Training loss: 0.0493821
DEBUG:root:[ Iteration 375 ] Training loss: 0.039648
DEBUG:root:[ Iteration 378 ] Training loss: 0.0506792
DEBUG:root:[ Iteration 380 ] Test loss: 0.0410063
DEBUG:root:[ Iteration 381 ] Training loss: 0.038738
DEBUG:root:[ Iteration 384 ] Training loss: 0.030518
DEBUG:root:[ Iteration 387 ] Training loss: 0.0312618
DEBUG:root:[ Iteration 390 ] Training loss: 0.0420906
DEBUG:root:[ Iteration 393 ] Training loss: 0.0481888
DEBUG:root:[ Iteration 396 ] Training loss: 0.0367685
DEBUG:root:[ Iteration 399 ] Training loss: 0.035751
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-29-2016_16h48m17s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.194548
DEBUG:root:[ Iteration 0 ] Test loss: 0.191219
DEBUG:root:[ Iteration 3 ] Training loss: 0.198693
DEBUG:root:[ Iteration 6 ] Training loss: 0.185266
DEBUG:root:[ Iteration 9 ] Training loss: 0.147506
DEBUG:root:[ Iteration 12 ] Training loss: 0.0792999
DEBUG:root:[ Iteration 15 ] Training loss: 0.0787701
DEBUG:root:[ Iteration 18 ] Training loss: 0.0791023
DEBUG:root:[ Iteration 20 ] Test loss: 0.0988289
DEBUG:root:[ Iteration 21 ] Training loss: 0.0666283
DEBUG:root:[ Iteration 24 ] Training loss: 0.0553296
DEBUG:root:[ Iteration 27 ] Training loss: 0.0623264
DEBUG:root:[ Iteration 30 ] Training loss: 0.0491871
DEBUG:root:[ Iteration 33 ] Training loss: 0.0271987
DEBUG:root:[ Iteration 36 ] Training loss: 0.0286156
DEBUG:root:[ Iteration 39 ] Training loss: 0.014858
DEBUG:root:[ Iteration 40 ] Test loss: 0.0185944
DEBUG:root:[ Iteration 42 ] Training loss: 0.0181703
DEBUG:root:[ Iteration 45 ] Training loss: 0.0160075
DEBUG:root:[ Iteration 48 ] Training loss: 0.0188165
DEBUG:root:[ Iteration 51 ] Training loss: 0.0164594
DEBUG:root:[ Iteration 54 ] Training loss: 0.0145969
DEBUG:root:[ Iteration 57 ] Training loss: 0.0222754
DEBUG:root:[ Iteration 60 ] Training loss: 0.0169363
DEBUG:root:[ Iteration 60 ] Test loss: 0.01578
DEBUG:root:[ Iteration 63 ] Training loss: 0.0266593
DEBUG:root:[ Iteration 66 ] Training loss: 0.0182557
DEBUG:root:[ Iteration 69 ] Training loss: 0.0267011
DEBUG:root:[ Iteration 72 ] Training loss: 0.015701
DEBUG:root:[ Iteration 75 ] Training loss: 0.0255807
DEBUG:root:[ Iteration 78 ] Training loss: 0.0237273
DEBUG:root:[ Iteration 80 ] Test loss: 0.0154273
DEBUG:root:[ Iteration 81 ] Training loss: 0.0153485
DEBUG:root:[ Iteration 84 ] Training loss: 0.0181614
DEBUG:root:[ Iteration 87 ] Training loss: 0.0146011
DEBUG:root:[ Iteration 90 ] Training loss: 0.0158869
DEBUG:root:[ Iteration 93 ] Training loss: 0.0163145
DEBUG:root:[ Iteration 96 ] Training loss: 0.0107598
DEBUG:root:[ Iteration 99 ] Training loss: 0.0121552
DEBUG:root:[ Iteration 100 ] Test loss: 0.01326
DEBUG:root:[ Iteration 102 ] Training loss: 0.0207831
DEBUG:root:[ Iteration 105 ] Training loss: 0.016146
DEBUG:root:[ Iteration 108 ] Training loss: 0.0182131
DEBUG:root:[ Iteration 111 ] Training loss: 0.0137167
DEBUG:root:[ Iteration 114 ] Training loss: 0.0174007
DEBUG:root:[ Iteration 117 ] Training loss: 0.0126748
DEBUG:root:[ Iteration 120 ] Training loss: 0.0123299
DEBUG:root:[ Iteration 120 ] Test loss: 0.0133513
DEBUG:root:[ Iteration 123 ] Training loss: 0.0127884
DEBUG:root:[ Iteration 126 ] Training loss: 0.0143514
DEBUG:root:[ Iteration 129 ] Training loss: 0.0116183
DEBUG:root:[ Iteration 132 ] Training loss: 0.0135523
DEBUG:root:[ Iteration 135 ] Training loss: 0.0129768
DEBUG:root:[ Iteration 138 ] Training loss: 0.0215555
DEBUG:root:[ Iteration 140 ] Test loss: 0.0131289
DEBUG:root:[ Iteration 141 ] Training loss: 0.011318
DEBUG:root:[ Iteration 144 ] Training loss: 0.0187438
DEBUG:root:[ Iteration 147 ] Training loss: 0.0197201
DEBUG:root:[ Iteration 150 ] Training loss: 0.0179756
DEBUG:root:[ Iteration 153 ] Training loss: 0.0149551
DEBUG:root:[ Iteration 156 ] Training loss: 0.0139747
DEBUG:root:[ Iteration 159 ] Training loss: 0.0171677
DEBUG:root:[ Iteration 160 ] Test loss: 0.0109679
DEBUG:root:[ Iteration 162 ] Training loss: 0.0240138
DEBUG:root:[ Iteration 165 ] Training loss: 0.0149382
DEBUG:root:[ Iteration 168 ] Training loss: 0.0126144
DEBUG:root:[ Iteration 171 ] Training loss: 0.0078674
DEBUG:root:[ Iteration 174 ] Training loss: 0.0188779
DEBUG:root:[ Iteration 177 ] Training loss: 0.0220754
DEBUG:root:[ Iteration 180 ] Training loss: 0.0185571
DEBUG:root:[ Iteration 180 ] Test loss: 0.00996216
DEBUG:root:[ Iteration 183 ] Training loss: 0.0159153
DEBUG:root:[ Iteration 186 ] Training loss: 0.012153
DEBUG:root:[ Iteration 189 ] Training loss: 0.0121321
DEBUG:root:[ Iteration 192 ] Training loss: 0.011941
DEBUG:root:[ Iteration 195 ] Training loss: 0.0182423
DEBUG:root:[ Iteration 198 ] Training loss: 0.0198039
DEBUG:root:[ Iteration 200 ] Test loss: 0.0108307
DEBUG:root:[ Iteration 201 ] Training loss: 0.0105742
DEBUG:root:[ Iteration 204 ] Training loss: 0.0106978
DEBUG:root:[ Iteration 207 ] Training loss: 0.0140594
DEBUG:root:[ Iteration 210 ] Training loss: 0.0175382
DEBUG:root:[ Iteration 213 ] Training loss: 0.0232486
DEBUG:root:[ Iteration 216 ] Training loss: 0.016909
DEBUG:root:[ Iteration 219 ] Training loss: 0.0137165
DEBUG:root:[ Iteration 220 ] Test loss: 0.0109072
DEBUG:root:[ Iteration 222 ] Training loss: 0.0175097
DEBUG:root:[ Iteration 225 ] Training loss: 0.0112943
DEBUG:root:[ Iteration 228 ] Training loss: 0.018787
DEBUG:root:[ Iteration 231 ] Training loss: 0.0168758
DEBUG:root:[ Iteration 234 ] Training loss: 0.0180563
DEBUG:root:[ Iteration 237 ] Training loss: 0.00899041
DEBUG:root:[ Iteration 240 ] Training loss: 0.0112116
DEBUG:root:[ Iteration 240 ] Test loss: 0.0112697
DEBUG:root:[ Iteration 243 ] Training loss: 0.0167128
DEBUG:root:[ Iteration 246 ] Training loss: 0.0144255
DEBUG:root:[ Iteration 249 ] Training loss: 0.020028
DEBUG:root:[ Iteration 252 ] Training loss: 0.0186125
DEBUG:root:[ Iteration 255 ] Training loss: 0.0117334
DEBUG:root:[ Iteration 258 ] Training loss: 0.0103722
DEBUG:root:[ Iteration 260 ] Test loss: 0.00954203
DEBUG:root:[ Iteration 261 ] Training loss: 0.00871187
DEBUG:root:[ Iteration 264 ] Training loss: 0.00738943
DEBUG:root:[ Iteration 267 ] Training loss: 0.0119953
DEBUG:root:[ Iteration 270 ] Training loss: 0.014663
DEBUG:root:[ Iteration 273 ] Training loss: 0.0186092
DEBUG:root:[ Iteration 276 ] Training loss: 0.0154215
DEBUG:root:[ Iteration 279 ] Training loss: 0.0161307
DEBUG:root:[ Iteration 280 ] Test loss: 0.0102404
DEBUG:root:[ Iteration 282 ] Training loss: 0.0185656
DEBUG:root:[ Iteration 285 ] Training loss: 0.010541
DEBUG:root:[ Iteration 288 ] Training loss: 0.0220414
DEBUG:root:[ Iteration 291 ] Training loss: 0.00834192
DEBUG:root:[ Iteration 294 ] Training loss: 0.0101233
DEBUG:root:[ Iteration 297 ] Training loss: 0.0135557
DEBUG:root:[ Iteration 300 ] Training loss: 0.00967682
DEBUG:root:[ Iteration 300 ] Test loss: 0.0100835
DEBUG:root:[ Iteration 303 ] Training loss: 0.0230327
DEBUG:root:[ Iteration 306 ] Training loss: 0.0091151
DEBUG:root:[ Iteration 309 ] Training loss: 0.00905906
DEBUG:root:[ Iteration 312 ] Training loss: 0.0165809
DEBUG:root:[ Iteration 315 ] Training loss: 0.0107888
DEBUG:root:[ Iteration 318 ] Training loss: 0.0170689
DEBUG:root:[ Iteration 320 ] Test loss: 0.00793933
DEBUG:root:[ Iteration 321 ] Training loss: 0.0109428
DEBUG:root:[ Iteration 324 ] Training loss: 0.0135083
DEBUG:root:[ Iteration 327 ] Training loss: 0.0102856
DEBUG:root:[ Iteration 330 ] Training loss: 0.0130349
DEBUG:root:[ Iteration 333 ] Training loss: 0.00984279
DEBUG:root:[ Iteration 336 ] Training loss: 0.0119514
DEBUG:root:[ Iteration 339 ] Training loss: 0.0180064
DEBUG:root:[ Iteration 340 ] Test loss: 0.00756764
DEBUG:root:[ Iteration 342 ] Training loss: 0.00890479
DEBUG:root:[ Iteration 345 ] Training loss: 0.0248381
DEBUG:root:[ Iteration 348 ] Training loss: 0.0151633
DEBUG:root:[ Iteration 351 ] Training loss: 0.0138169
DEBUG:root:[ Iteration 354 ] Training loss: 0.0216012
DEBUG:root:[ Iteration 357 ] Training loss: 0.0114478
DEBUG:root:[ Iteration 360 ] Training loss: 0.0119185
DEBUG:root:[ Iteration 360 ] Test loss: 0.00861058
DEBUG:root:[ Iteration 363 ] Training loss: 0.0145627
DEBUG:root:[ Iteration 366 ] Training loss: 0.0106234
DEBUG:root:[ Iteration 369 ] Training loss: 0.0105167
DEBUG:root:[ Iteration 372 ] Training loss: 0.016763
DEBUG:root:[ Iteration 375 ] Training loss: 0.0135297
DEBUG:root:[ Iteration 378 ] Training loss: 0.0128656
DEBUG:root:[ Iteration 380 ] Test loss: 0.0107573
DEBUG:root:[ Iteration 381 ] Training loss: 0.0157031
DEBUG:root:[ Iteration 384 ] Training loss: 0.0165336
DEBUG:root:[ Iteration 387 ] Training loss: 0.00896606
DEBUG:root:[ Iteration 390 ] Training loss: 0.0128048
DEBUG:root:[ Iteration 393 ] Training loss: 0.0177763
DEBUG:root:[ Iteration 396 ] Training loss: 0.00612192
DEBUG:root:[ Iteration 399 ] Training loss: 0.0143004
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-29-2016_20h21m24s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.202612
DEBUG:root:[ Iteration 0 ] Test loss: 0.208904
DEBUG:root:[ Iteration 3 ] Training loss: 0.201331
DEBUG:root:[ Iteration 6 ] Training loss: 0.205022
DEBUG:root:[ Iteration 9 ] Training loss: 0.197436
DEBUG:root:[ Iteration 12 ] Training loss: 0.188115
DEBUG:root:[ Iteration 15 ] Training loss: 0.171405
DEBUG:root:[ Iteration 18 ] Training loss: 0.109112
DEBUG:root:[ Iteration 20 ] Test loss: 0.0917018
DEBUG:root:[ Iteration 21 ] Training loss: 0.0876005
DEBUG:root:[ Iteration 24 ] Training loss: 0.0934591
DEBUG:root:[ Iteration 27 ] Training loss: 0.0822135
DEBUG:root:[ Iteration 30 ] Training loss: 0.0926749
DEBUG:root:[ Iteration 33 ] Training loss: 0.0789554
DEBUG:root:[ Iteration 36 ] Training loss: 0.0736932
DEBUG:root:[ Iteration 39 ] Training loss: 0.0738806
DEBUG:root:[ Iteration 40 ] Test loss: 0.0773752
DEBUG:root:[ Iteration 42 ] Training loss: 0.0582026
DEBUG:root:[ Iteration 45 ] Training loss: 0.0564284
DEBUG:root:[ Iteration 48 ] Training loss: 0.0481362
DEBUG:root:[ Iteration 51 ] Training loss: 0.0385153
DEBUG:root:[ Iteration 54 ] Training loss: 0.0365079
DEBUG:root:[ Iteration 57 ] Training loss: 0.0214526
DEBUG:root:[ Iteration 60 ] Training loss: 0.0299146
DEBUG:root:[ Iteration 60 ] Test loss: 0.033698
DEBUG:root:[ Iteration 63 ] Training loss: 0.0252784
DEBUG:root:[ Iteration 66 ] Training loss: 0.0264801
DEBUG:root:[ Iteration 69 ] Training loss: 0.0244833
DEBUG:root:[ Iteration 72 ] Training loss: 0.0219651
DEBUG:root:[ Iteration 75 ] Training loss: 0.0256365
DEBUG:root:[ Iteration 78 ] Training loss: 0.0229566
DEBUG:root:[ Iteration 80 ] Test loss: 0.0248035
DEBUG:root:[ Iteration 81 ] Training loss: 0.0225689
DEBUG:root:[ Iteration 84 ] Training loss: 0.029154
DEBUG:root:[ Iteration 87 ] Training loss: 0.0169549
DEBUG:root:[ Iteration 90 ] Training loss: 0.0202535
DEBUG:root:[ Iteration 93 ] Training loss: 0.0188586
DEBUG:root:[ Iteration 96 ] Training loss: 0.0274502
DEBUG:root:[ Iteration 99 ] Training loss: 0.0202567
DEBUG:root:[ Iteration 100 ] Test loss: 0.0230728
DEBUG:root:[ Iteration 102 ] Training loss: 0.0155247
DEBUG:root:[ Iteration 105 ] Training loss: 0.018736
DEBUG:root:[ Iteration 108 ] Training loss: 0.01962
DEBUG:root:[ Iteration 111 ] Training loss: 0.0239073
DEBUG:root:[ Iteration 114 ] Training loss: 0.019216
DEBUG:root:[ Iteration 117 ] Training loss: 0.0173426
DEBUG:root:[ Iteration 120 ] Training loss: 0.0160223
DEBUG:root:[ Iteration 120 ] Test loss: 0.0264393
DEBUG:root:[ Iteration 123 ] Training loss: 0.0167141
DEBUG:root:[ Iteration 126 ] Training loss: 0.0211228
DEBUG:root:[ Iteration 129 ] Training loss: 0.0206329
DEBUG:root:[ Iteration 132 ] Training loss: 0.013882
DEBUG:root:[ Iteration 135 ] Training loss: 0.0188117
DEBUG:root:[ Iteration 138 ] Training loss: 0.0211044
DEBUG:root:[ Iteration 140 ] Test loss: 0.0119749
DEBUG:root:[ Iteration 141 ] Training loss: 0.0176434
DEBUG:root:[ Iteration 144 ] Training loss: 0.0143259
DEBUG:root:[ Iteration 147 ] Training loss: 0.0239853
DEBUG:root:[ Iteration 150 ] Training loss: 0.0201217
DEBUG:root:[ Iteration 153 ] Training loss: 0.0188115
DEBUG:root:[ Iteration 156 ] Training loss: 0.0151602
DEBUG:root:[ Iteration 159 ] Training loss: 0.0185245
DEBUG:root:[ Iteration 160 ] Test loss: 0.0190761
DEBUG:root:[ Iteration 162 ] Training loss: 0.016307
DEBUG:root:[ Iteration 165 ] Training loss: 0.0113161
DEBUG:root:[ Iteration 168 ] Training loss: 0.0159761
DEBUG:root:[ Iteration 171 ] Training loss: 0.0130001
DEBUG:root:[ Iteration 174 ] Training loss: 0.016204
DEBUG:root:[ Iteration 177 ] Training loss: 0.0211806
DEBUG:root:[ Iteration 180 ] Training loss: 0.0187614
DEBUG:root:[ Iteration 180 ] Test loss: 0.0191447
DEBUG:root:[ Iteration 183 ] Training loss: 0.0157355
DEBUG:root:[ Iteration 186 ] Training loss: 0.0193897
DEBUG:root:[ Iteration 189 ] Training loss: 0.0205728
DEBUG:root:[ Iteration 192 ] Training loss: 0.0161837
DEBUG:root:[ Iteration 195 ] Training loss: 0.0154083
DEBUG:root:[ Iteration 198 ] Training loss: 0.0150743
DEBUG:root:[ Iteration 200 ] Test loss: 0.0183422
DEBUG:root:[ Iteration 201 ] Training loss: 0.0179124
DEBUG:root:[ Iteration 204 ] Training loss: 0.0117886
DEBUG:root:[ Iteration 207 ] Training loss: 0.0190515
DEBUG:root:[ Iteration 210 ] Training loss: 0.0164606
DEBUG:root:[ Iteration 213 ] Training loss: 0.012129
DEBUG:root:[ Iteration 216 ] Training loss: 0.0132912
DEBUG:root:[ Iteration 219 ] Training loss: 0.017047
DEBUG:root:[ Iteration 220 ] Test loss: 0.0192333
DEBUG:root:[ Iteration 222 ] Training loss: 0.0128282
DEBUG:root:[ Iteration 225 ] Training loss: 0.0155044
DEBUG:root:[ Iteration 228 ] Training loss: 0.0150154
DEBUG:root:[ Iteration 231 ] Training loss: 0.0153697
DEBUG:root:[ Iteration 234 ] Training loss: 0.0126473
DEBUG:root:[ Iteration 237 ] Training loss: 0.0192745
DEBUG:root:[ Iteration 240 ] Training loss: 0.0185639
DEBUG:root:[ Iteration 240 ] Test loss: 0.0180829
DEBUG:root:[ Iteration 243 ] Training loss: 0.0138864
DEBUG:root:[ Iteration 246 ] Training loss: 0.0172436
DEBUG:root:[ Iteration 249 ] Training loss: 0.0125619
DEBUG:root:[ Iteration 252 ] Training loss: 0.0121636
DEBUG:root:[ Iteration 255 ] Training loss: 0.0156268
DEBUG:root:[ Iteration 258 ] Training loss: 0.0167705
DEBUG:root:[ Iteration 260 ] Test loss: 0.0203802
DEBUG:root:[ Iteration 261 ] Training loss: 0.0118997
DEBUG:root:[ Iteration 264 ] Training loss: 0.011895
DEBUG:root:[ Iteration 267 ] Training loss: 0.0136751
DEBUG:root:[ Iteration 270 ] Training loss: 0.0193755
DEBUG:root:[ Iteration 273 ] Training loss: 0.0119005
DEBUG:root:[ Iteration 276 ] Training loss: 0.0146461
DEBUG:root:[ Iteration 279 ] Training loss: 0.0133193
DEBUG:root:[ Iteration 280 ] Test loss: 0.0145971
DEBUG:root:[ Iteration 282 ] Training loss: 0.0154304
DEBUG:root:[ Iteration 285 ] Training loss: 0.0142789
DEBUG:root:[ Iteration 288 ] Training loss: 0.00949879
DEBUG:root:[ Iteration 291 ] Training loss: 0.00911565
DEBUG:root:[ Iteration 294 ] Training loss: 0.0205491
DEBUG:root:[ Iteration 297 ] Training loss: 0.0170138
DEBUG:root:[ Iteration 300 ] Training loss: 0.0136273
DEBUG:root:[ Iteration 300 ] Test loss: 0.0194128
DEBUG:root:[ Iteration 303 ] Training loss: 0.0144831
DEBUG:root:[ Iteration 306 ] Training loss: 0.0139763
DEBUG:root:[ Iteration 309 ] Training loss: 0.0155927
DEBUG:root:[ Iteration 312 ] Training loss: 0.0132559
DEBUG:root:[ Iteration 315 ] Training loss: 0.0104686
DEBUG:root:[ Iteration 318 ] Training loss: 0.0125706
DEBUG:root:[ Iteration 320 ] Test loss: 0.0207816
DEBUG:root:[ Iteration 321 ] Training loss: 0.006639
DEBUG:root:[ Iteration 324 ] Training loss: 0.0184548
DEBUG:root:[ Iteration 327 ] Training loss: 0.0100464
DEBUG:root:[ Iteration 330 ] Training loss: 0.0123209
DEBUG:root:[ Iteration 333 ] Training loss: 0.011821
DEBUG:root:[ Iteration 336 ] Training loss: 0.00955322
DEBUG:root:[ Iteration 339 ] Training loss: 0.0157388
DEBUG:root:[ Iteration 340 ] Test loss: 0.0131982
DEBUG:root:[ Iteration 342 ] Training loss: 0.00930443
DEBUG:root:[ Iteration 345 ] Training loss: 0.0119892
DEBUG:root:[ Iteration 348 ] Training loss: 0.016189
DEBUG:root:[ Iteration 351 ] Training loss: 0.00952057
DEBUG:root:[ Iteration 354 ] Training loss: 0.01041
DEBUG:root:[ Iteration 357 ] Training loss: 0.0158596
DEBUG:root:[ Iteration 360 ] Training loss: 0.010374
DEBUG:root:[ Iteration 360 ] Test loss: 0.0166341
DEBUG:root:[ Iteration 363 ] Training loss: 0.00903948
DEBUG:root:[ Iteration 366 ] Training loss: 0.0103266
DEBUG:root:[ Iteration 369 ] Training loss: 0.00903206
DEBUG:root:[ Iteration 372 ] Training loss: 0.0118154
DEBUG:root:[ Iteration 375 ] Training loss: 0.00934279
DEBUG:root:[ Iteration 378 ] Training loss: 0.0125358
DEBUG:root:[ Iteration 380 ] Test loss: 0.018559
DEBUG:root:[ Iteration 381 ] Training loss: 0.0060791
DEBUG:root:[ Iteration 384 ] Training loss: 0.0129177
DEBUG:root:[ Iteration 387 ] Training loss: 0.011238
DEBUG:root:[ Iteration 390 ] Training loss: 0.013587
DEBUG:root:[ Iteration 393 ] Training loss: 0.0128109
DEBUG:root:[ Iteration 396 ] Training loss: 0.0100334
DEBUG:root:[ Iteration 399 ] Training loss: 0.00988074
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_02-29-2016_22h44m24s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0277549
DEBUG:root:[ Iteration 0 ] Test loss: 0.0310382
DEBUG:root:[ Iteration 3 ] Training loss: 0.031047
DEBUG:root:[ Iteration 6 ] Training loss: 0.0388238
DEBUG:root:[ Iteration 9 ] Training loss: 0.0291432
DEBUG:root:[ Iteration 12 ] Training loss: 0.027131
DEBUG:root:[ Iteration 15 ] Training loss: 0.0235835
DEBUG:root:[ Iteration 18 ] Training loss: 0.0220996
DEBUG:root:[ Iteration 20 ] Test loss: 0.0291044
DEBUG:root:[ Iteration 21 ] Training loss: 0.0216921
DEBUG:root:[ Iteration 24 ] Training loss: 0.0214234
DEBUG:root:[ Iteration 27 ] Training loss: 0.0275471
DEBUG:root:[ Iteration 30 ] Training loss: 0.0260739
DEBUG:root:[ Iteration 33 ] Training loss: 0.0254738
DEBUG:root:[ Iteration 36 ] Training loss: 0.0211983
DEBUG:root:[ Iteration 39 ] Training loss: 0.0203974
DEBUG:root:[ Iteration 40 ] Test loss: 0.0169305
DEBUG:root:[ Iteration 42 ] Training loss: 0.0216796
DEBUG:root:[ Iteration 45 ] Training loss: 0.0153242
DEBUG:root:[ Iteration 48 ] Training loss: 0.0172449
DEBUG:root:[ Iteration 51 ] Training loss: 0.0211795
DEBUG:root:[ Iteration 54 ] Training loss: 0.0188337
DEBUG:root:[ Iteration 57 ] Training loss: 0.0146129
DEBUG:root:[ Iteration 60 ] Training loss: 0.018164
DEBUG:root:[ Iteration 60 ] Test loss: 0.0155066
DEBUG:root:[ Iteration 63 ] Training loss: 0.0135167
DEBUG:root:[ Iteration 66 ] Training loss: 0.0128455
DEBUG:root:[ Iteration 69 ] Training loss: 0.015444
DEBUG:root:[ Iteration 72 ] Training loss: 0.015097
DEBUG:root:[ Iteration 75 ] Training loss: 0.0182066
DEBUG:root:[ Iteration 78 ] Training loss: 0.0215388
DEBUG:root:[ Iteration 80 ] Test loss: 0.0228344
DEBUG:root:[ Iteration 81 ] Training loss: 0.0223152
DEBUG:root:[ Iteration 84 ] Training loss: 0.0183027
DEBUG:root:[ Iteration 87 ] Training loss: 0.0181801
DEBUG:root:[ Iteration 90 ] Training loss: 0.0165192
DEBUG:root:[ Iteration 93 ] Training loss: 0.0223098
DEBUG:root:[ Iteration 96 ] Training loss: 0.0200102
DEBUG:root:[ Iteration 99 ] Training loss: 0.0174235
DEBUG:root:[ Iteration 100 ] Test loss: 0.0160481
DEBUG:root:[ Iteration 102 ] Training loss: 0.0147238
DEBUG:root:[ Iteration 105 ] Training loss: 0.0189913
DEBUG:root:[ Iteration 108 ] Training loss: 0.0150641
DEBUG:root:[ Iteration 111 ] Training loss: 0.0149814
DEBUG:root:[ Iteration 114 ] Training loss: 0.0171861
DEBUG:root:[ Iteration 117 ] Training loss: 0.0164016
DEBUG:root:[ Iteration 120 ] Training loss: 0.0128889
DEBUG:root:[ Iteration 120 ] Test loss: 0.0143086
DEBUG:root:[ Iteration 123 ] Training loss: 0.0135842
DEBUG:root:[ Iteration 126 ] Training loss: 0.0158153
DEBUG:root:[ Iteration 129 ] Training loss: 0.00979272
DEBUG:root:[ Iteration 132 ] Training loss: 0.0194674
DEBUG:root:[ Iteration 135 ] Training loss: 0.0122633
DEBUG:root:[ Iteration 138 ] Training loss: 0.0172201
DEBUG:root:[ Iteration 140 ] Test loss: 0.0189467
DEBUG:root:[ Iteration 141 ] Training loss: 0.0142086
DEBUG:root:[ Iteration 144 ] Training loss: 0.0189882
DEBUG:root:[ Iteration 147 ] Training loss: 0.0130775
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-01-2016_00h20m12s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0159293
DEBUG:root:[ Iteration 0 ] Test loss: 0.0102971
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-01-2016_12h13m53s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0172419
DEBUG:root:[ Iteration 0 ] Test loss: 0.0170761
DEBUG:root:[ Iteration 3 ] Training loss: 0.0203116
DEBUG:root:[ Iteration 6 ] Training loss: 0.0260546
DEBUG:root:[ Iteration 9 ] Training loss: 0.0215556
DEBUG:root:[ Iteration 12 ] Training loss: 0.0132281
DEBUG:root:[ Iteration 15 ] Training loss: 0.0193599
DEBUG:root:[ Iteration 18 ] Training loss: 0.014635
DEBUG:root:[ Iteration 20 ] Test loss: 0.0143683
DEBUG:root:[ Iteration 21 ] Training loss: 0.0199388
DEBUG:root:[ Iteration 24 ] Training loss: 0.016665
DEBUG:root:[ Iteration 27 ] Training loss: 0.0167911
DEBUG:root:[ Iteration 30 ] Training loss: 0.0169637
DEBUG:root:[ Iteration 33 ] Training loss: 0.0145362
DEBUG:root:[ Iteration 36 ] Training loss: 0.0152781
DEBUG:root:[ Iteration 39 ] Training loss: 0.0153563
DEBUG:root:[ Iteration 40 ] Test loss: 0.0135006
DEBUG:root:[ Iteration 42 ] Training loss: 0.0126649
DEBUG:root:[ Iteration 45 ] Training loss: 0.0196228
DEBUG:root:[ Iteration 48 ] Training loss: 0.0141951
DEBUG:root:[ Iteration 51 ] Training loss: 0.0161593
DEBUG:root:[ Iteration 54 ] Training loss: 0.0151002
DEBUG:root:[ Iteration 57 ] Training loss: 0.0144011
DEBUG:root:[ Iteration 60 ] Training loss: 0.0177747
DEBUG:root:[ Iteration 60 ] Test loss: 0.0184003
DEBUG:root:[ Iteration 63 ] Training loss: 0.0215388
DEBUG:root:[ Iteration 66 ] Training loss: 0.016276
DEBUG:root:[ Iteration 69 ] Training loss: 0.0151161
DEBUG:root:[ Iteration 72 ] Training loss: 0.0109547
DEBUG:root:[ Iteration 75 ] Training loss: 0.013098
DEBUG:root:[ Iteration 78 ] Training loss: 0.0131968
DEBUG:root:[ Iteration 80 ] Test loss: 0.0127102
DEBUG:root:[ Iteration 81 ] Training loss: 0.0130209
DEBUG:root:[ Iteration 84 ] Training loss: 0.0186518
DEBUG:root:[ Iteration 87 ] Training loss: 0.0171474
DEBUG:root:[ Iteration 90 ] Training loss: 0.011155
DEBUG:root:[ Iteration 93 ] Training loss: 0.0147823
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-01-2016_12h16m19s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0942823
DEBUG:root:[ Iteration 0 ] Test loss: 0.0824246
DEBUG:root:[ Iteration 3 ] Training loss: 0.0840404
DEBUG:root:[ Iteration 6 ] Training loss: 0.0827791
DEBUG:root:[ Iteration 9 ] Training loss: 0.0853316
DEBUG:root:[ Iteration 12 ] Training loss: 0.0675645
DEBUG:root:[ Iteration 15 ] Training loss: 0.068993
DEBUG:root:[ Iteration 18 ] Training loss: 0.0649602
DEBUG:root:[ Iteration 20 ] Test loss: 0.064182
DEBUG:root:[ Iteration 21 ] Training loss: 0.0612778
DEBUG:root:[ Iteration 24 ] Training loss: 0.0580337
DEBUG:root:[ Iteration 27 ] Training loss: 0.0613372
DEBUG:root:[ Iteration 30 ] Training loss: 0.0526009
DEBUG:root:[ Iteration 33 ] Training loss: 0.0589445
DEBUG:root:[ Iteration 36 ] Training loss: 0.0637459
DEBUG:root:[ Iteration 39 ] Training loss: 0.0556809
DEBUG:root:[ Iteration 40 ] Test loss: 0.0515433
DEBUG:root:[ Iteration 42 ] Training loss: 0.0563111
DEBUG:root:[ Iteration 45 ] Training loss: 0.0535654
DEBUG:root:[ Iteration 48 ] Training loss: 0.0508917
DEBUG:root:[ Iteration 51 ] Training loss: 0.0567171
DEBUG:root:[ Iteration 54 ] Training loss: 0.0524783
DEBUG:root:[ Iteration 57 ] Training loss: 0.0506611
DEBUG:root:[ Iteration 60 ] Training loss: 0.0465781
DEBUG:root:[ Iteration 60 ] Test loss: 0.0512026
DEBUG:root:[ Iteration 63 ] Training loss: 0.0501028
DEBUG:root:[ Iteration 66 ] Training loss: 0.0490754
DEBUG:root:[ Iteration 69 ] Training loss: 0.052001
DEBUG:root:[ Iteration 72 ] Training loss: 0.0499467
DEBUG:root:[ Iteration 75 ] Training loss: 0.0411829
DEBUG:root:[ Iteration 78 ] Training loss: 0.047049
DEBUG:root:[ Iteration 80 ] Test loss: 0.0456738
DEBUG:root:[ Iteration 81 ] Training loss: 0.0410576
DEBUG:root:[ Iteration 84 ] Training loss: 0.041831
DEBUG:root:[ Iteration 87 ] Training loss: 0.0381805
DEBUG:root:[ Iteration 90 ] Training loss: 0.036685
DEBUG:root:[ Iteration 93 ] Training loss: 0.0506517
DEBUG:root:[ Iteration 96 ] Training loss: 0.0431485
DEBUG:root:[ Iteration 99 ] Training loss: 0.0349491
DEBUG:root:[ Iteration 100 ] Test loss: 0.041024
DEBUG:root:[ Iteration 102 ] Training loss: 0.0346866
DEBUG:root:[ Iteration 105 ] Training loss: 0.0402006
DEBUG:root:[ Iteration 108 ] Training loss: 0.037629
DEBUG:root:[ Iteration 111 ] Training loss: 0.038934
DEBUG:root:[ Iteration 114 ] Training loss: 0.0407615
DEBUG:root:[ Iteration 117 ] Training loss: 0.0357456
DEBUG:root:[ Iteration 120 ] Training loss: 0.0399473
DEBUG:root:[ Iteration 120 ] Test loss: 0.0309703
DEBUG:root:[ Iteration 123 ] Training loss: 0.0370627
DEBUG:root:[ Iteration 126 ] Training loss: 0.0340206
DEBUG:root:[ Iteration 129 ] Training loss: 0.043296
DEBUG:root:[ Iteration 132 ] Training loss: 0.0333919
DEBUG:root:[ Iteration 135 ] Training loss: 0.0364305
DEBUG:root:[ Iteration 138 ] Training loss: 0.0321013
DEBUG:root:[ Iteration 140 ] Test loss: 0.043134
DEBUG:root:[ Iteration 141 ] Training loss: 0.0361269
DEBUG:root:[ Iteration 144 ] Training loss: 0.0466113
DEBUG:root:[ Iteration 147 ] Training loss: 0.0341143
DEBUG:root:[ Iteration 150 ] Training loss: 0.0367024
DEBUG:root:[ Iteration 153 ] Training loss: 0.0299606
DEBUG:root:[ Iteration 156 ] Training loss: 0.0336136
DEBUG:root:[ Iteration 159 ] Training loss: 0.0302774
DEBUG:root:[ Iteration 160 ] Test loss: 0.0337885
DEBUG:root:[ Iteration 162 ] Training loss: 0.0337297
DEBUG:root:[ Iteration 165 ] Training loss: 0.0283399
DEBUG:root:[ Iteration 168 ] Training loss: 0.035014
DEBUG:root:[ Iteration 171 ] Training loss: 0.0347269
DEBUG:root:[ Iteration 174 ] Training loss: 0.0329495
DEBUG:root:[ Iteration 177 ] Training loss: 0.0330449
DEBUG:root:[ Iteration 180 ] Training loss: 0.0356229
DEBUG:root:[ Iteration 180 ] Test loss: 0.0318073
DEBUG:root:[ Iteration 183 ] Training loss: 0.0299123
DEBUG:root:[ Iteration 186 ] Training loss: 0.0316104
DEBUG:root:[ Iteration 189 ] Training loss: 0.0397273
DEBUG:root:[ Iteration 192 ] Training loss: 0.0376905
DEBUG:root:[ Iteration 195 ] Training loss: 0.0336383
DEBUG:root:[ Iteration 198 ] Training loss: 0.0299214
DEBUG:root:[ Iteration 200 ] Test loss: 0.0286666
DEBUG:root:[ Iteration 201 ] Training loss: 0.0349828
DEBUG:root:[ Iteration 204 ] Training loss: 0.0351451
DEBUG:root:[ Iteration 207 ] Training loss: 0.0242267
DEBUG:root:[ Iteration 210 ] Training loss: 0.0307799
DEBUG:root:[ Iteration 213 ] Training loss: 0.0242057
DEBUG:root:[ Iteration 216 ] Training loss: 0.0274121
DEBUG:root:[ Iteration 219 ] Training loss: 0.0234857
DEBUG:root:[ Iteration 220 ] Test loss: 0.0245291
DEBUG:root:[ Iteration 222 ] Training loss: 0.0294611
DEBUG:root:[ Iteration 225 ] Training loss: 0.0185699
DEBUG:root:[ Iteration 228 ] Training loss: 0.0243912
DEBUG:root:[ Iteration 231 ] Training loss: 0.0280522
DEBUG:root:[ Iteration 234 ] Training loss: 0.0319647
DEBUG:root:[ Iteration 237 ] Training loss: 0.0251497
DEBUG:root:[ Iteration 240 ] Training loss: 0.0302584
DEBUG:root:[ Iteration 240 ] Test loss: 0.0380806
DEBUG:root:[ Iteration 243 ] Training loss: 0.0267684
DEBUG:root:[ Iteration 246 ] Training loss: 0.0233094
DEBUG:root:[ Iteration 249 ] Training loss: 0.0280292
DEBUG:root:[ Iteration 252 ] Training loss: 0.0262226
DEBUG:root:[ Iteration 255 ] Training loss: 0.0243173
DEBUG:root:[ Iteration 258 ] Training loss: 0.0267605
DEBUG:root:[ Iteration 260 ] Test loss: 0.0279619
DEBUG:root:[ Iteration 261 ] Training loss: 0.0251095
DEBUG:root:[ Iteration 264 ] Training loss: 0.034414
DEBUG:root:[ Iteration 267 ] Training loss: 0.0262603
DEBUG:root:[ Iteration 270 ] Training loss: 0.0210742
DEBUG:root:[ Iteration 273 ] Training loss: 0.0209423
DEBUG:root:[ Iteration 276 ] Training loss: 0.0235613
DEBUG:root:[ Iteration 279 ] Training loss: 0.0238452
DEBUG:root:[ Iteration 280 ] Test loss: 0.0254725
DEBUG:root:[ Iteration 282 ] Training loss: 0.0197129
DEBUG:root:[ Iteration 285 ] Training loss: 0.0251398
DEBUG:root:[ Iteration 288 ] Training loss: 0.0241988
DEBUG:root:[ Iteration 291 ] Training loss: 0.0259847
DEBUG:root:[ Iteration 294 ] Training loss: 0.0161391
DEBUG:root:[ Iteration 297 ] Training loss: 0.0184949
DEBUG:root:[ Iteration 300 ] Training loss: 0.0208362
DEBUG:root:[ Iteration 300 ] Test loss: 0.0221993
DEBUG:root:[ Iteration 303 ] Training loss: 0.0196438
DEBUG:root:[ Iteration 306 ] Training loss: 0.0220011
DEBUG:root:[ Iteration 309 ] Training loss: 0.0217446
DEBUG:root:[ Iteration 312 ] Training loss: 0.0245896
DEBUG:root:[ Iteration 315 ] Training loss: 0.0198964
DEBUG:root:[ Iteration 318 ] Training loss: 0.0241373
DEBUG:root:[ Iteration 320 ] Test loss: 0.0278486
DEBUG:root:[ Iteration 321 ] Training loss: 0.0234541
DEBUG:root:[ Iteration 324 ] Training loss: 0.020122
DEBUG:root:[ Iteration 327 ] Training loss: 0.0201681
DEBUG:root:[ Iteration 330 ] Training loss: 0.0165023
DEBUG:root:[ Iteration 333 ] Training loss: 0.0180816
DEBUG:root:[ Iteration 336 ] Training loss: 0.0170727
DEBUG:root:[ Iteration 339 ] Training loss: 0.0227792
DEBUG:root:[ Iteration 340 ] Test loss: 0.0232272
DEBUG:root:[ Iteration 342 ] Training loss: 0.0153682
DEBUG:root:[ Iteration 345 ] Training loss: 0.0158051
DEBUG:root:[ Iteration 348 ] Training loss: 0.0175498
DEBUG:root:[ Iteration 351 ] Training loss: 0.0221596
DEBUG:root:[ Iteration 354 ] Training loss: 0.0207585
DEBUG:root:[ Iteration 357 ] Training loss: 0.0197187
DEBUG:root:[ Iteration 360 ] Training loss: 0.0177091
DEBUG:root:[ Iteration 360 ] Test loss: 0.0185117
DEBUG:root:[ Iteration 363 ] Training loss: 0.0200372
DEBUG:root:[ Iteration 366 ] Training loss: 0.0187324
DEBUG:root:[ Iteration 369 ] Training loss: 0.0217185
DEBUG:root:[ Iteration 372 ] Training loss: 0.0184122
DEBUG:root:[ Iteration 375 ] Training loss: 0.0189864
DEBUG:root:[ Iteration 378 ] Training loss: 0.0243352
DEBUG:root:[ Iteration 380 ] Test loss: 0.020587
DEBUG:root:[ Iteration 381 ] Training loss: 0.0195
DEBUG:root:[ Iteration 384 ] Training loss: 0.0179332
DEBUG:root:[ Iteration 387 ] Training loss: 0.0180142
DEBUG:root:[ Iteration 390 ] Training loss: 0.0181064
DEBUG:root:[ Iteration 393 ] Training loss: 0.0223166
DEBUG:root:[ Iteration 396 ] Training loss: 0.0156523
DEBUG:root:[ Iteration 399 ] Training loss: 0.0173303
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-01-2016_15h07m35s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-02-2016_10h59m08s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0946563
DEBUG:root:[ Iteration 0 ] Test loss: 0.103895
DEBUG:root:[ Iteration 3 ] Training loss: 0.0795807
DEBUG:root:[ Iteration 6 ] Training loss: 0.0903869
DEBUG:root:[ Iteration 9 ] Training loss: 0.0843845
DEBUG:root:[ Iteration 12 ] Training loss: 0.0654449
DEBUG:root:[ Iteration 15 ] Training loss: 0.0732332
DEBUG:root:[ Iteration 18 ] Training loss: 0.0730175
DEBUG:root:[ Iteration 20 ] Test loss: 0.063985
DEBUG:root:[ Iteration 21 ] Training loss: 0.0504753
DEBUG:root:[ Iteration 24 ] Training loss: 0.0676806
DEBUG:root:[ Iteration 27 ] Training loss: 0.0649161
DEBUG:root:[ Iteration 30 ] Training loss: 0.0574472
DEBUG:root:[ Iteration 33 ] Training loss: 0.0505734
DEBUG:root:[ Iteration 36 ] Training loss: 0.0554818
DEBUG:root:[ Iteration 39 ] Training loss: 0.0641726
DEBUG:root:[ Iteration 40 ] Test loss: 0.0541836
DEBUG:root:[ Iteration 42 ] Training loss: 0.0603152
DEBUG:root:[ Iteration 45 ] Training loss: 0.0647402
DEBUG:root:[ Iteration 48 ] Training loss: 0.0510938
DEBUG:root:[ Iteration 51 ] Training loss: 0.0524413
DEBUG:root:[ Iteration 54 ] Training loss: 0.0483557
DEBUG:root:[ Iteration 57 ] Training loss: 0.050465
DEBUG:root:[ Iteration 60 ] Training loss: 0.0517544
DEBUG:root:[ Iteration 60 ] Test loss: 0.0444942
DEBUG:root:[ Iteration 63 ] Training loss: 0.0556382
DEBUG:root:[ Iteration 66 ] Training loss: 0.0445339
DEBUG:root:[ Iteration 69 ] Training loss: 0.050575
DEBUG:root:[ Iteration 72 ] Training loss: 0.0501527
DEBUG:root:[ Iteration 75 ] Training loss: 0.0471222
DEBUG:root:[ Iteration 78 ] Training loss: 0.0415403
DEBUG:root:[ Iteration 80 ] Test loss: 0.0410343
DEBUG:root:[ Iteration 81 ] Training loss: 0.0471054
DEBUG:root:[ Iteration 84 ] Training loss: 0.0429085
DEBUG:root:[ Iteration 87 ] Training loss: 0.0438443
DEBUG:root:[ Iteration 90 ] Training loss: 0.051261
DEBUG:root:[ Iteration 93 ] Training loss: 0.0326244
DEBUG:root:[ Iteration 96 ] Training loss: 0.0491536
DEBUG:root:[ Iteration 99 ] Training loss: 0.0413342
DEBUG:root:[ Iteration 100 ] Test loss: 0.0453729
DEBUG:root:[ Iteration 102 ] Training loss: 0.0383762
DEBUG:root:[ Iteration 105 ] Training loss: 0.0425552
DEBUG:root:[ Iteration 108 ] Training loss: 0.0400223
DEBUG:root:[ Iteration 111 ] Training loss: 0.0390634
DEBUG:root:[ Iteration 114 ] Training loss: 0.0436227
DEBUG:root:[ Iteration 117 ] Training loss: 0.0363084
DEBUG:root:[ Iteration 120 ] Training loss: 0.0371064
DEBUG:root:[ Iteration 120 ] Test loss: 0.0353006
DEBUG:root:[ Iteration 123 ] Training loss: 0.0370753
DEBUG:root:[ Iteration 126 ] Training loss: 0.033379
DEBUG:root:[ Iteration 129 ] Training loss: 0.0320781
DEBUG:root:[ Iteration 132 ] Training loss: 0.0336751
DEBUG:root:[ Iteration 135 ] Training loss: 0.0301123
DEBUG:root:[ Iteration 138 ] Training loss: 0.0379891
DEBUG:root:[ Iteration 140 ] Test loss: 0.0322795
DEBUG:root:[ Iteration 141 ] Training loss: 0.0414421
DEBUG:root:[ Iteration 144 ] Training loss: 0.0436399
DEBUG:root:[ Iteration 147 ] Training loss: 0.0337587
DEBUG:root:[ Iteration 150 ] Training loss: 0.0500445
DEBUG:root:[ Iteration 153 ] Training loss: 0.0317855
DEBUG:root:[ Iteration 156 ] Training loss: 0.0343366
DEBUG:root:[ Iteration 159 ] Training loss: 0.0337616
DEBUG:root:[ Iteration 160 ] Test loss: 0.0295682
DEBUG:root:[ Iteration 162 ] Training loss: 0.0321637
DEBUG:root:[ Iteration 165 ] Training loss: 0.0372212
DEBUG:root:[ Iteration 168 ] Training loss: 0.0220865
DEBUG:root:[ Iteration 171 ] Training loss: 0.0248977
DEBUG:root:[ Iteration 174 ] Training loss: 0.0313986
DEBUG:root:[ Iteration 177 ] Training loss: 0.0348852
DEBUG:root:[ Iteration 180 ] Training loss: 0.0273609
DEBUG:root:[ Iteration 180 ] Test loss: 0.025916
DEBUG:root:[ Iteration 183 ] Training loss: 0.0292353
DEBUG:root:[ Iteration 186 ] Training loss: 0.0294095
DEBUG:root:[ Iteration 189 ] Training loss: 0.0251094
DEBUG:root:[ Iteration 192 ] Training loss: 0.0289633
DEBUG:root:[ Iteration 195 ] Training loss: 0.0216013
DEBUG:root:[ Iteration 198 ] Training loss: 0.0268434
DEBUG:root:[ Iteration 200 ] Test loss: 0.0313645
DEBUG:root:[ Iteration 201 ] Training loss: 0.0215541
DEBUG:root:[ Iteration 204 ] Training loss: 0.020123
DEBUG:root:[ Iteration 207 ] Training loss: 0.0277509
DEBUG:root:[ Iteration 210 ] Training loss: 0.0237323
DEBUG:root:[ Iteration 213 ] Training loss: 0.021569
DEBUG:root:[ Iteration 216 ] Training loss: 0.0252678
DEBUG:root:[ Iteration 219 ] Training loss: 0.0229475
DEBUG:root:[ Iteration 220 ] Test loss: 0.028455
DEBUG:root:[ Iteration 222 ] Training loss: 0.023529
DEBUG:root:[ Iteration 225 ] Training loss: 0.0196254
DEBUG:root:[ Iteration 228 ] Training loss: 0.0335652
DEBUG:root:[ Iteration 231 ] Training loss: 0.0224232
DEBUG:root:[ Iteration 234 ] Training loss: 0.0234408
DEBUG:root:[ Iteration 237 ] Training loss: 0.0256626
DEBUG:root:[ Iteration 240 ] Training loss: 0.0226249
DEBUG:root:[ Iteration 240 ] Test loss: 0.0235473
DEBUG:root:[ Iteration 243 ] Training loss: 0.0201531
DEBUG:root:[ Iteration 246 ] Training loss: 0.0302976
DEBUG:root:[ Iteration 249 ] Training loss: 0.0262132
DEBUG:root:[ Iteration 252 ] Training loss: 0.0249778
DEBUG:root:[ Iteration 255 ] Training loss: 0.0236018
DEBUG:root:[ Iteration 258 ] Training loss: 0.0356082
DEBUG:root:[ Iteration 260 ] Test loss: 0.0232654
DEBUG:root:[ Iteration 261 ] Training loss: 0.0188674
DEBUG:root:[ Iteration 264 ] Training loss: 0.023448
DEBUG:root:[ Iteration 267 ] Training loss: 0.0269765
DEBUG:root:[ Iteration 270 ] Training loss: 0.0212148
DEBUG:root:[ Iteration 273 ] Training loss: 0.0171028
DEBUG:root:[ Iteration 276 ] Training loss: 0.0221244
DEBUG:root:[ Iteration 279 ] Training loss: 0.0191083
DEBUG:root:[ Iteration 280 ] Test loss: 0.023827
DEBUG:root:[ Iteration 282 ] Training loss: 0.0243224
DEBUG:root:[ Iteration 285 ] Training loss: 0.0177624
DEBUG:root:[ Iteration 288 ] Training loss: 0.0156073
DEBUG:root:[ Iteration 291 ] Training loss: 0.0235201
DEBUG:root:[ Iteration 294 ] Training loss: 0.0183421
DEBUG:root:[ Iteration 297 ] Training loss: 0.0219615
DEBUG:root:[ Iteration 300 ] Training loss: 0.0188485
DEBUG:root:[ Iteration 300 ] Test loss: 0.0234506
DEBUG:root:[ Iteration 303 ] Training loss: 0.0179641
DEBUG:root:[ Iteration 306 ] Training loss: 0.0197575
DEBUG:root:[ Iteration 309 ] Training loss: 0.0182852
DEBUG:root:[ Iteration 312 ] Training loss: 0.0212449
DEBUG:root:[ Iteration 315 ] Training loss: 0.0204424
DEBUG:root:[ Iteration 318 ] Training loss: 0.0217754
DEBUG:root:[ Iteration 320 ] Test loss: 0.0232784
DEBUG:root:[ Iteration 321 ] Training loss: 0.0290525
DEBUG:root:[ Iteration 324 ] Training loss: 0.0173448
DEBUG:root:[ Iteration 327 ] Training loss: 0.0155816
DEBUG:root:[ Iteration 330 ] Training loss: 0.0224749
DEBUG:root:[ Iteration 333 ] Training loss: 0.0243366
DEBUG:root:[ Iteration 336 ] Training loss: 0.0234996
DEBUG:root:[ Iteration 339 ] Training loss: 0.0169607
DEBUG:root:[ Iteration 340 ] Test loss: 0.0265323
DEBUG:root:[ Iteration 342 ] Training loss: 0.0207932
DEBUG:root:[ Iteration 345 ] Training loss: 0.0168711
DEBUG:root:[ Iteration 348 ] Training loss: 0.0193152
DEBUG:root:[ Iteration 351 ] Training loss: 0.0148286
DEBUG:root:[ Iteration 354 ] Training loss: 0.0190819
DEBUG:root:[ Iteration 357 ] Training loss: 0.0188372
DEBUG:root:[ Iteration 360 ] Training loss: 0.017299
DEBUG:root:[ Iteration 360 ] Test loss: 0.0225296
DEBUG:root:[ Iteration 363 ] Training loss: 0.0197488
DEBUG:root:[ Iteration 366 ] Training loss: 0.0167014
DEBUG:root:[ Iteration 369 ] Training loss: 0.0145539
DEBUG:root:[ Iteration 372 ] Training loss: 0.0163056
DEBUG:root:[ Iteration 375 ] Training loss: 0.019864
DEBUG:root:[ Iteration 378 ] Training loss: 0.0228504
DEBUG:root:[ Iteration 380 ] Test loss: 0.0205716
DEBUG:root:[ Iteration 381 ] Training loss: 0.0220417
DEBUG:root:[ Iteration 384 ] Training loss: 0.0194102
DEBUG:root:[ Iteration 387 ] Training loss: 0.0197159
DEBUG:root:[ Iteration 390 ] Training loss: 0.0258011
DEBUG:root:[ Iteration 393 ] Training loss: 0.0166342
DEBUG:root:[ Iteration 396 ] Training loss: 0.0189376
DEBUG:root:[ Iteration 399 ] Training loss: 0.0217288
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-02-2016_14h49m19s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0951488
DEBUG:root:[ Iteration 0 ] Test loss: 0.0962862
DEBUG:root:[ Iteration 3 ] Training loss: 0.0740147
DEBUG:root:[ Iteration 6 ] Training loss: 0.0773805
DEBUG:root:[ Iteration 9 ] Training loss: 0.0800732
DEBUG:root:[ Iteration 12 ] Training loss: 0.0748286
DEBUG:root:[ Iteration 15 ] Training loss: 0.0747261
DEBUG:root:[ Iteration 18 ] Training loss: 0.0747032
DEBUG:root:[ Iteration 20 ] Test loss: 0.0757206
DEBUG:root:[ Iteration 21 ] Training loss: 0.064486
DEBUG:root:[ Iteration 24 ] Training loss: 0.0576007
DEBUG:root:[ Iteration 27 ] Training loss: 0.0676708
DEBUG:root:[ Iteration 30 ] Training loss: 0.0700812
DEBUG:root:[ Iteration 33 ] Training loss: 0.0539495
DEBUG:root:[ Iteration 36 ] Training loss: 0.0584586
DEBUG:root:[ Iteration 39 ] Training loss: 0.0614679
DEBUG:root:[ Iteration 40 ] Test loss: 0.0597079
DEBUG:root:[ Iteration 42 ] Training loss: 0.0658041
DEBUG:root:[ Iteration 45 ] Training loss: 0.0602185
DEBUG:root:[ Iteration 48 ] Training loss: 0.0475966
DEBUG:root:[ Iteration 51 ] Training loss: 0.0539804
DEBUG:root:[ Iteration 54 ] Training loss: 0.0453199
DEBUG:root:[ Iteration 57 ] Training loss: 0.0656445
DEBUG:root:[ Iteration 60 ] Training loss: 0.0496253
DEBUG:root:[ Iteration 60 ] Test loss: 0.0454386
DEBUG:root:[ Iteration 63 ] Training loss: 0.0352632
DEBUG:root:[ Iteration 66 ] Training loss: 0.0554493
DEBUG:root:[ Iteration 69 ] Training loss: 0.0446077
DEBUG:root:[ Iteration 72 ] Training loss: 0.0483915
DEBUG:root:[ Iteration 75 ] Training loss: 0.0508356
DEBUG:root:[ Iteration 78 ] Training loss: 0.0460625
DEBUG:root:[ Iteration 80 ] Test loss: 0.0468861
DEBUG:root:[ Iteration 81 ] Training loss: 0.0390766
DEBUG:root:[ Iteration 84 ] Training loss: 0.0490715
DEBUG:root:[ Iteration 87 ] Training loss: 0.039782
DEBUG:root:[ Iteration 90 ] Training loss: 0.0388072
DEBUG:root:[ Iteration 93 ] Training loss: 0.0479626
DEBUG:root:[ Iteration 96 ] Training loss: 0.0446784
DEBUG:root:[ Iteration 99 ] Training loss: 0.0346818
DEBUG:root:[ Iteration 100 ] Test loss: 0.0432594
DEBUG:root:[ Iteration 102 ] Training loss: 0.0379914
DEBUG:root:[ Iteration 105 ] Training loss: 0.0515455
DEBUG:root:[ Iteration 108 ] Training loss: 0.0384613
DEBUG:root:[ Iteration 111 ] Training loss: 0.0404869
DEBUG:root:[ Iteration 114 ] Training loss: 0.0387914
DEBUG:root:[ Iteration 117 ] Training loss: 0.0361032
DEBUG:root:[ Iteration 120 ] Training loss: 0.0403406
DEBUG:root:[ Iteration 120 ] Test loss: 0.0368466
DEBUG:root:[ Iteration 123 ] Training loss: 0.0425198
DEBUG:root:[ Iteration 126 ] Training loss: 0.0442574
DEBUG:root:[ Iteration 129 ] Training loss: 0.0416826
DEBUG:root:[ Iteration 132 ] Training loss: 0.0334279
DEBUG:root:[ Iteration 135 ] Training loss: 0.0375937
DEBUG:root:[ Iteration 138 ] Training loss: 0.0387099
DEBUG:root:[ Iteration 140 ] Test loss: 0.0397686
DEBUG:root:[ Iteration 141 ] Training loss: 0.027552
DEBUG:root:[ Iteration 144 ] Training loss: 0.0340421
DEBUG:root:[ Iteration 147 ] Training loss: 0.031165
DEBUG:root:[ Iteration 150 ] Training loss: 0.0310081
DEBUG:root:[ Iteration 153 ] Training loss: 0.0299685
DEBUG:root:[ Iteration 156 ] Training loss: 0.0322753
DEBUG:root:[ Iteration 159 ] Training loss: 0.0350347
DEBUG:root:[ Iteration 160 ] Test loss: 0.0397224
DEBUG:root:[ Iteration 162 ] Training loss: 0.0413604
DEBUG:root:[ Iteration 165 ] Training loss: 0.0321799
DEBUG:root:[ Iteration 168 ] Training loss: 0.0347743
DEBUG:root:[ Iteration 171 ] Training loss: 0.0337886
DEBUG:root:[ Iteration 174 ] Training loss: 0.0321436
DEBUG:root:[ Iteration 177 ] Training loss: 0.0333314
DEBUG:root:[ Iteration 180 ] Training loss: 0.0295585
DEBUG:root:[ Iteration 180 ] Test loss: 0.0341758
DEBUG:root:[ Iteration 183 ] Training loss: 0.0282434
DEBUG:root:[ Iteration 186 ] Training loss: 0.0300469
DEBUG:root:[ Iteration 189 ] Training loss: 0.0271508
DEBUG:root:[ Iteration 192 ] Training loss: 0.0218964
DEBUG:root:[ Iteration 195 ] Training loss: 0.0274563
DEBUG:root:[ Iteration 198 ] Training loss: 0.0255449
DEBUG:root:[ Iteration 200 ] Test loss: 0.0334653
DEBUG:root:[ Iteration 201 ] Training loss: 0.0303625
DEBUG:root:[ Iteration 204 ] Training loss: 0.0255078
DEBUG:root:[ Iteration 207 ] Training loss: 0.0281816
DEBUG:root:[ Iteration 210 ] Training loss: 0.0297888
DEBUG:root:[ Iteration 213 ] Training loss: 0.0411291
DEBUG:root:[ Iteration 216 ] Training loss: 0.026355
DEBUG:root:[ Iteration 219 ] Training loss: 0.0292494
DEBUG:root:[ Iteration 220 ] Test loss: 0.0332468
DEBUG:root:[ Iteration 222 ] Training loss: 0.0289148
DEBUG:root:[ Iteration 225 ] Training loss: 0.0327745
DEBUG:root:[ Iteration 228 ] Training loss: 0.0294714
DEBUG:root:[ Iteration 231 ] Training loss: 0.0235905
DEBUG:root:[ Iteration 234 ] Training loss: 0.0233944
DEBUG:root:[ Iteration 237 ] Training loss: 0.0250931
DEBUG:root:[ Iteration 240 ] Training loss: 0.0245204
DEBUG:root:[ Iteration 240 ] Test loss: 0.0312833
DEBUG:root:[ Iteration 243 ] Training loss: 0.0254392
DEBUG:root:[ Iteration 246 ] Training loss: 0.0179828
DEBUG:root:[ Iteration 249 ] Training loss: 0.0295807
DEBUG:root:[ Iteration 252 ] Training loss: 0.0226489
DEBUG:root:[ Iteration 255 ] Training loss: 0.0227805
DEBUG:root:[ Iteration 258 ] Training loss: 0.0206586
DEBUG:root:[ Iteration 260 ] Test loss: 0.0331775
DEBUG:root:[ Iteration 261 ] Training loss: 0.0210714
DEBUG:root:[ Iteration 264 ] Training loss: 0.0242951
DEBUG:root:[ Iteration 267 ] Training loss: 0.0239715
DEBUG:root:[ Iteration 270 ] Training loss: 0.0188045
DEBUG:root:[ Iteration 273 ] Training loss: 0.0273049
DEBUG:root:[ Iteration 276 ] Training loss: 0.0212771
DEBUG:root:[ Iteration 279 ] Training loss: 0.0240101
DEBUG:root:[ Iteration 280 ] Test loss: 0.0252099
DEBUG:root:[ Iteration 282 ] Training loss: 0.0212327
DEBUG:root:[ Iteration 285 ] Training loss: 0.0200037
DEBUG:root:[ Iteration 288 ] Training loss: 0.0269814
DEBUG:root:[ Iteration 291 ] Training loss: 0.0217435
DEBUG:root:[ Iteration 294 ] Training loss: 0.0185617
DEBUG:root:[ Iteration 297 ] Training loss: 0.0222792
DEBUG:root:[ Iteration 300 ] Training loss: 0.0211487
DEBUG:root:[ Iteration 300 ] Test loss: 0.0319185
DEBUG:root:[ Iteration 303 ] Training loss: 0.0243638
DEBUG:root:[ Iteration 306 ] Training loss: 0.0197366
DEBUG:root:[ Iteration 309 ] Training loss: 0.0207527
DEBUG:root:[ Iteration 312 ] Training loss: 0.0198436
DEBUG:root:[ Iteration 315 ] Training loss: 0.0246639
DEBUG:root:[ Iteration 318 ] Training loss: 0.025462
DEBUG:root:[ Iteration 320 ] Test loss: 0.0238701
DEBUG:root:[ Iteration 321 ] Training loss: 0.0292741
DEBUG:root:[ Iteration 324 ] Training loss: 0.0232394
DEBUG:root:[ Iteration 327 ] Training loss: 0.0164426
DEBUG:root:[ Iteration 330 ] Training loss: 0.0234529
DEBUG:root:[ Iteration 333 ] Training loss: 0.020516
DEBUG:root:[ Iteration 336 ] Training loss: 0.019018
DEBUG:root:[ Iteration 339 ] Training loss: 0.021149
DEBUG:root:[ Iteration 340 ] Test loss: 0.0212872
DEBUG:root:[ Iteration 342 ] Training loss: 0.0151146
DEBUG:root:[ Iteration 345 ] Training loss: 0.0226957
DEBUG:root:[ Iteration 348 ] Training loss: 0.0269994
DEBUG:root:[ Iteration 351 ] Training loss: 0.0214151
DEBUG:root:[ Iteration 354 ] Training loss: 0.0159783
DEBUG:root:[ Iteration 357 ] Training loss: 0.0178142
DEBUG:root:[ Iteration 360 ] Training loss: 0.0143641
DEBUG:root:[ Iteration 360 ] Test loss: 0.0227207
DEBUG:root:[ Iteration 363 ] Training loss: 0.0171045
DEBUG:root:[ Iteration 366 ] Training loss: 0.0199195
DEBUG:root:[ Iteration 369 ] Training loss: 0.0178752
DEBUG:root:[ Iteration 372 ] Training loss: 0.0166092
DEBUG:root:[ Iteration 375 ] Training loss: 0.0190126
DEBUG:root:[ Iteration 378 ] Training loss: 0.0170506
DEBUG:root:[ Iteration 380 ] Test loss: 0.0210238
DEBUG:root:[ Iteration 381 ] Training loss: 0.0170221
DEBUG:root:[ Iteration 384 ] Training loss: 0.0182405
DEBUG:root:[ Iteration 387 ] Training loss: 0.0192725
DEBUG:root:[ Iteration 390 ] Training loss: 0.0237766
DEBUG:root:[ Iteration 393 ] Training loss: 0.0169153
DEBUG:root:[ Iteration 396 ] Training loss: 0.02186
DEBUG:root:[ Iteration 399 ] Training loss: 0.0143738
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-02-2016_19h37m17s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0493055
DEBUG:root:[ Iteration 0 ] Test loss: 0.0511024
DEBUG:root:[ Iteration 3 ] Training loss: 0.0470843
DEBUG:root:[ Iteration 6 ] Training loss: 0.0508613
DEBUG:root:[ Iteration 9 ] Training loss: 0.0543269
DEBUG:root:[ Iteration 12 ] Training loss: 0.0454127
DEBUG:root:[ Iteration 15 ] Training loss: 0.028496
DEBUG:root:[ Iteration 18 ] Training loss: 0.0501636
DEBUG:root:[ Iteration 20 ] Test loss: 0.0410767
DEBUG:root:[ Iteration 21 ] Training loss: 0.0378743
DEBUG:root:[ Iteration 24 ] Training loss: 0.0256982
DEBUG:root:[ Iteration 27 ] Training loss: 0.0310948
DEBUG:root:[ Iteration 30 ] Training loss: 0.0296284
DEBUG:root:[ Iteration 33 ] Training loss: 0.0365602
DEBUG:root:[ Iteration 36 ] Training loss: 0.041841
DEBUG:root:[ Iteration 39 ] Training loss: 0.0227892
DEBUG:root:[ Iteration 40 ] Test loss: 0.0356648
DEBUG:root:[ Iteration 42 ] Training loss: 0.0283797
DEBUG:root:[ Iteration 45 ] Training loss: 0.0281977
DEBUG:root:[ Iteration 48 ] Training loss: 0.0345046
DEBUG:root:[ Iteration 51 ] Training loss: 0.0207016
DEBUG:root:[ Iteration 54 ] Training loss: 0.0352538
DEBUG:root:[ Iteration 57 ] Training loss: 0.0293233
DEBUG:root:[ Iteration 60 ] Training loss: 0.0243416
DEBUG:root:[ Iteration 60 ] Test loss: 0.0359699
DEBUG:root:[ Iteration 63 ] Training loss: 0.0246092
DEBUG:root:[ Iteration 66 ] Training loss: 0.0334466
DEBUG:root:[ Iteration 69 ] Training loss: 0.0353224
DEBUG:root:[ Iteration 72 ] Training loss: 0.0367168
DEBUG:root:[ Iteration 75 ] Training loss: 0.0207489
DEBUG:root:[ Iteration 78 ] Training loss: 0.0239569
DEBUG:root:[ Iteration 80 ] Test loss: 0.0365671
DEBUG:root:[ Iteration 81 ] Training loss: 0.0318802
DEBUG:root:[ Iteration 84 ] Training loss: 0.029356
DEBUG:root:[ Iteration 87 ] Training loss: 0.0296231
DEBUG:root:[ Iteration 90 ] Training loss: 0.0275882
DEBUG:root:[ Iteration 93 ] Training loss: 0.0283403
DEBUG:root:[ Iteration 96 ] Training loss: 0.0249141
DEBUG:root:[ Iteration 99 ] Training loss: 0.0289928
DEBUG:root:[ Iteration 100 ] Test loss: 0.03333
DEBUG:root:[ Iteration 102 ] Training loss: 0.0200514
DEBUG:root:[ Iteration 105 ] Training loss: 0.0285451
DEBUG:root:[ Iteration 108 ] Training loss: 0.0241245
DEBUG:root:[ Iteration 111 ] Training loss: 0.0304832
DEBUG:root:[ Iteration 114 ] Training loss: 0.0240417
DEBUG:root:[ Iteration 117 ] Training loss: 0.0270403
DEBUG:root:[ Iteration 120 ] Training loss: 0.0219483
DEBUG:root:[ Iteration 120 ] Test loss: 0.0284385
DEBUG:root:[ Iteration 123 ] Training loss: 0.0242949
DEBUG:root:[ Iteration 126 ] Training loss: 0.0217381
DEBUG:root:[ Iteration 129 ] Training loss: 0.0230507
DEBUG:root:[ Iteration 132 ] Training loss: 0.023321
DEBUG:root:[ Iteration 135 ] Training loss: 0.0247141
DEBUG:root:[ Iteration 138 ] Training loss: 0.0260312
DEBUG:root:[ Iteration 140 ] Test loss: 0.0302478
DEBUG:root:[ Iteration 141 ] Training loss: 0.0160546
DEBUG:root:[ Iteration 144 ] Training loss: 0.0362313
DEBUG:root:[ Iteration 147 ] Training loss: 0.0315293
DEBUG:root:[ Iteration 150 ] Training loss: 0.0279726
DEBUG:root:[ Iteration 153 ] Training loss: 0.0198963
DEBUG:root:[ Iteration 156 ] Training loss: 0.0261734
DEBUG:root:[ Iteration 159 ] Training loss: 0.0221279
DEBUG:root:[ Iteration 160 ] Test loss: 0.0332267
DEBUG:root:[ Iteration 162 ] Training loss: 0.0224445
DEBUG:root:[ Iteration 165 ] Training loss: 0.0278656
DEBUG:root:[ Iteration 168 ] Training loss: 0.0182095
DEBUG:root:[ Iteration 171 ] Training loss: 0.026434
DEBUG:root:[ Iteration 174 ] Training loss: 0.0224514
DEBUG:root:[ Iteration 177 ] Training loss: 0.0206536
DEBUG:root:[ Iteration 180 ] Training loss: 0.027101
DEBUG:root:[ Iteration 180 ] Test loss: 0.0338104
DEBUG:root:[ Iteration 183 ] Training loss: 0.0264907
DEBUG:root:[ Iteration 186 ] Training loss: 0.0194881
DEBUG:root:[ Iteration 189 ] Training loss: 0.0289707
DEBUG:root:[ Iteration 192 ] Training loss: 0.0216816
DEBUG:root:[ Iteration 195 ] Training loss: 0.0226711
DEBUG:root:[ Iteration 198 ] Training loss: 0.0250312
DEBUG:root:[ Iteration 200 ] Test loss: 0.0295379
DEBUG:root:[ Iteration 201 ] Training loss: 0.0262034
DEBUG:root:[ Iteration 204 ] Training loss: 0.0168969
DEBUG:root:[ Iteration 207 ] Training loss: 0.0221611
DEBUG:root:[ Iteration 210 ] Training loss: 0.0173838
DEBUG:root:[ Iteration 213 ] Training loss: 0.0193183
DEBUG:root:[ Iteration 216 ] Training loss: 0.0203878
DEBUG:root:[ Iteration 219 ] Training loss: 0.0240155
DEBUG:root:[ Iteration 220 ] Test loss: 0.0325108
DEBUG:root:[ Iteration 222 ] Training loss: 0.0221788
DEBUG:root:[ Iteration 225 ] Training loss: 0.0237587
DEBUG:root:[ Iteration 228 ] Training loss: 0.0221392
DEBUG:root:[ Iteration 231 ] Training loss: 0.0207579
DEBUG:root:[ Iteration 234 ] Training loss: 0.0193759
DEBUG:root:[ Iteration 237 ] Training loss: 0.0280022
DEBUG:root:[ Iteration 240 ] Training loss: 0.0232022
DEBUG:root:[ Iteration 240 ] Test loss: 0.0302465
DEBUG:root:[ Iteration 243 ] Training loss: 0.0225351
DEBUG:root:[ Iteration 246 ] Training loss: 0.0226334
DEBUG:root:[ Iteration 249 ] Training loss: 0.0256641
DEBUG:root:[ Iteration 252 ] Training loss: 0.0183529
DEBUG:root:[ Iteration 255 ] Training loss: 0.0201218
DEBUG:root:[ Iteration 258 ] Training loss: 0.023461
DEBUG:root:[ Iteration 260 ] Test loss: 0.0359252
DEBUG:root:[ Iteration 261 ] Training loss: 0.0180345
DEBUG:root:[ Iteration 264 ] Training loss: 0.0211386
DEBUG:root:[ Iteration 267 ] Training loss: 0.0200716
DEBUG:root:[ Iteration 270 ] Training loss: 0.0219303
DEBUG:root:[ Iteration 273 ] Training loss: 0.0254137
DEBUG:root:[ Iteration 276 ] Training loss: 0.0219335
DEBUG:root:[ Iteration 279 ] Training loss: 0.0231418
DEBUG:root:[ Iteration 280 ] Test loss: 0.0307022
DEBUG:root:[ Iteration 282 ] Training loss: 0.0213963
DEBUG:root:[ Iteration 285 ] Training loss: 0.0197759
DEBUG:root:[ Iteration 288 ] Training loss: 0.0181693
DEBUG:root:[ Iteration 291 ] Training loss: 0.0191542
DEBUG:root:[ Iteration 294 ] Training loss: 0.0194728
DEBUG:root:[ Iteration 297 ] Training loss: 0.0218857
DEBUG:root:[ Iteration 300 ] Training loss: 0.02288
DEBUG:root:[ Iteration 300 ] Test loss: 0.0258431
DEBUG:root:[ Iteration 303 ] Training loss: 0.0142228
DEBUG:root:[ Iteration 306 ] Training loss: 0.0201664
DEBUG:root:[ Iteration 309 ] Training loss: 0.0234062
DEBUG:root:[ Iteration 312 ] Training loss: 0.0176796
DEBUG:root:[ Iteration 315 ] Training loss: 0.0203477
DEBUG:root:[ Iteration 318 ] Training loss: 0.0227317
DEBUG:root:[ Iteration 320 ] Test loss: 0.0323793
DEBUG:root:[ Iteration 321 ] Training loss: 0.0228912
DEBUG:root:[ Iteration 324 ] Training loss: 0.0289955
DEBUG:root:[ Iteration 327 ] Training loss: 0.0205043
DEBUG:root:[ Iteration 330 ] Training loss: 0.021542
DEBUG:root:[ Iteration 333 ] Training loss: 0.0145493
DEBUG:root:[ Iteration 336 ] Training loss: 0.017381
DEBUG:root:[ Iteration 339 ] Training loss: 0.0209703
DEBUG:root:[ Iteration 340 ] Test loss: 0.0317141
DEBUG:root:[ Iteration 342 ] Training loss: 0.0222083
DEBUG:root:[ Iteration 345 ] Training loss: 0.0160861
DEBUG:root:[ Iteration 348 ] Training loss: 0.0245696
DEBUG:root:[ Iteration 351 ] Training loss: 0.0216939
DEBUG:root:[ Iteration 354 ] Training loss: 0.0168737
DEBUG:root:[ Iteration 357 ] Training loss: 0.0177736
DEBUG:root:[ Iteration 360 ] Training loss: 0.0217546
DEBUG:root:[ Iteration 360 ] Test loss: 0.0296234
DEBUG:root:[ Iteration 363 ] Training loss: 0.0181996
DEBUG:root:[ Iteration 366 ] Training loss: 0.0177206
DEBUG:root:[ Iteration 369 ] Training loss: 0.0199944
DEBUG:root:[ Iteration 372 ] Training loss: 0.0220837
DEBUG:root:[ Iteration 375 ] Training loss: 0.0195475
DEBUG:root:[ Iteration 378 ] Training loss: 0.0181085
DEBUG:root:[ Iteration 380 ] Test loss: 0.0258025
DEBUG:root:[ Iteration 381 ] Training loss: 0.0206448
DEBUG:root:[ Iteration 384 ] Training loss: 0.0201932
DEBUG:root:[ Iteration 387 ] Training loss: 0.0198843
DEBUG:root:[ Iteration 390 ] Training loss: 0.0213302
DEBUG:root:[ Iteration 393 ] Training loss: 0.0163281
DEBUG:root:[ Iteration 396 ] Training loss: 0.0209574
DEBUG:root:[ Iteration 399 ] Training loss: 0.0159307
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-05-2016_18h11m08s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0719726
DEBUG:root:[ Iteration 0 ] Test loss: 0.0779446
DEBUG:root:[ Iteration 3 ] Training loss: 0.0468065
DEBUG:root:[ Iteration 6 ] Training loss: 0.0590944
DEBUG:root:[ Iteration 9 ] Training loss: 0.0536196
DEBUG:root:[ Iteration 12 ] Training loss: 0.0340847
DEBUG:root:[ Iteration 15 ] Training loss: 0.0374243
DEBUG:root:[ Iteration 18 ] Training loss: 0.0447276
DEBUG:root:[ Iteration 20 ] Test loss: 0.0506374
DEBUG:root:[ Iteration 21 ] Training loss: 0.0349355
DEBUG:root:[ Iteration 24 ] Training loss: 0.037688
DEBUG:root:[ Iteration 27 ] Training loss: 0.0322145
DEBUG:root:[ Iteration 30 ] Training loss: 0.0338579
DEBUG:root:[ Iteration 33 ] Training loss: 0.031966
DEBUG:root:[ Iteration 36 ] Training loss: 0.0365047
DEBUG:root:[ Iteration 39 ] Training loss: 0.0278301
DEBUG:root:[ Iteration 40 ] Test loss: 0.0447866
DEBUG:root:[ Iteration 42 ] Training loss: 0.031403
DEBUG:root:[ Iteration 45 ] Training loss: 0.0323088
DEBUG:root:[ Iteration 48 ] Training loss: 0.0346661
DEBUG:root:[ Iteration 51 ] Training loss: 0.0277327
DEBUG:root:[ Iteration 54 ] Training loss: 0.0282795
DEBUG:root:[ Iteration 57 ] Training loss: 0.0280643
DEBUG:root:[ Iteration 60 ] Training loss: 0.0278917
DEBUG:root:[ Iteration 60 ] Test loss: 0.034063
DEBUG:root:[ Iteration 63 ] Training loss: 0.032308
DEBUG:root:[ Iteration 66 ] Training loss: 0.0289952
DEBUG:root:[ Iteration 69 ] Training loss: 0.0269555
DEBUG:root:[ Iteration 72 ] Training loss: 0.0358656
DEBUG:root:[ Iteration 75 ] Training loss: 0.0267272
DEBUG:root:[ Iteration 78 ] Training loss: 0.0231541
DEBUG:root:[ Iteration 80 ] Test loss: 0.0330044
DEBUG:root:[ Iteration 81 ] Training loss: 0.0319591
DEBUG:root:[ Iteration 84 ] Training loss: 0.027755
DEBUG:root:[ Iteration 87 ] Training loss: 0.0227264
DEBUG:root:[ Iteration 90 ] Training loss: 0.0247972
DEBUG:root:[ Iteration 93 ] Training loss: 0.0304373
DEBUG:root:[ Iteration 96 ] Training loss: 0.0214171
DEBUG:root:[ Iteration 99 ] Training loss: 0.0283494
DEBUG:root:[ Iteration 100 ] Test loss: 0.0337971
DEBUG:root:[ Iteration 102 ] Training loss: 0.0248441
DEBUG:root:[ Iteration 105 ] Training loss: 0.0210273
DEBUG:root:[ Iteration 108 ] Training loss: 0.0235899
DEBUG:root:[ Iteration 111 ] Training loss: 0.0271162
DEBUG:root:[ Iteration 114 ] Training loss: 0.0265485
DEBUG:root:[ Iteration 117 ] Training loss: 0.0248836
DEBUG:root:[ Iteration 120 ] Training loss: 0.0290624
DEBUG:root:[ Iteration 120 ] Test loss: 0.027286
DEBUG:root:[ Iteration 123 ] Training loss: 0.0296005
DEBUG:root:[ Iteration 126 ] Training loss: 0.022232
DEBUG:root:[ Iteration 129 ] Training loss: 0.0206188
DEBUG:root:[ Iteration 132 ] Training loss: 0.0227358
DEBUG:root:[ Iteration 135 ] Training loss: 0.018484
DEBUG:root:[ Iteration 138 ] Training loss: 0.0223373
DEBUG:root:[ Iteration 140 ] Test loss: 0.0349804
DEBUG:root:[ Iteration 141 ] Training loss: 0.0252667
DEBUG:root:[ Iteration 144 ] Training loss: 0.0235232
DEBUG:root:[ Iteration 147 ] Training loss: 0.0218131
DEBUG:root:[ Iteration 150 ] Training loss: 0.0250111
DEBUG:root:[ Iteration 153 ] Training loss: 0.0260284
DEBUG:root:[ Iteration 156 ] Training loss: 0.0247655
DEBUG:root:[ Iteration 159 ] Training loss: 0.0270766
DEBUG:root:[ Iteration 160 ] Test loss: 0.0300568
DEBUG:root:[ Iteration 162 ] Training loss: 0.0275317
DEBUG:root:[ Iteration 165 ] Training loss: 0.0194558
DEBUG:root:[ Iteration 168 ] Training loss: 0.0218937
DEBUG:root:[ Iteration 171 ] Training loss: 0.0238047
DEBUG:root:[ Iteration 174 ] Training loss: 0.0245001
DEBUG:root:[ Iteration 177 ] Training loss: 0.027968
DEBUG:root:[ Iteration 180 ] Training loss: 0.0254751
DEBUG:root:[ Iteration 180 ] Test loss: 0.0325457
DEBUG:root:[ Iteration 183 ] Training loss: 0.0194216
DEBUG:root:[ Iteration 186 ] Training loss: 0.0218442
DEBUG:root:[ Iteration 189 ] Training loss: 0.0237537
DEBUG:root:[ Iteration 192 ] Training loss: 0.0179007
DEBUG:root:[ Iteration 195 ] Training loss: 0.0188291
DEBUG:root:[ Iteration 198 ] Training loss: 0.0173977
DEBUG:root:[ Iteration 200 ] Test loss: 0.0279889
DEBUG:root:[ Iteration 201 ] Training loss: 0.0171436
DEBUG:root:[ Iteration 204 ] Training loss: 0.0269097
DEBUG:root:[ Iteration 207 ] Training loss: 0.0230289
DEBUG:root:[ Iteration 210 ] Training loss: 0.0182395
DEBUG:root:[ Iteration 213 ] Training loss: 0.0134323
DEBUG:root:[ Iteration 216 ] Training loss: 0.0200816
DEBUG:root:[ Iteration 219 ] Training loss: 0.0160721
DEBUG:root:[ Iteration 220 ] Test loss: 0.0277748
DEBUG:root:[ Iteration 222 ] Training loss: 0.0183507
DEBUG:root:[ Iteration 225 ] Training loss: 0.0199357
DEBUG:root:[ Iteration 228 ] Training loss: 0.0218499
DEBUG:root:[ Iteration 231 ] Training loss: 0.0281064
DEBUG:root:[ Iteration 234 ] Training loss: 0.0189569
DEBUG:root:[ Iteration 237 ] Training loss: 0.0316186
DEBUG:root:[ Iteration 240 ] Training loss: 0.0175845
DEBUG:root:[ Iteration 240 ] Test loss: 0.0345789
DEBUG:root:[ Iteration 243 ] Training loss: 0.0169236
DEBUG:root:[ Iteration 246 ] Training loss: 0.0189914
DEBUG:root:[ Iteration 249 ] Training loss: 0.0231324
DEBUG:root:[ Iteration 252 ] Training loss: 0.0156862
DEBUG:root:[ Iteration 255 ] Training loss: 0.0190831
DEBUG:root:[ Iteration 258 ] Training loss: 0.0223207
DEBUG:root:[ Iteration 260 ] Test loss: 0.0304001
DEBUG:root:[ Iteration 261 ] Training loss: 0.0207984
DEBUG:root:[ Iteration 264 ] Training loss: 0.0215124
DEBUG:root:[ Iteration 267 ] Training loss: 0.0193259
DEBUG:root:[ Iteration 270 ] Training loss: 0.0202041
DEBUG:root:[ Iteration 273 ] Training loss: 0.0202968
DEBUG:root:[ Iteration 276 ] Training loss: 0.0218077
DEBUG:root:[ Iteration 279 ] Training loss: 0.0160112
DEBUG:root:[ Iteration 280 ] Test loss: 0.026526
DEBUG:root:[ Iteration 282 ] Training loss: 0.0217985
DEBUG:root:[ Iteration 285 ] Training loss: 0.0263901
DEBUG:root:[ Iteration 288 ] Training loss: 0.01683
DEBUG:root:[ Iteration 291 ] Training loss: 0.0191265
DEBUG:root:[ Iteration 294 ] Training loss: 0.0202617
DEBUG:root:[ Iteration 297 ] Training loss: 0.0200704
DEBUG:root:[ Iteration 300 ] Training loss: 0.0190507
DEBUG:root:[ Iteration 300 ] Test loss: 0.0224042
DEBUG:root:[ Iteration 303 ] Training loss: 0.01795
DEBUG:root:[ Iteration 306 ] Training loss: 0.0211065
DEBUG:root:[ Iteration 309 ] Training loss: 0.0197468
DEBUG:root:[ Iteration 312 ] Training loss: 0.0158838
DEBUG:root:[ Iteration 315 ] Training loss: 0.023136
DEBUG:root:[ Iteration 318 ] Training loss: 0.0163199
DEBUG:root:[ Iteration 320 ] Test loss: 0.0231629
DEBUG:root:[ Iteration 321 ] Training loss: 0.0155467
DEBUG:root:[ Iteration 324 ] Training loss: 0.0170245
DEBUG:root:[ Iteration 327 ] Training loss: 0.0192639
DEBUG:root:[ Iteration 330 ] Training loss: 0.0221774
DEBUG:root:[ Iteration 333 ] Training loss: 0.0159777
DEBUG:root:[ Iteration 336 ] Training loss: 0.0155238
DEBUG:root:[ Iteration 339 ] Training loss: 0.0201643
DEBUG:root:[ Iteration 340 ] Test loss: 0.02957
DEBUG:root:[ Iteration 342 ] Training loss: 0.0149031
DEBUG:root:[ Iteration 345 ] Training loss: 0.0138751
DEBUG:root:[ Iteration 348 ] Training loss: 0.0172963
DEBUG:root:[ Iteration 351 ] Training loss: 0.0186786
DEBUG:root:[ Iteration 354 ] Training loss: 0.025824
DEBUG:root:[ Iteration 357 ] Training loss: 0.0254528
DEBUG:root:[ Iteration 360 ] Training loss: 0.0174123
DEBUG:root:[ Iteration 360 ] Test loss: 0.0283324
DEBUG:root:[ Iteration 363 ] Training loss: 0.0190365
DEBUG:root:[ Iteration 366 ] Training loss: 0.0159052
DEBUG:root:[ Iteration 369 ] Training loss: 0.0148615
DEBUG:root:[ Iteration 372 ] Training loss: 0.0222605
DEBUG:root:[ Iteration 375 ] Training loss: 0.013716
DEBUG:root:[ Iteration 378 ] Training loss: 0.0194708
DEBUG:root:[ Iteration 380 ] Test loss: 0.024927
DEBUG:root:[ Iteration 381 ] Training loss: 0.0187985
DEBUG:root:[ Iteration 384 ] Training loss: 0.0165991
DEBUG:root:[ Iteration 387 ] Training loss: 0.0142244
DEBUG:root:[ Iteration 390 ] Training loss: 0.0190371
DEBUG:root:[ Iteration 393 ] Training loss: 0.019971
DEBUG:root:[ Iteration 396 ] Training loss: 0.0143632
DEBUG:root:[ Iteration 399 ] Training loss: 0.0190031
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-11-2016_14h51m00s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0174384
DEBUG:root:[ Iteration 0 ] Test loss: 0.031672
DEBUG:root:[ Iteration 3 ] Training loss: 0.0233557
DEBUG:root:[ Iteration 6 ] Training loss: 0.0128365
DEBUG:root:[ Iteration 9 ] Training loss: 0.0203918
DEBUG:root:[ Iteration 12 ] Training loss: 0.0180163
DEBUG:root:[ Iteration 15 ] Training loss: 0.0208967
DEBUG:root:[ Iteration 18 ] Training loss: 0.0173427
DEBUG:root:[ Iteration 20 ] Test loss: 0.0305078
DEBUG:root:[ Iteration 21 ] Training loss: 0.0156245
DEBUG:root:[ Iteration 24 ] Training loss: 0.0177528
DEBUG:root:[ Iteration 27 ] Training loss: 0.0162867
DEBUG:root:[ Iteration 30 ] Training loss: 0.0197296
DEBUG:root:[ Iteration 33 ] Training loss: 0.0212926
DEBUG:root:[ Iteration 36 ] Training loss: 0.0187061
DEBUG:root:[ Iteration 39 ] Training loss: 0.0128358
DEBUG:root:[ Iteration 40 ] Test loss: 0.0229171
DEBUG:root:[ Iteration 42 ] Training loss: 0.0201733
DEBUG:root:[ Iteration 45 ] Training loss: 0.0176719
DEBUG:root:[ Iteration 48 ] Training loss: 0.0152328
DEBUG:root:[ Iteration 51 ] Training loss: 0.0112996
DEBUG:root:[ Iteration 54 ] Training loss: 0.0228071
DEBUG:root:[ Iteration 57 ] Training loss: 0.0178087
DEBUG:root:[ Iteration 60 ] Training loss: 0.0116114
DEBUG:root:[ Iteration 60 ] Test loss: 0.0305932
DEBUG:root:[ Iteration 63 ] Training loss: 0.018369
DEBUG:root:[ Iteration 66 ] Training loss: 0.017279
DEBUG:root:[ Iteration 69 ] Training loss: 0.0181686
DEBUG:root:[ Iteration 72 ] Training loss: 0.0128116
DEBUG:root:[ Iteration 75 ] Training loss: 0.0212592
DEBUG:root:[ Iteration 78 ] Training loss: 0.0174361
DEBUG:root:[ Iteration 80 ] Test loss: 0.0263848
DEBUG:root:[ Iteration 81 ] Training loss: 0.0158129
DEBUG:root:[ Iteration 84 ] Training loss: 0.0156027
DEBUG:root:[ Iteration 87 ] Training loss: 0.0164363
DEBUG:root:[ Iteration 90 ] Training loss: 0.0150477
DEBUG:root:[ Iteration 93 ] Training loss: 0.0202409
DEBUG:root:[ Iteration 96 ] Training loss: 0.0232011
DEBUG:root:[ Iteration 99 ] Training loss: 0.0144679
DEBUG:root:[ Iteration 100 ] Test loss: 0.0261353
DEBUG:root:[ Iteration 102 ] Training loss: 0.0124569
DEBUG:root:[ Iteration 105 ] Training loss: 0.0197619
DEBUG:root:[ Iteration 108 ] Training loss: 0.0176813
DEBUG:root:[ Iteration 111 ] Training loss: 0.0165732
DEBUG:root:[ Iteration 114 ] Training loss: 0.0187698
DEBUG:root:[ Iteration 117 ] Training loss: 0.0188388
DEBUG:root:[ Iteration 120 ] Training loss: 0.0191054
DEBUG:root:[ Iteration 120 ] Test loss: 0.0261555
DEBUG:root:[ Iteration 123 ] Training loss: 0.0167342
DEBUG:root:[ Iteration 126 ] Training loss: 0.016957
DEBUG:root:[ Iteration 129 ] Training loss: 0.0163876
DEBUG:root:[ Iteration 132 ] Training loss: 0.0173026
DEBUG:root:[ Iteration 135 ] Training loss: 0.0180615
DEBUG:root:[ Iteration 138 ] Training loss: 0.027281
DEBUG:root:[ Iteration 140 ] Test loss: 0.0287054
DEBUG:root:[ Iteration 141 ] Training loss: 0.0149228
DEBUG:root:[ Iteration 144 ] Training loss: 0.0178387
DEBUG:root:[ Iteration 147 ] Training loss: 0.0183907
DEBUG:root:[ Iteration 150 ] Training loss: 0.0124627
DEBUG:root:[ Iteration 153 ] Training loss: 0.0164203
DEBUG:root:[ Iteration 156 ] Training loss: 0.0159098
DEBUG:root:[ Iteration 159 ] Training loss: 0.0192552
DEBUG:root:[ Iteration 160 ] Test loss: 0.0348722
DEBUG:root:[ Iteration 162 ] Training loss: 0.0152728
DEBUG:root:[ Iteration 165 ] Training loss: 0.0171857
DEBUG:root:[ Iteration 168 ] Training loss: 0.0172486
DEBUG:root:[ Iteration 171 ] Training loss: 0.0163668
DEBUG:root:[ Iteration 174 ] Training loss: 0.0144284
DEBUG:root:[ Iteration 177 ] Training loss: 0.0160745
DEBUG:root:[ Iteration 180 ] Training loss: 0.0197367
DEBUG:root:[ Iteration 180 ] Test loss: 0.027109
DEBUG:root:[ Iteration 183 ] Training loss: 0.0141273
DEBUG:root:[ Iteration 186 ] Training loss: 0.0172146
DEBUG:root:[ Iteration 189 ] Training loss: 0.0156266
DEBUG:root:[ Iteration 192 ] Training loss: 0.0205704
DEBUG:root:[ Iteration 195 ] Training loss: 0.0120409
DEBUG:root:[ Iteration 198 ] Training loss: 0.0159085
DEBUG:root:[ Iteration 200 ] Test loss: 0.0258046
DEBUG:root:[ Iteration 201 ] Training loss: 0.0103514
DEBUG:root:[ Iteration 204 ] Training loss: 0.0132841
DEBUG:root:[ Iteration 207 ] Training loss: 0.0167888
DEBUG:root:[ Iteration 210 ] Training loss: 0.0182329
DEBUG:root:[ Iteration 213 ] Training loss: 0.0161896
DEBUG:root:[ Iteration 216 ] Training loss: 0.0173593
DEBUG:root:[ Iteration 219 ] Training loss: 0.0106775
DEBUG:root:[ Iteration 220 ] Test loss: 0.0312186
DEBUG:root:[ Iteration 222 ] Training loss: 0.0179754
DEBUG:root:[ Iteration 225 ] Training loss: 0.0159774
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-11-2016_17h14m03s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0520803
DEBUG:root:[ Iteration 0 ] Test loss: 0.0548001
DEBUG:root:[ Iteration 3 ] Training loss: 0.0301089
DEBUG:root:[ Iteration 6 ] Training loss: 0.0313383
DEBUG:root:[ Iteration 9 ] Training loss: 0.0351579
DEBUG:root:[ Iteration 12 ] Training loss: 0.0234025
DEBUG:root:[ Iteration 15 ] Training loss: 0.0233025
DEBUG:root:[ Iteration 18 ] Training loss: 0.0219979
DEBUG:root:[ Iteration 20 ] Test loss: 0.0190854
DEBUG:root:[ Iteration 21 ] Training loss: 0.0194314
DEBUG:root:[ Iteration 24 ] Training loss: 0.0219454
DEBUG:root:[ Iteration 27 ] Training loss: 0.0195876
DEBUG:root:[ Iteration 30 ] Training loss: 0.017311
DEBUG:root:[ Iteration 33 ] Training loss: 0.0152165
DEBUG:root:[ Iteration 36 ] Training loss: 0.0129579
DEBUG:root:[ Iteration 39 ] Training loss: 0.0131219
DEBUG:root:[ Iteration 40 ] Test loss: 0.0151272
DEBUG:root:[ Iteration 42 ] Training loss: 0.0153571
DEBUG:root:[ Iteration 45 ] Training loss: 0.0122011
DEBUG:root:[ Iteration 48 ] Training loss: 0.0118715
DEBUG:root:[ Iteration 51 ] Training loss: 0.0118138
DEBUG:root:[ Iteration 54 ] Training loss: 0.0105708
DEBUG:root:[ Iteration 57 ] Training loss: 0.0105252
DEBUG:root:[ Iteration 60 ] Training loss: 0.0106372
DEBUG:root:[ Iteration 60 ] Test loss: 0.0128951
DEBUG:root:[ Iteration 63 ] Training loss: 0.00990074
DEBUG:root:[ Iteration 66 ] Training loss: 0.00879869
DEBUG:root:[ Iteration 69 ] Training loss: 0.0100772
DEBUG:root:[ Iteration 72 ] Training loss: 0.0114348
DEBUG:root:[ Iteration 75 ] Training loss: 0.00812067
DEBUG:root:[ Iteration 78 ] Training loss: 0.00801229
DEBUG:root:[ Iteration 80 ] Test loss: 0.0113826
DEBUG:root:[ Iteration 81 ] Training loss: 0.010291
DEBUG:root:[ Iteration 84 ] Training loss: 0.0072843
DEBUG:root:[ Iteration 87 ] Training loss: 0.00926538
DEBUG:root:[ Iteration 90 ] Training loss: 0.00658358
DEBUG:root:[ Iteration 93 ] Training loss: 0.00730297
DEBUG:root:[ Iteration 96 ] Training loss: 0.00678171
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-12-2016_13h34m21s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0450179
DEBUG:root:[ Iteration 0 ] Test loss: 0.0548001
DEBUG:root:[ Iteration 3 ] Training loss: 0.0307489
DEBUG:root:[ Iteration 6 ] Training loss: 0.0265961
DEBUG:root:[ Iteration 9 ] Training loss: 0.0307595
DEBUG:root:[ Iteration 12 ] Training loss: 0.0254608
DEBUG:root:[ Iteration 15 ] Training loss: 0.0181106
DEBUG:root:[ Iteration 18 ] Training loss: 0.019846
DEBUG:root:[ Iteration 20 ] Test loss: 0.0207643
DEBUG:root:[ Iteration 21 ] Training loss: 0.0187318
DEBUG:root:[ Iteration 24 ] Training loss: 0.0171669
DEBUG:root:[ Iteration 27 ] Training loss: 0.014991
DEBUG:root:[ Iteration 30 ] Training loss: 0.0143328
DEBUG:root:[ Iteration 33 ] Training loss: 0.0132428
DEBUG:root:[ Iteration 36 ] Training loss: 0.0144373
DEBUG:root:[ Iteration 39 ] Training loss: 0.0113375
DEBUG:root:[ Iteration 40 ] Test loss: 0.0159055
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-12-2016_13h56m18s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0464962
DEBUG:root:[ Iteration 0 ] Test loss: 0.0476985
DEBUG:root:[ Iteration 3 ] Training loss: 0.0334413
DEBUG:root:[ Iteration 6 ] Training loss: 0.0279562
DEBUG:root:[ Iteration 9 ] Training loss: 0.0223257
DEBUG:root:[ Iteration 12 ] Training loss: 0.0217473
DEBUG:root:[ Iteration 15 ] Training loss: 0.0212181
DEBUG:root:[ Iteration 18 ] Training loss: 0.0166906
DEBUG:root:[ Iteration 20 ] Test loss: 0.0226578
DEBUG:root:[ Iteration 21 ] Training loss: 0.0173707
DEBUG:root:[ Iteration 24 ] Training loss: 0.0141891
DEBUG:root:[ Iteration 27 ] Training loss: 0.0153709
DEBUG:root:[ Iteration 30 ] Training loss: 0.0144305
DEBUG:root:[ Iteration 33 ] Training loss: 0.0137356
DEBUG:root:[ Iteration 36 ] Training loss: 0.014987
DEBUG:root:[ Iteration 39 ] Training loss: 0.0118384
DEBUG:root:[ Iteration 40 ] Test loss: 0.0168897
DEBUG:root:[ Iteration 42 ] Training loss: 0.0130796
DEBUG:root:[ Iteration 45 ] Training loss: 0.012732
DEBUG:root:[ Iteration 48 ] Training loss: 0.0112877
DEBUG:root:[ Iteration 51 ] Training loss: 0.00949238
DEBUG:root:[ Iteration 54 ] Training loss: 0.0117199
DEBUG:root:[ Iteration 57 ] Training loss: 0.00833766
DEBUG:root:[ Iteration 60 ] Training loss: 0.0102585
DEBUG:root:[ Iteration 60 ] Test loss: 0.0123781
DEBUG:root:[ Iteration 63 ] Training loss: 0.0115456
DEBUG:root:[ Iteration 66 ] Training loss: 0.0103115
DEBUG:root:[ Iteration 69 ] Training loss: 0.0104604
DEBUG:root:[ Iteration 72 ] Training loss: 0.00883363
DEBUG:root:[ Iteration 75 ] Training loss: 0.00867791
DEBUG:root:[ Iteration 78 ] Training loss: 0.00767581
DEBUG:root:[ Iteration 80 ] Test loss: 0.0114642
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-12-2016_15h03m44s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0499053
DEBUG:root:[ Iteration 0 ] Test loss: 0.0520225
DEBUG:root:[ Iteration 3 ] Training loss: 0.0300395
DEBUG:root:[ Iteration 6 ] Training loss: 0.0273176
DEBUG:root:[ Iteration 9 ] Training loss: 0.0251567
DEBUG:root:[ Iteration 12 ] Training loss: 0.0224907
DEBUG:root:[ Iteration 15 ] Training loss: 0.0215943
DEBUG:root:[ Iteration 18 ] Training loss: 0.0315123
DEBUG:root:[ Iteration 20 ] Test loss: 0.0278335
DEBUG:root:[ Iteration 21 ] Training loss: 0.021169
DEBUG:root:[ Iteration 24 ] Training loss: 0.0187952
DEBUG:root:[ Iteration 27 ] Training loss: 0.0179575
DEBUG:root:[ Iteration 30 ] Training loss: 0.0209329
DEBUG:root:[ Iteration 33 ] Training loss: 0.0151474
DEBUG:root:[ Iteration 36 ] Training loss: 0.0138988
DEBUG:root:[ Iteration 39 ] Training loss: 0.0105634
DEBUG:root:[ Iteration 40 ] Test loss: 0.022778
DEBUG:root:[ Iteration 42 ] Training loss: 0.0135982
DEBUG:root:[ Iteration 45 ] Training loss: 0.0157494
DEBUG:root:[ Iteration 48 ] Training loss: 0.011533
DEBUG:root:[ Iteration 51 ] Training loss: 0.00981993
DEBUG:root:[ Iteration 54 ] Training loss: 0.0107718
DEBUG:root:[ Iteration 57 ] Training loss: 0.009014
DEBUG:root:[ Iteration 60 ] Training loss: 0.0104608
DEBUG:root:[ Iteration 60 ] Test loss: 0.0129651
DEBUG:root:[ Iteration 63 ] Training loss: 0.010623
DEBUG:root:[ Iteration 66 ] Training loss: 0.009147
DEBUG:root:[ Iteration 69 ] Training loss: 0.0109067
DEBUG:root:[ Iteration 72 ] Training loss: 0.00888459
DEBUG:root:[ Iteration 75 ] Training loss: 0.00699421
DEBUG:root:[ Iteration 78 ] Training loss: 0.0101754
DEBUG:root:[ Iteration 80 ] Test loss: 0.0110473
DEBUG:root:[ Iteration 81 ] Training loss: 0.00752875
DEBUG:root:[ Iteration 84 ] Training loss: 0.00707655
DEBUG:root:[ Iteration 87 ] Training loss: 0.0112198
DEBUG:root:[ Iteration 90 ] Training loss: 0.0101601
DEBUG:root:[ Iteration 93 ] Training loss: 0.00894746
DEBUG:root:[ Iteration 96 ] Training loss: 0.00842403
DEBUG:root:[ Iteration 99 ] Training loss: 0.00747913
DEBUG:root:[ Iteration 100 ] Test loss: 0.0115997
DEBUG:root:[ Iteration 102 ] Training loss: 0.00706997
DEBUG:root:[ Iteration 105 ] Training loss: 0.00699832
DEBUG:root:[ Iteration 108 ] Training loss: 0.0071512
DEBUG:root:[ Iteration 111 ] Training loss: 0.00766426
DEBUG:root:[ Iteration 114 ] Training loss: 0.00728959
DEBUG:root:[ Iteration 117 ] Training loss: 0.00793664
DEBUG:root:[ Iteration 120 ] Training loss: 0.00813767
DEBUG:root:[ Iteration 120 ] Test loss: 0.011016
DEBUG:root:[ Iteration 123 ] Training loss: 0.00852592
DEBUG:root:[ Iteration 126 ] Training loss: 0.00614546
DEBUG:root:[ Iteration 129 ] Training loss: 0.00594397
DEBUG:root:[ Iteration 132 ] Training loss: 0.00600162
DEBUG:root:[ Iteration 135 ] Training loss: 0.00719837
DEBUG:root:[ Iteration 138 ] Training loss: 0.00452625
DEBUG:root:[ Iteration 140 ] Test loss: 0.0106816
DEBUG:root:[ Iteration 141 ] Training loss: 0.00835261
DEBUG:root:[ Iteration 144 ] Training loss: 0.00579233
DEBUG:root:[ Iteration 147 ] Training loss: 0.00636544
DEBUG:root:[ Iteration 150 ] Training loss: 0.00701526
DEBUG:root:[ Iteration 153 ] Training loss: 0.00571194
DEBUG:root:[ Iteration 156 ] Training loss: 0.00812878
DEBUG:root:[ Iteration 159 ] Training loss: 0.00631033
DEBUG:root:[ Iteration 160 ] Test loss: 0.00933822
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-12-2016_15h17m36s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0227134
DEBUG:root:[ Iteration 0 ] Test loss: 0.0232335
DEBUG:root:[ Iteration 3 ] Training loss: 0.017406
DEBUG:root:[ Iteration 6 ] Training loss: 0.021901
DEBUG:root:[ Iteration 9 ] Training loss: 0.0189529
DEBUG:root:[ Iteration 12 ] Training loss: 0.0188822
DEBUG:root:[ Iteration 15 ] Training loss: 0.0102057
DEBUG:root:[ Iteration 18 ] Training loss: 0.0211648
DEBUG:root:[ Iteration 20 ] Test loss: 0.0180786
DEBUG:root:[ Iteration 21 ] Training loss: 0.0215134
DEBUG:root:[ Iteration 24 ] Training loss: 0.0162701
DEBUG:root:[ Iteration 27 ] Training loss: 0.0139986
DEBUG:root:[ Iteration 30 ] Training loss: 0.012259
DEBUG:root:[ Iteration 33 ] Training loss: 0.0161065
DEBUG:root:[ Iteration 36 ] Training loss: 0.0128861
DEBUG:root:[ Iteration 39 ] Training loss: 0.0119881
DEBUG:root:[ Iteration 40 ] Test loss: 0.01095
DEBUG:root:[ Iteration 42 ] Training loss: 0.0151799
DEBUG:root:[ Iteration 45 ] Training loss: 0.0129425
DEBUG:root:[ Iteration 48 ] Training loss: 0.0104874
DEBUG:root:[ Iteration 51 ] Training loss: 0.0106857
DEBUG:root:[ Iteration 54 ] Training loss: 0.0130986
DEBUG:root:[ Iteration 57 ] Training loss: 0.0113166
DEBUG:root:[ Iteration 60 ] Training loss: 0.0109204
DEBUG:root:[ Iteration 60 ] Test loss: 0.0145784
DEBUG:root:[ Iteration 63 ] Training loss: 0.00864681
DEBUG:root:[ Iteration 66 ] Training loss: 0.0131139
DEBUG:root:[ Iteration 69 ] Training loss: 0.00850003
DEBUG:root:[ Iteration 72 ] Training loss: 0.00794129
DEBUG:root:[ Iteration 75 ] Training loss: 0.0113199
DEBUG:root:[ Iteration 78 ] Training loss: 0.00842699
DEBUG:root:[ Iteration 80 ] Test loss: 0.0126054
DEBUG:root:[ Iteration 81 ] Training loss: 0.00885161
DEBUG:root:[ Iteration 84 ] Training loss: 0.00865264
DEBUG:root:[ Iteration 87 ] Training loss: 0.00872369
DEBUG:root:[ Iteration 90 ] Training loss: 0.00728754
DEBUG:root:[ Iteration 93 ] Training loss: 0.00969176
DEBUG:root:[ Iteration 96 ] Training loss: 0.0102865
DEBUG:root:[ Iteration 99 ] Training loss: 0.00751418
DEBUG:root:[ Iteration 100 ] Test loss: 0.00921538
DEBUG:root:[ Iteration 102 ] Training loss: 0.00979226
DEBUG:root:[ Iteration 105 ] Training loss: 0.0100116
DEBUG:root:[ Iteration 108 ] Training loss: 0.00928691
DEBUG:root:[ Iteration 111 ] Training loss: 0.0104189
DEBUG:root:[ Iteration 114 ] Training loss: 0.0101961
DEBUG:root:[ Iteration 117 ] Training loss: 0.00720087
DEBUG:root:[ Iteration 120 ] Training loss: 0.00851925
DEBUG:root:[ Iteration 120 ] Test loss: 0.0135084
DEBUG:root:[ Iteration 123 ] Training loss: 0.00924033
DEBUG:root:[ Iteration 126 ] Training loss: 0.00598621
DEBUG:root:[ Iteration 129 ] Training loss: 0.00793064
DEBUG:root:[ Iteration 132 ] Training loss: 0.0105396
DEBUG:root:[ Iteration 135 ] Training loss: 0.00680961
DEBUG:root:[ Iteration 138 ] Training loss: 0.00779009
DEBUG:root:[ Iteration 140 ] Test loss: 0.00895572
DEBUG:root:[ Iteration 141 ] Training loss: 0.00685909
DEBUG:root:[ Iteration 144 ] Training loss: 0.00881944
DEBUG:root:[ Iteration 147 ] Training loss: 0.00702621
DEBUG:root:[ Iteration 150 ] Training loss: 0.00704391
DEBUG:root:[ Iteration 153 ] Training loss: 0.00814955
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-12-2016_16h01m29s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0152416
DEBUG:root:[ Iteration 0 ] Test loss: 0.0165612
DEBUG:root:[ Iteration 3 ] Training loss: 0.0152434
DEBUG:root:[ Iteration 6 ] Training loss: 0.0143527
DEBUG:root:[ Iteration 9 ] Training loss: 0.0181009
DEBUG:root:[ Iteration 12 ] Training loss: 0.0254562
DEBUG:root:[ Iteration 15 ] Training loss: 0.0189424
DEBUG:root:[ Iteration 18 ] Training loss: 0.0168701
DEBUG:root:[ Iteration 20 ] Test loss: 0.017181
DEBUG:root:[ Iteration 21 ] Training loss: 0.015257
DEBUG:root:[ Iteration 24 ] Training loss: 0.0155738
DEBUG:root:[ Iteration 27 ] Training loss: 0.015249
DEBUG:root:[ Iteration 30 ] Training loss: 0.0130903
DEBUG:root:[ Iteration 33 ] Training loss: 0.014586
DEBUG:root:[ Iteration 36 ] Training loss: 0.0163462
DEBUG:root:[ Iteration 39 ] Training loss: 0.016769
DEBUG:root:[ Iteration 40 ] Test loss: 0.0106894
DEBUG:root:[ Iteration 42 ] Training loss: 0.0146305
DEBUG:root:[ Iteration 45 ] Training loss: 0.0100674
DEBUG:root:[ Iteration 48 ] Training loss: 0.0129794
DEBUG:root:[ Iteration 51 ] Training loss: 0.00930147
DEBUG:root:[ Iteration 54 ] Training loss: 0.0104642
DEBUG:root:[ Iteration 57 ] Training loss: 0.010858
DEBUG:root:[ Iteration 60 ] Training loss: 0.0114145
DEBUG:root:[ Iteration 60 ] Test loss: 0.0132728
DEBUG:root:[ Iteration 63 ] Training loss: 0.00950586
DEBUG:root:[ Iteration 66 ] Training loss: 0.00814098
DEBUG:root:[ Iteration 69 ] Training loss: 0.0152657
DEBUG:root:[ Iteration 72 ] Training loss: 0.0104324
DEBUG:root:[ Iteration 75 ] Training loss: 0.00798943
DEBUG:root:[ Iteration 78 ] Training loss: 0.0106954
DEBUG:root:[ Iteration 80 ] Test loss: 0.010085
DEBUG:root:[ Iteration 81 ] Training loss: 0.00844333
DEBUG:root:[ Iteration 84 ] Training loss: 0.008377
DEBUG:root:[ Iteration 87 ] Training loss: 0.0107576
DEBUG:root:[ Iteration 90 ] Training loss: 0.00930547
DEBUG:root:[ Iteration 93 ] Training loss: 0.00978702
DEBUG:root:[ Iteration 96 ] Training loss: 0.0105258
DEBUG:root:[ Iteration 99 ] Training loss: 0.00810594
DEBUG:root:[ Iteration 100 ] Test loss: 0.00948372
DEBUG:root:[ Iteration 102 ] Training loss: 0.0103344
DEBUG:root:[ Iteration 105 ] Training loss: 0.00734859
DEBUG:root:[ Iteration 108 ] Training loss: 0.00911192
DEBUG:root:[ Iteration 111 ] Training loss: 0.00964562
DEBUG:root:[ Iteration 114 ] Training loss: 0.010586
DEBUG:root:[ Iteration 117 ] Training loss: 0.00935876
DEBUG:root:[ Iteration 120 ] Training loss: 0.00913712
DEBUG:root:[ Iteration 120 ] Test loss: 0.00976804
DEBUG:root:[ Iteration 123 ] Training loss: 0.00889832
DEBUG:root:[ Iteration 126 ] Training loss: 0.00838758
DEBUG:root:[ Iteration 129 ] Training loss: 0.00813132
DEBUG:root:[ Iteration 132 ] Training loss: 0.00740165
DEBUG:root:[ Iteration 135 ] Training loss: 0.00798253
DEBUG:root:[ Iteration 138 ] Training loss: 0.00778839
DEBUG:root:[ Iteration 140 ] Test loss: 0.00833381
DEBUG:root:[ Iteration 141 ] Training loss: 0.00816411
DEBUG:root:[ Iteration 144 ] Training loss: 0.00845565
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-12-2016_17h04m53s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0189608
DEBUG:root:[ Iteration 0 ] Test loss: 0.0154502
DEBUG:root:[ Iteration 3 ] Training loss: 0.0157609
DEBUG:root:[ Iteration 6 ] Training loss: 0.0186661
DEBUG:root:[ Iteration 9 ] Training loss: 0.0206969
DEBUG:root:[ Iteration 12 ] Training loss: 0.0132277
DEBUG:root:[ Iteration 15 ] Training loss: 0.0117837
DEBUG:root:[ Iteration 18 ] Training loss: 0.0150554
DEBUG:root:[ Iteration 20 ] Test loss: 0.0124788
DEBUG:root:[ Iteration 21 ] Training loss: 0.0136378
DEBUG:root:[ Iteration 24 ] Training loss: 0.0131961
DEBUG:root:[ Iteration 27 ] Training loss: 0.012097
DEBUG:root:[ Iteration 30 ] Training loss: 0.0112648
DEBUG:root:[ Iteration 33 ] Training loss: 0.0113733
DEBUG:root:[ Iteration 36 ] Training loss: 0.0150994
DEBUG:root:[ Iteration 39 ] Training loss: 0.0129924
DEBUG:root:[ Iteration 40 ] Test loss: 0.0144423
DEBUG:root:[ Iteration 42 ] Training loss: 0.0124084
DEBUG:root:[ Iteration 45 ] Training loss: 0.0124742
DEBUG:root:[ Iteration 48 ] Training loss: 0.0129207
DEBUG:root:[ Iteration 51 ] Training loss: 0.00882743
DEBUG:root:[ Iteration 54 ] Training loss: 0.0137993
DEBUG:root:[ Iteration 57 ] Training loss: 0.01165
DEBUG:root:[ Iteration 60 ] Training loss: 0.0103291
DEBUG:root:[ Iteration 60 ] Test loss: 0.0121294
DEBUG:root:[ Iteration 63 ] Training loss: 0.0114848
DEBUG:root:[ Iteration 66 ] Training loss: 0.0136815
DEBUG:root:[ Iteration 69 ] Training loss: 0.00772464
DEBUG:root:[ Iteration 72 ] Training loss: 0.0111453
DEBUG:root:[ Iteration 75 ] Training loss: 0.0181304
DEBUG:root:[ Iteration 78 ] Training loss: 0.00905398
DEBUG:root:[ Iteration 80 ] Test loss: 0.0115648
DEBUG:root:[ Iteration 81 ] Training loss: 0.0133511
DEBUG:root:[ Iteration 84 ] Training loss: 0.00953185
DEBUG:root:[ Iteration 87 ] Training loss: 0.0103619
DEBUG:root:[ Iteration 90 ] Training loss: 0.0107004
DEBUG:root:[ Iteration 93 ] Training loss: 0.00885049
DEBUG:root:[ Iteration 96 ] Training loss: 0.0125519
DEBUG:root:[ Iteration 99 ] Training loss: 0.00908411
DEBUG:root:[ Iteration 100 ] Test loss: 0.0146643
DEBUG:root:[ Iteration 102 ] Training loss: 0.0125173
DEBUG:root:[ Iteration 105 ] Training loss: 0.0122701
DEBUG:root:[ Iteration 108 ] Training loss: 0.0106757
DEBUG:root:[ Iteration 111 ] Training loss: 0.0114052
DEBUG:root:[ Iteration 114 ] Training loss: 0.00840727
DEBUG:root:[ Iteration 117 ] Training loss: 0.00807818
DEBUG:root:[ Iteration 120 ] Training loss: 0.0115149
DEBUG:root:[ Iteration 120 ] Test loss: 0.0111641
DEBUG:root:[ Iteration 123 ] Training loss: 0.00552312
DEBUG:root:[ Iteration 126 ] Training loss: 0.0107357
DEBUG:root:[ Iteration 129 ] Training loss: 0.0079199
DEBUG:root:[ Iteration 132 ] Training loss: 0.00909639
DEBUG:root:[ Iteration 135 ] Training loss: 0.0119336
DEBUG:root:[ Iteration 138 ] Training loss: 0.00952352
DEBUG:root:[ Iteration 140 ] Test loss: 0.0100626
DEBUG:root:[ Iteration 141 ] Training loss: 0.00961305
DEBUG:root:[ Iteration 144 ] Training loss: 0.00806195
DEBUG:root:[ Iteration 147 ] Training loss: 0.00907174
DEBUG:root:[ Iteration 150 ] Training loss: 0.00846927
DEBUG:root:[ Iteration 153 ] Training loss: 0.00806676
DEBUG:root:[ Iteration 156 ] Training loss: 0.00949273
DEBUG:root:[ Iteration 159 ] Training loss: 0.00774267
DEBUG:root:[ Iteration 160 ] Test loss: 0.00779661
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-12-2016_17h59m34s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0154073
DEBUG:root:[ Iteration 0 ] Test loss: 0.0180276
DEBUG:root:[ Iteration 3 ] Training loss: 0.0155712
DEBUG:root:[ Iteration 6 ] Training loss: 0.0137838
DEBUG:root:[ Iteration 9 ] Training loss: 0.0211208
DEBUG:root:[ Iteration 12 ] Training loss: 0.0157627
DEBUG:root:[ Iteration 15 ] Training loss: 0.0165722
DEBUG:root:[ Iteration 18 ] Training loss: 0.016951
DEBUG:root:[ Iteration 20 ] Test loss: 0.0111381
DEBUG:root:[ Iteration 21 ] Training loss: 0.0107936
DEBUG:root:[ Iteration 24 ] Training loss: 0.0151416
DEBUG:root:[ Iteration 27 ] Training loss: 0.0178239
DEBUG:root:[ Iteration 30 ] Training loss: 0.0124027
DEBUG:root:[ Iteration 33 ] Training loss: 0.0147312
DEBUG:root:[ Iteration 36 ] Training loss: 0.0131883
DEBUG:root:[ Iteration 39 ] Training loss: 0.0110886
DEBUG:root:[ Iteration 40 ] Test loss: 0.015337
DEBUG:root:[ Iteration 42 ] Training loss: 0.010252
DEBUG:root:[ Iteration 45 ] Training loss: 0.0138122
DEBUG:root:[ Iteration 48 ] Training loss: 0.0122398
DEBUG:root:[ Iteration 51 ] Training loss: 0.0145842
DEBUG:root:[ Iteration 54 ] Training loss: 0.0121987
DEBUG:root:[ Iteration 57 ] Training loss: 0.0141078
DEBUG:root:[ Iteration 60 ] Training loss: 0.0102483
DEBUG:root:[ Iteration 60 ] Test loss: 0.0125044
DEBUG:root:[ Iteration 63 ] Training loss: 0.0138665
DEBUG:root:[ Iteration 66 ] Training loss: 0.00856636
DEBUG:root:[ Iteration 69 ] Training loss: 0.00917365
DEBUG:root:[ Iteration 72 ] Training loss: 0.0146556
DEBUG:root:[ Iteration 75 ] Training loss: 0.010466
DEBUG:root:[ Iteration 78 ] Training loss: 0.0106486
DEBUG:root:[ Iteration 80 ] Test loss: 0.00963187
DEBUG:root:[ Iteration 81 ] Training loss: 0.0108929
DEBUG:root:[ Iteration 84 ] Training loss: 0.0113219
DEBUG:root:[ Iteration 87 ] Training loss: 0.0115848
DEBUG:root:[ Iteration 90 ] Training loss: 0.00763331
DEBUG:root:[ Iteration 93 ] Training loss: 0.0114105
DEBUG:root:[ Iteration 96 ] Training loss: 0.00978859
DEBUG:root:[ Iteration 99 ] Training loss: 0.0117981
DEBUG:root:[ Iteration 100 ] Test loss: 0.00904082
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-12-2016_18h06m44s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0223515
DEBUG:root:[ Iteration 0 ] Test loss: 0.0238834
DEBUG:root:[ Iteration 3 ] Training loss: 0.021095
DEBUG:root:[ Iteration 6 ] Training loss: 0.0148637
DEBUG:root:[ Iteration 9 ] Training loss: 0.0231538
DEBUG:root:[ Iteration 12 ] Training loss: 0.0217923
DEBUG:root:[ Iteration 15 ] Training loss: 0.0162782
DEBUG:root:[ Iteration 18 ] Training loss: 0.0179268
DEBUG:root:[ Iteration 20 ] Test loss: 0.0213468
DEBUG:root:[ Iteration 21 ] Training loss: 0.0190642
DEBUG:root:[ Iteration 24 ] Training loss: 0.0167659
DEBUG:root:[ Iteration 27 ] Training loss: 0.0149606
DEBUG:root:[ Iteration 30 ] Training loss: 0.0135962
DEBUG:root:[ Iteration 33 ] Training loss: 0.0144958
DEBUG:root:[ Iteration 36 ] Training loss: 0.0144412
DEBUG:root:[ Iteration 39 ] Training loss: 0.0143365
DEBUG:root:[ Iteration 40 ] Test loss: 0.0135925
DEBUG:root:[ Iteration 42 ] Training loss: 0.0150819
DEBUG:root:[ Iteration 45 ] Training loss: 0.0112073
DEBUG:root:[ Iteration 48 ] Training loss: 0.00988154
DEBUG:root:[ Iteration 51 ] Training loss: 0.013495
DEBUG:root:[ Iteration 54 ] Training loss: 0.0108695
DEBUG:root:[ Iteration 57 ] Training loss: 0.0146143
DEBUG:root:[ Iteration 60 ] Training loss: 0.0139482
DEBUG:root:[ Iteration 60 ] Test loss: 0.0125195
DEBUG:root:[ Iteration 63 ] Training loss: 0.0100722
DEBUG:root:[ Iteration 66 ] Training loss: 0.0110001
DEBUG:root:[ Iteration 69 ] Training loss: 0.0105294
DEBUG:root:[ Iteration 72 ] Training loss: 0.0109369
DEBUG:root:[ Iteration 75 ] Training loss: 0.0112666
DEBUG:root:[ Iteration 78 ] Training loss: 0.0142958
DEBUG:root:[ Iteration 80 ] Test loss: 0.0111038
DEBUG:root:[ Iteration 81 ] Training loss: 0.00994834
DEBUG:root:[ Iteration 84 ] Training loss: 0.0121538
DEBUG:root:[ Iteration 87 ] Training loss: 0.0119219
DEBUG:root:[ Iteration 90 ] Training loss: 0.0106255
DEBUG:root:[ Iteration 93 ] Training loss: 0.00869699
DEBUG:root:[ Iteration 96 ] Training loss: 0.00944994
DEBUG:root:[ Iteration 99 ] Training loss: 0.00988636
DEBUG:root:[ Iteration 100 ] Test loss: 0.0100894
DEBUG:root:[ Iteration 102 ] Training loss: 0.00907269
DEBUG:root:[ Iteration 105 ] Training loss: 0.0104395
DEBUG:root:[ Iteration 108 ] Training loss: 0.0124278
DEBUG:root:[ Iteration 111 ] Training loss: 0.0099045
DEBUG:root:[ Iteration 114 ] Training loss: 0.0121173
DEBUG:root:[ Iteration 117 ] Training loss: 0.0110263
DEBUG:root:[ Iteration 120 ] Training loss: 0.00857396
DEBUG:root:[ Iteration 120 ] Test loss: 0.0104108
DEBUG:root:[ Iteration 123 ] Training loss: 0.0121282
DEBUG:root:[ Iteration 126 ] Training loss: 0.0102482
DEBUG:root:[ Iteration 129 ] Training loss: 0.0105653
DEBUG:root:[ Iteration 132 ] Training loss: 0.00918137
DEBUG:root:[ Iteration 135 ] Training loss: 0.0074609
DEBUG:root:[ Iteration 138 ] Training loss: 0.00867547
DEBUG:root:[ Iteration 140 ] Test loss: 0.00969603
DEBUG:root:[ Iteration 141 ] Training loss: 0.0111806
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-12-2016_18h10m23s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.191665
DEBUG:root:[ Iteration 0 ] Test loss: 0.194748
DEBUG:root:[ Iteration 3 ] Training loss: 0.185685
DEBUG:root:[ Iteration 6 ] Training loss: 0.184241
DEBUG:root:[ Iteration 9 ] Training loss: 0.193052
DEBUG:root:[ Iteration 12 ] Training loss: 0.182485
DEBUG:root:[ Iteration 15 ] Training loss: 0.182329
DEBUG:root:[ Iteration 18 ] Training loss: 0.170312
DEBUG:root:[ Iteration 20 ] Test loss: 0.163323
DEBUG:root:[ Iteration 21 ] Training loss: 0.161775
DEBUG:root:[ Iteration 24 ] Training loss: 0.119851
DEBUG:root:[ Iteration 27 ] Training loss: 0.133804
DEBUG:root:[ Iteration 30 ] Training loss: 0.15095
DEBUG:root:[ Iteration 33 ] Training loss: 0.148306
DEBUG:root:[ Iteration 36 ] Training loss: 0.1285
DEBUG:root:[ Iteration 39 ] Training loss: 0.138516
DEBUG:root:[ Iteration 40 ] Test loss: 0.134809
DEBUG:root:[ Iteration 42 ] Training loss: 0.129961
DEBUG:root:[ Iteration 45 ] Training loss: 0.126218
DEBUG:root:[ Iteration 48 ] Training loss: 0.12356
DEBUG:root:[ Iteration 51 ] Training loss: 0.108565
DEBUG:root:[ Iteration 54 ] Training loss: 0.112139
DEBUG:root:[ Iteration 57 ] Training loss: 0.115594
DEBUG:root:[ Iteration 60 ] Training loss: 0.0988026
DEBUG:root:[ Iteration 60 ] Test loss: 0.106713
DEBUG:root:[ Iteration 63 ] Training loss: 0.123403
DEBUG:root:[ Iteration 66 ] Training loss: 0.113792
DEBUG:root:[ Iteration 69 ] Training loss: 0.119831
DEBUG:root:[ Iteration 72 ] Training loss: 0.0883919
DEBUG:root:[ Iteration 75 ] Training loss: 0.0923313
DEBUG:root:[ Iteration 78 ] Training loss: 0.0955236
DEBUG:root:[ Iteration 80 ] Test loss: 0.0943309
DEBUG:root:[ Iteration 81 ] Training loss: 0.100514
DEBUG:root:[ Iteration 84 ] Training loss: 0.104907
DEBUG:root:[ Iteration 87 ] Training loss: 0.0846461
DEBUG:root:[ Iteration 90 ] Training loss: 0.0969818
DEBUG:root:[ Iteration 93 ] Training loss: 0.0788902
DEBUG:root:[ Iteration 96 ] Training loss: 0.0896586
DEBUG:root:[ Iteration 99 ] Training loss: 0.0894003
DEBUG:root:[ Iteration 100 ] Test loss: 0.0808272
DEBUG:root:[ Iteration 102 ] Training loss: 0.0861495
DEBUG:root:[ Iteration 105 ] Training loss: 0.076381
DEBUG:root:[ Iteration 108 ] Training loss: 0.0945645
DEBUG:root:[ Iteration 111 ] Training loss: 0.0822085
DEBUG:root:[ Iteration 114 ] Training loss: 0.0783051
DEBUG:root:[ Iteration 117 ] Training loss: 0.0686842
DEBUG:root:[ Iteration 120 ] Training loss: 0.067608
DEBUG:root:[ Iteration 120 ] Test loss: 0.0600135
DEBUG:root:[ Iteration 123 ] Training loss: 0.0727784
DEBUG:root:[ Iteration 126 ] Training loss: 0.0636578
DEBUG:root:[ Iteration 129 ] Training loss: 0.0650931
DEBUG:root:[ Iteration 132 ] Training loss: 0.0812031
DEBUG:root:[ Iteration 135 ] Training loss: 0.0654319
DEBUG:root:[ Iteration 138 ] Training loss: 0.0587129
DEBUG:root:[ Iteration 140 ] Test loss: 0.0596563
DEBUG:root:[ Iteration 141 ] Training loss: 0.0732794
DEBUG:root:[ Iteration 144 ] Training loss: 0.0753385
DEBUG:root:[ Iteration 147 ] Training loss: 0.0638739
DEBUG:root:[ Iteration 150 ] Training loss: 0.0595511
DEBUG:root:[ Iteration 153 ] Training loss: 0.0743729
DEBUG:root:[ Iteration 156 ] Training loss: 0.0663488
DEBUG:root:[ Iteration 159 ] Training loss: 0.0617467
DEBUG:root:[ Iteration 160 ] Test loss: 0.0638649
DEBUG:root:[ Iteration 162 ] Training loss: 0.0564042
DEBUG:root:[ Iteration 165 ] Training loss: 0.0578164
DEBUG:root:[ Iteration 168 ] Training loss: 0.0529946
DEBUG:root:[ Iteration 171 ] Training loss: 0.0604399
DEBUG:root:[ Iteration 174 ] Training loss: 0.0481289
DEBUG:root:[ Iteration 177 ] Training loss: 0.0686827
DEBUG:root:[ Iteration 180 ] Training loss: 0.0455274
DEBUG:root:[ Iteration 180 ] Test loss: 0.0563066
DEBUG:root:[ Iteration 183 ] Training loss: 0.0482108
DEBUG:root:[ Iteration 186 ] Training loss: 0.0542265
DEBUG:root:[ Iteration 189 ] Training loss: 0.0415559
DEBUG:root:[ Iteration 192 ] Training loss: 0.0615298
DEBUG:root:[ Iteration 195 ] Training loss: 0.0699557
DEBUG:root:[ Iteration 198 ] Training loss: 0.0499361
DEBUG:root:[ Iteration 200 ] Test loss: 0.0634734
DEBUG:root:[ Iteration 201 ] Training loss: 0.0486873
DEBUG:root:[ Iteration 204 ] Training loss: 0.0644523
DEBUG:root:[ Iteration 207 ] Training loss: 0.0561564
DEBUG:root:[ Iteration 210 ] Training loss: 0.0528815
DEBUG:root:[ Iteration 213 ] Training loss: 0.0548962
DEBUG:root:[ Iteration 216 ] Training loss: 0.0446637
DEBUG:root:[ Iteration 219 ] Training loss: 0.04695
DEBUG:root:[ Iteration 220 ] Test loss: 0.0433071
DEBUG:root:[ Iteration 222 ] Training loss: 0.0549969
DEBUG:root:[ Iteration 225 ] Training loss: 0.0522494
DEBUG:root:[ Iteration 228 ] Training loss: 0.0445041
DEBUG:root:[ Iteration 231 ] Training loss: 0.0369024
DEBUG:root:[ Iteration 234 ] Training loss: 0.0360953
DEBUG:root:[ Iteration 237 ] Training loss: 0.0372006
DEBUG:root:[ Iteration 240 ] Training loss: 0.0394076
DEBUG:root:[ Iteration 240 ] Test loss: 0.0310979
DEBUG:root:[ Iteration 243 ] Training loss: 0.03755
DEBUG:root:[ Iteration 246 ] Training loss: 0.0324778
DEBUG:root:[ Iteration 249 ] Training loss: 0.0381439
DEBUG:root:[ Iteration 252 ] Training loss: 0.043102
DEBUG:root:[ Iteration 255 ] Training loss: 0.0347197
DEBUG:root:[ Iteration 258 ] Training loss: 0.034471
DEBUG:root:[ Iteration 260 ] Test loss: 0.0499634
DEBUG:root:[ Iteration 261 ] Training loss: 0.0408204
DEBUG:root:[ Iteration 264 ] Training loss: 0.0394189
DEBUG:root:[ Iteration 267 ] Training loss: 0.0376605
DEBUG:root:[ Iteration 270 ] Training loss: 0.0421564
DEBUG:root:[ Iteration 273 ] Training loss: 0.0392736
DEBUG:root:[ Iteration 276 ] Training loss: 0.037498
DEBUG:root:[ Iteration 279 ] Training loss: 0.0407148
DEBUG:root:[ Iteration 280 ] Test loss: 0.0410874
DEBUG:root:[ Iteration 282 ] Training loss: 0.0517947
DEBUG:root:[ Iteration 285 ] Training loss: 0.0452686
DEBUG:root:[ Iteration 288 ] Training loss: 0.0266055
DEBUG:root:[ Iteration 291 ] Training loss: 0.0459681
DEBUG:root:[ Iteration 294 ] Training loss: 0.0469948
DEBUG:root:[ Iteration 297 ] Training loss: 0.0410675
DEBUG:root:[ Iteration 300 ] Training loss: 0.0394748
DEBUG:root:[ Iteration 300 ] Test loss: 0.0421145
DEBUG:root:[ Iteration 303 ] Training loss: 0.0378996
DEBUG:root:[ Iteration 306 ] Training loss: 0.0376491
DEBUG:root:[ Iteration 309 ] Training loss: 0.0387329
DEBUG:root:[ Iteration 312 ] Training loss: 0.0451159
DEBUG:root:[ Iteration 315 ] Training loss: 0.038899
DEBUG:root:[ Iteration 318 ] Training loss: 0.0432149
DEBUG:root:[ Iteration 320 ] Test loss: 0.0478746
DEBUG:root:[ Iteration 321 ] Training loss: 0.0478231
DEBUG:root:[ Iteration 324 ] Training loss: 0.0381684
DEBUG:root:[ Iteration 327 ] Training loss: 0.0417466
DEBUG:root:[ Iteration 330 ] Training loss: 0.0366014
DEBUG:root:[ Iteration 333 ] Training loss: 0.0389453
DEBUG:root:[ Iteration 336 ] Training loss: 0.0312822
DEBUG:root:[ Iteration 339 ] Training loss: 0.0356206
DEBUG:root:[ Iteration 340 ] Test loss: 0.0461407
DEBUG:root:[ Iteration 342 ] Training loss: 0.0379684
DEBUG:root:[ Iteration 345 ] Training loss: 0.0290399
DEBUG:root:[ Iteration 348 ] Training loss: 0.0344956
DEBUG:root:[ Iteration 351 ] Training loss: 0.0267876
DEBUG:root:[ Iteration 354 ] Training loss: 0.0351812
DEBUG:root:[ Iteration 357 ] Training loss: 0.0318142
DEBUG:root:[ Iteration 360 ] Training loss: 0.0324368
DEBUG:root:[ Iteration 360 ] Test loss: 0.0321882
DEBUG:root:[ Iteration 363 ] Training loss: 0.0367325
DEBUG:root:[ Iteration 366 ] Training loss: 0.0342973
DEBUG:root:[ Iteration 369 ] Training loss: 0.0266212
DEBUG:root:[ Iteration 372 ] Training loss: 0.0389804
DEBUG:root:[ Iteration 375 ] Training loss: 0.0259489
DEBUG:root:[ Iteration 378 ] Training loss: 0.0303743
DEBUG:root:[ Iteration 380 ] Test loss: 0.0346551
DEBUG:root:[ Iteration 381 ] Training loss: 0.0324018
DEBUG:root:[ Iteration 384 ] Training loss: 0.0336238
DEBUG:root:[ Iteration 387 ] Training loss: 0.0299567
DEBUG:root:[ Iteration 390 ] Training loss: 0.037632
DEBUG:root:[ Iteration 393 ] Training loss: 0.0354408
DEBUG:root:[ Iteration 396 ] Training loss: 0.0260327
DEBUG:root:[ Iteration 399 ] Training loss: 0.0343787
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-14-2016_21h31m24s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.134305
DEBUG:root:[ Iteration 0 ] Test loss: 0.146633
DEBUG:root:[ Iteration 3 ] Training loss: 0.141481
DEBUG:root:[ Iteration 6 ] Training loss: 0.144337
DEBUG:root:[ Iteration 9 ] Training loss: 0.128471
DEBUG:root:[ Iteration 12 ] Training loss: 0.113761
DEBUG:root:[ Iteration 15 ] Training loss: 0.0799341
DEBUG:root:[ Iteration 18 ] Training loss: 0.0665573
DEBUG:root:[ Iteration 20 ] Test loss: 0.0809012
DEBUG:root:[ Iteration 21 ] Training loss: 0.0802964
DEBUG:root:[ Iteration 24 ] Training loss: 0.0905347
DEBUG:root:[ Iteration 27 ] Training loss: 0.0635678
DEBUG:root:[ Iteration 30 ] Training loss: 0.0785333
DEBUG:root:[ Iteration 33 ] Training loss: 0.0765896
DEBUG:root:[ Iteration 36 ] Training loss: 0.079637
DEBUG:root:[ Iteration 39 ] Training loss: 0.084449
DEBUG:root:[ Iteration 40 ] Test loss: 0.0961881
DEBUG:root:[ Iteration 42 ] Training loss: 0.0637485
DEBUG:root:[ Iteration 45 ] Training loss: 0.084826
DEBUG:root:[ Iteration 48 ] Training loss: 0.0853415
DEBUG:root:[ Iteration 51 ] Training loss: 0.0610863
DEBUG:root:[ Iteration 54 ] Training loss: 0.077456
DEBUG:root:[ Iteration 57 ] Training loss: 0.0574832
DEBUG:root:[ Iteration 60 ] Training loss: 0.0673246
DEBUG:root:[ Iteration 60 ] Test loss: 0.0794183
DEBUG:root:[ Iteration 63 ] Training loss: 0.0699155
DEBUG:root:[ Iteration 66 ] Training loss: 0.0838574
DEBUG:root:[ Iteration 69 ] Training loss: 0.0754848
DEBUG:root:[ Iteration 72 ] Training loss: 0.072832
DEBUG:root:[ Iteration 75 ] Training loss: 0.0591145
DEBUG:root:[ Iteration 78 ] Training loss: 0.0824843
DEBUG:root:[ Iteration 80 ] Test loss: 0.0831524
DEBUG:root:[ Iteration 81 ] Training loss: 0.0673521
DEBUG:root:[ Iteration 84 ] Training loss: 0.0861908
DEBUG:root:[ Iteration 87 ] Training loss: 0.0708525
DEBUG:root:[ Iteration 90 ] Training loss: 0.0685069
DEBUG:root:[ Iteration 93 ] Training loss: 0.0737676
DEBUG:root:[ Iteration 96 ] Training loss: 0.0768508
DEBUG:root:[ Iteration 99 ] Training loss: 0.0646738
DEBUG:root:[ Iteration 100 ] Test loss: 0.0811663
DEBUG:root:[ Iteration 102 ] Training loss: 0.0595563
DEBUG:root:[ Iteration 105 ] Training loss: 0.0745211
DEBUG:root:[ Iteration 108 ] Training loss: 0.0693202
DEBUG:root:[ Iteration 111 ] Training loss: 0.0810802
DEBUG:root:[ Iteration 114 ] Training loss: 0.0723744
DEBUG:root:[ Iteration 117 ] Training loss: 0.0593446
DEBUG:root:[ Iteration 120 ] Training loss: 0.0552722
DEBUG:root:[ Iteration 120 ] Test loss: 0.0753334
DEBUG:root:[ Iteration 123 ] Training loss: 0.0738698
DEBUG:root:[ Iteration 126 ] Training loss: 0.071966
DEBUG:root:[ Iteration 129 ] Training loss: 0.0637342
DEBUG:root:[ Iteration 132 ] Training loss: 0.0729917
DEBUG:root:[ Iteration 135 ] Training loss: 0.056778
DEBUG:root:[ Iteration 138 ] Training loss: 0.0648941
DEBUG:root:[ Iteration 140 ] Test loss: 0.0813683
DEBUG:root:[ Iteration 141 ] Training loss: 0.0682291
DEBUG:root:[ Iteration 144 ] Training loss: 0.0542391
DEBUG:root:[ Iteration 147 ] Training loss: 0.0561741
DEBUG:root:[ Iteration 150 ] Training loss: 0.0579329
DEBUG:root:[ Iteration 153 ] Training loss: 0.0691311
DEBUG:root:[ Iteration 156 ] Training loss: 0.0629616
DEBUG:root:[ Iteration 159 ] Training loss: 0.0637047
DEBUG:root:[ Iteration 160 ] Test loss: 0.0715785
DEBUG:root:[ Iteration 162 ] Training loss: 0.0543044
DEBUG:root:[ Iteration 165 ] Training loss: 0.0496071
DEBUG:root:[ Iteration 168 ] Training loss: 0.0610772
DEBUG:root:[ Iteration 171 ] Training loss: 0.0611119
DEBUG:root:[ Iteration 174 ] Training loss: 0.0562524
DEBUG:root:[ Iteration 177 ] Training loss: 0.0738036
DEBUG:root:[ Iteration 180 ] Training loss: 0.0507085
DEBUG:root:[ Iteration 180 ] Test loss: 0.0649802
DEBUG:root:[ Iteration 183 ] Training loss: 0.0523768
DEBUG:root:[ Iteration 186 ] Training loss: 0.0499069
DEBUG:root:[ Iteration 189 ] Training loss: 0.0556626
DEBUG:root:[ Iteration 192 ] Training loss: 0.0582352
DEBUG:root:[ Iteration 195 ] Training loss: 0.0470049
DEBUG:root:[ Iteration 198 ] Training loss: 0.0515344
DEBUG:root:[ Iteration 200 ] Test loss: 0.0537905
DEBUG:root:[ Iteration 201 ] Training loss: 0.0498031
DEBUG:root:[ Iteration 204 ] Training loss: 0.0495187
DEBUG:root:[ Iteration 207 ] Training loss: 0.0565586
DEBUG:root:[ Iteration 210 ] Training loss: 0.048127
DEBUG:root:[ Iteration 213 ] Training loss: 0.0602647
DEBUG:root:[ Iteration 216 ] Training loss: 0.0441531
DEBUG:root:[ Iteration 219 ] Training loss: 0.053947
DEBUG:root:[ Iteration 220 ] Test loss: 0.0679853
DEBUG:root:[ Iteration 222 ] Training loss: 0.0485014
DEBUG:root:[ Iteration 225 ] Training loss: 0.0522885
DEBUG:root:[ Iteration 228 ] Training loss: 0.0509432
DEBUG:root:[ Iteration 231 ] Training loss: 0.0657049
DEBUG:root:[ Iteration 234 ] Training loss: 0.0464627
DEBUG:root:[ Iteration 237 ] Training loss: 0.0503481
DEBUG:root:[ Iteration 240 ] Training loss: 0.043346
DEBUG:root:[ Iteration 240 ] Test loss: 0.0491952
DEBUG:root:[ Iteration 243 ] Training loss: 0.0473205
DEBUG:root:[ Iteration 246 ] Training loss: 0.0463665
DEBUG:root:[ Iteration 249 ] Training loss: 0.0592429
DEBUG:root:[ Iteration 252 ] Training loss: 0.0537773
DEBUG:root:[ Iteration 255 ] Training loss: 0.0441357
DEBUG:root:[ Iteration 258 ] Training loss: 0.050172
DEBUG:root:[ Iteration 260 ] Test loss: 0.0720933
DEBUG:root:[ Iteration 261 ] Training loss: 0.0568583
DEBUG:root:[ Iteration 264 ] Training loss: 0.0476518
DEBUG:root:[ Iteration 267 ] Training loss: 0.0559443
DEBUG:root:[ Iteration 270 ] Training loss: 0.0575744
DEBUG:root:[ Iteration 273 ] Training loss: 0.0525468
DEBUG:root:[ Iteration 276 ] Training loss: 0.0455859
DEBUG:root:[ Iteration 279 ] Training loss: 0.0454095
DEBUG:root:[ Iteration 280 ] Test loss: 0.0626843
DEBUG:root:[ Iteration 282 ] Training loss: 0.0476732
DEBUG:root:[ Iteration 285 ] Training loss: 0.0428106
DEBUG:root:[ Iteration 288 ] Training loss: 0.0388169
DEBUG:root:[ Iteration 291 ] Training loss: 0.0565065
DEBUG:root:[ Iteration 294 ] Training loss: 0.0463486
DEBUG:root:[ Iteration 297 ] Training loss: 0.0481665
DEBUG:root:[ Iteration 300 ] Training loss: 0.0459365
DEBUG:root:[ Iteration 300 ] Test loss: 0.068298
DEBUG:root:[ Iteration 303 ] Training loss: 0.0429636
DEBUG:root:[ Iteration 306 ] Training loss: 0.0454212
DEBUG:root:[ Iteration 309 ] Training loss: 0.0408341
DEBUG:root:[ Iteration 312 ] Training loss: 0.0523051
DEBUG:root:[ Iteration 315 ] Training loss: 0.0380228
DEBUG:root:[ Iteration 318 ] Training loss: 0.0344439
DEBUG:root:[ Iteration 320 ] Test loss: 0.0523531
DEBUG:root:[ Iteration 321 ] Training loss: 0.0393233
DEBUG:root:[ Iteration 324 ] Training loss: 0.0421817
DEBUG:root:[ Iteration 327 ] Training loss: 0.0470815
DEBUG:root:[ Iteration 330 ] Training loss: 0.0427168
DEBUG:root:[ Iteration 333 ] Training loss: 0.0436615
DEBUG:root:[ Iteration 336 ] Training loss: 0.0399871
DEBUG:root:[ Iteration 339 ] Training loss: 0.0433837
DEBUG:root:[ Iteration 340 ] Test loss: 0.0492747
DEBUG:root:[ Iteration 342 ] Training loss: 0.0433346
DEBUG:root:[ Iteration 345 ] Training loss: 0.0314222
DEBUG:root:[ Iteration 348 ] Training loss: 0.0464085
DEBUG:root:[ Iteration 351 ] Training loss: 0.0445273
DEBUG:root:[ Iteration 354 ] Training loss: 0.0437457
DEBUG:root:[ Iteration 357 ] Training loss: 0.0387694
DEBUG:root:[ Iteration 360 ] Training loss: 0.0397044
DEBUG:root:[ Iteration 360 ] Test loss: 0.0445034
DEBUG:root:[ Iteration 363 ] Training loss: 0.0397837
DEBUG:root:[ Iteration 366 ] Training loss: 0.0399156
DEBUG:root:[ Iteration 369 ] Training loss: 0.048379
DEBUG:root:[ Iteration 372 ] Training loss: 0.0379366
DEBUG:root:[ Iteration 375 ] Training loss: 0.0410421
DEBUG:root:[ Iteration 378 ] Training loss: 0.0344252
DEBUG:root:[ Iteration 380 ] Test loss: 0.0587274
DEBUG:root:[ Iteration 381 ] Training loss: 0.0501592
DEBUG:root:[ Iteration 384 ] Training loss: 0.0394716
DEBUG:root:[ Iteration 387 ] Training loss: 0.0470566
DEBUG:root:[ Iteration 390 ] Training loss: 0.035938
DEBUG:root:[ Iteration 393 ] Training loss: 0.0435031
DEBUG:root:[ Iteration 396 ] Training loss: 0.0368202
DEBUG:root:[ Iteration 399 ] Training loss: 0.040083
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-17-2016_19h59m52s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-17-2016_20h01m16s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.174818
DEBUG:root:[ Iteration 0 ] Test loss: 0.183447
DEBUG:root:[ Iteration 0 ] Training loss: 0.174117
DEBUG:root:[ Iteration 0 ] Test loss: 0.18051
DEBUG:root:[ Iteration 3 ] Training loss: 0.171732
DEBUG:root:[ Iteration 6 ] Training loss: 0.160118
DEBUG:root:[ Iteration 9 ] Training loss: 0.118874
DEBUG:root:[ Iteration 12 ] Training loss: 0.106212
DEBUG:root:[ Iteration 15 ] Training loss: 0.121669
DEBUG:root:[ Iteration 18 ] Training loss: 0.109428
DEBUG:root:[ Iteration 20 ] Test loss: 0.105109
DEBUG:root:[ Iteration 21 ] Training loss: 0.123338
DEBUG:root:[ Iteration 24 ] Training loss: 0.115779
DEBUG:root:[ Iteration 27 ] Training loss: 0.104202
DEBUG:root:[ Iteration 30 ] Training loss: 0.0933773
DEBUG:root:[ Iteration 33 ] Training loss: 0.113015
DEBUG:root:[ Iteration 36 ] Training loss: 0.116596
DEBUG:root:[ Iteration 39 ] Training loss: 0.111757
DEBUG:root:[ Iteration 40 ] Test loss: 0.12235
DEBUG:root:[ Iteration 42 ] Training loss: 0.0948903
DEBUG:root:[ Iteration 45 ] Training loss: 0.112469
DEBUG:root:[ Iteration 48 ] Training loss: 0.0965815
DEBUG:root:[ Iteration 51 ] Training loss: 0.104732
DEBUG:root:[ Iteration 54 ] Training loss: 0.102734
DEBUG:root:[ Iteration 57 ] Training loss: 0.0919063
DEBUG:root:[ Iteration 60 ] Training loss: 0.0911993
DEBUG:root:[ Iteration 60 ] Test loss: 0.0842514
DEBUG:root:[ Iteration 63 ] Training loss: 0.0841606
DEBUG:root:[ Iteration 66 ] Training loss: 0.0957873
DEBUG:root:[ Iteration 69 ] Training loss: 0.0755263
DEBUG:root:[ Iteration 72 ] Training loss: 0.0910672
DEBUG:root:[ Iteration 75 ] Training loss: 0.0814831
DEBUG:root:[ Iteration 78 ] Training loss: 0.0793768
DEBUG:root:[ Iteration 80 ] Test loss: 0.0776871
DEBUG:root:[ Iteration 81 ] Training loss: 0.0976472
DEBUG:root:[ Iteration 84 ] Training loss: 0.0831519
DEBUG:root:[ Iteration 87 ] Training loss: 0.0834798
DEBUG:root:[ Iteration 90 ] Training loss: 0.0674152
DEBUG:root:[ Iteration 93 ] Training loss: 0.0697999
DEBUG:root:[ Iteration 96 ] Training loss: 0.0669121
DEBUG:root:[ Iteration 99 ] Training loss: 0.0794583
DEBUG:root:[ Iteration 100 ] Test loss: 0.0631375
DEBUG:root:[ Iteration 102 ] Training loss: 0.0618044
DEBUG:root:[ Iteration 105 ] Training loss: 0.0777792
DEBUG:root:[ Iteration 108 ] Training loss: 0.0656185
DEBUG:root:[ Iteration 111 ] Training loss: 0.0686981
DEBUG:root:[ Iteration 114 ] Training loss: 0.0648649
DEBUG:root:[ Iteration 117 ] Training loss: 0.0820179
DEBUG:root:[ Iteration 120 ] Training loss: 0.0754132
DEBUG:root:[ Iteration 120 ] Test loss: 0.0732807
DEBUG:root:[ Iteration 123 ] Training loss: 0.0630816
DEBUG:root:[ Iteration 126 ] Training loss: 0.0682721
DEBUG:root:[ Iteration 129 ] Training loss: 0.0655239
DEBUG:root:[ Iteration 132 ] Training loss: 0.0717757
DEBUG:root:[ Iteration 135 ] Training loss: 0.0667116
DEBUG:root:[ Iteration 138 ] Training loss: 0.0624383
DEBUG:root:[ Iteration 140 ] Test loss: 0.0675007
DEBUG:root:[ Iteration 141 ] Training loss: 0.0724125
DEBUG:root:[ Iteration 144 ] Training loss: 0.0663045
DEBUG:root:[ Iteration 147 ] Training loss: 0.056467
DEBUG:root:[ Iteration 150 ] Training loss: 0.0646492
DEBUG:root:[ Iteration 153 ] Training loss: 0.0542011
DEBUG:root:[ Iteration 156 ] Training loss: 0.0566003
DEBUG:root:[ Iteration 159 ] Training loss: 0.0764996
DEBUG:root:[ Iteration 160 ] Test loss: 0.0638363
DEBUG:root:[ Iteration 162 ] Training loss: 0.0572331
DEBUG:root:[ Iteration 165 ] Training loss: 0.0623001
DEBUG:root:[ Iteration 168 ] Training loss: 0.057097
DEBUG:root:[ Iteration 171 ] Training loss: 0.0717764
DEBUG:root:[ Iteration 174 ] Training loss: 0.0629095
DEBUG:root:[ Iteration 177 ] Training loss: 0.0596342
DEBUG:root:[ Iteration 180 ] Training loss: 0.0544974
DEBUG:root:[ Iteration 180 ] Test loss: 0.0592789
DEBUG:root:[ Iteration 183 ] Training loss: 0.0477235
DEBUG:root:[ Iteration 186 ] Training loss: 0.056959
DEBUG:root:[ Iteration 189 ] Training loss: 0.0575061
DEBUG:root:[ Iteration 192 ] Training loss: 0.0599623
DEBUG:root:[ Iteration 195 ] Training loss: 0.0589914
DEBUG:root:[ Iteration 198 ] Training loss: 0.0710672
DEBUG:root:[ Iteration 200 ] Test loss: 0.0585658
DEBUG:root:[ Iteration 201 ] Training loss: 0.0498268
DEBUG:root:[ Iteration 204 ] Training loss: 0.0553613
DEBUG:root:[ Iteration 207 ] Training loss: 0.0538087
DEBUG:root:[ Iteration 210 ] Training loss: 0.0567854
DEBUG:root:[ Iteration 213 ] Training loss: 0.0625273
DEBUG:root:[ Iteration 216 ] Training loss: 0.0580095
DEBUG:root:[ Iteration 219 ] Training loss: 0.0662434
DEBUG:root:[ Iteration 220 ] Test loss: 0.050276
DEBUG:root:[ Iteration 222 ] Training loss: 0.0537976
DEBUG:root:[ Iteration 225 ] Training loss: 0.0671288
DEBUG:root:[ Iteration 228 ] Training loss: 0.0520842
DEBUG:root:[ Iteration 231 ] Training loss: 0.0479006
DEBUG:root:[ Iteration 234 ] Training loss: 0.0542468
DEBUG:root:[ Iteration 237 ] Training loss: 0.057143
DEBUG:root:[ Iteration 240 ] Training loss: 0.0541872
DEBUG:root:[ Iteration 240 ] Test loss: 0.0688567
DEBUG:root:[ Iteration 243 ] Training loss: 0.0493744
DEBUG:root:[ Iteration 246 ] Training loss: 0.0649545
DEBUG:root:[ Iteration 249 ] Training loss: 0.0614101
DEBUG:root:[ Iteration 252 ] Training loss: 0.078031
DEBUG:root:[ Iteration 255 ] Training loss: 0.0573879
DEBUG:root:[ Iteration 258 ] Training loss: 0.0523528
DEBUG:root:[ Iteration 260 ] Test loss: 0.0494705
DEBUG:root:[ Iteration 261 ] Training loss: 0.0486354
DEBUG:root:[ Iteration 264 ] Training loss: 0.0563056
DEBUG:root:[ Iteration 267 ] Training loss: 0.058966
DEBUG:root:[ Iteration 270 ] Training loss: 0.0510013
DEBUG:root:[ Iteration 273 ] Training loss: 0.0534975
DEBUG:root:[ Iteration 276 ] Training loss: 0.0480051
DEBUG:root:[ Iteration 279 ] Training loss: 0.0622139
DEBUG:root:[ Iteration 280 ] Test loss: 0.056039
DEBUG:root:[ Iteration 282 ] Training loss: 0.0567072
DEBUG:root:[ Iteration 285 ] Training loss: 0.0483498
DEBUG:root:[ Iteration 288 ] Training loss: 0.0416645
DEBUG:root:[ Iteration 291 ] Training loss: 0.0432934
DEBUG:root:[ Iteration 294 ] Training loss: 0.0446988
DEBUG:root:[ Iteration 297 ] Training loss: 0.0455279
DEBUG:root:[ Iteration 300 ] Training loss: 0.0523135
DEBUG:root:[ Iteration 300 ] Test loss: 0.0498928
DEBUG:root:[ Iteration 303 ] Training loss: 0.0439992
DEBUG:root:[ Iteration 306 ] Training loss: 0.0550253
DEBUG:root:[ Iteration 309 ] Training loss: 0.0490008
DEBUG:root:[ Iteration 312 ] Training loss: 0.0487007
DEBUG:root:[ Iteration 315 ] Training loss: 0.0304174
DEBUG:root:[ Iteration 318 ] Training loss: 0.0485263
DEBUG:root:[ Iteration 320 ] Test loss: 0.0593763
DEBUG:root:[ Iteration 321 ] Training loss: 0.0491713
DEBUG:root:[ Iteration 324 ] Training loss: 0.045509
DEBUG:root:[ Iteration 327 ] Training loss: 0.042681
DEBUG:root:[ Iteration 330 ] Training loss: 0.0584041
DEBUG:root:[ Iteration 333 ] Training loss: 0.0357171
DEBUG:root:[ Iteration 336 ] Training loss: 0.0367016
DEBUG:root:[ Iteration 339 ] Training loss: 0.0449362
DEBUG:root:[ Iteration 340 ] Test loss: 0.047403
DEBUG:root:[ Iteration 342 ] Training loss: 0.0466433
DEBUG:root:[ Iteration 345 ] Training loss: 0.040881
DEBUG:root:[ Iteration 348 ] Training loss: 0.0485556
DEBUG:root:[ Iteration 351 ] Training loss: 0.0488334
DEBUG:root:[ Iteration 354 ] Training loss: 0.0490854
DEBUG:root:[ Iteration 357 ] Training loss: 0.048666
DEBUG:root:[ Iteration 360 ] Training loss: 0.052024
DEBUG:root:[ Iteration 360 ] Test loss: 0.0589634
DEBUG:root:[ Iteration 363 ] Training loss: 0.0466727
DEBUG:root:[ Iteration 366 ] Training loss: 0.0370093
DEBUG:root:[ Iteration 369 ] Training loss: 0.0434373
DEBUG:root:[ Iteration 372 ] Training loss: 0.042479
DEBUG:root:[ Iteration 375 ] Training loss: 0.0398049
DEBUG:root:[ Iteration 378 ] Training loss: 0.0552011
DEBUG:root:[ Iteration 380 ] Test loss: 0.0401264
DEBUG:root:[ Iteration 381 ] Training loss: 0.0475213
DEBUG:root:[ Iteration 384 ] Training loss: 0.039685
DEBUG:root:[ Iteration 387 ] Training loss: 0.04175
DEBUG:root:[ Iteration 390 ] Training loss: 0.0457858
DEBUG:root:[ Iteration 393 ] Training loss: 0.0470572
DEBUG:root:[ Iteration 396 ] Training loss: 0.0363555
DEBUG:root:[ Iteration 399 ] Training loss: 0.0416727
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-17-2016_20h12m42s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.128901
DEBUG:root:[ Iteration 0 ] Test loss: 0.127119
DEBUG:root:[ Iteration 3 ] Training loss: 0.12711
DEBUG:root:[ Iteration 6 ] Training loss: 0.129379
DEBUG:root:[ Iteration 9 ] Training loss: 0.107274
DEBUG:root:[ Iteration 12 ] Training loss: 0.0578319
DEBUG:root:[ Iteration 15 ] Training loss: 0.0512377
DEBUG:root:[ Iteration 18 ] Training loss: 0.0544413
DEBUG:root:[ Iteration 20 ] Test loss: 0.0553902
DEBUG:root:[ Iteration 21 ] Training loss: 0.0452841
DEBUG:root:[ Iteration 24 ] Training loss: 0.0499135
DEBUG:root:[ Iteration 27 ] Training loss: 0.0498755
DEBUG:root:[ Iteration 30 ] Training loss: 0.05712
DEBUG:root:[ Iteration 33 ] Training loss: 0.075933
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-27-2016_11h13m08s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0164686
DEBUG:root:[ Iteration 0 ] Test loss: 0.0200549
DEBUG:root:[ Iteration 3 ] Training loss: 0.017684
DEBUG:root:[ Iteration 6 ] Training loss: 0.0163703
DEBUG:root:[ Iteration 9 ] Training loss: 0.0128383
DEBUG:root:[ Iteration 12 ] Training loss: 0.013831
DEBUG:root:[ Iteration 15 ] Training loss: 0.0119098
DEBUG:root:[ Iteration 18 ] Training loss: 0.0102644
DEBUG:root:[ Iteration 20 ] Test loss: 0.0111735
DEBUG:root:[ Iteration 21 ] Training loss: 0.00996634
DEBUG:root:[ Iteration 24 ] Training loss: 0.01068
DEBUG:root:[ Iteration 27 ] Training loss: 0.0106413
DEBUG:root:[ Iteration 30 ] Training loss: 0.00986551
DEBUG:root:[ Iteration 33 ] Training loss: 0.00838013
DEBUG:root:[ Iteration 36 ] Training loss: 0.00869585
DEBUG:root:[ Iteration 39 ] Training loss: 0.00890963
DEBUG:root:[ Iteration 40 ] Test loss: 0.0112488
DEBUG:root:[ Iteration 42 ] Training loss: 0.0105719
DEBUG:root:[ Iteration 45 ] Training loss: 0.0106581
DEBUG:root:[ Iteration 48 ] Training loss: 0.00695946
DEBUG:root:[ Iteration 51 ] Training loss: 0.00854702
DEBUG:root:[ Iteration 54 ] Training loss: 0.00679713
DEBUG:root:[ Iteration 57 ] Training loss: 0.00666276
DEBUG:root:[ Iteration 60 ] Training loss: 0.00875635
DEBUG:root:[ Iteration 60 ] Test loss: 0.0100822
DEBUG:root:[ Iteration 63 ] Training loss: 0.00861885
DEBUG:root:[ Iteration 66 ] Training loss: 0.00725412
DEBUG:root:[ Iteration 69 ] Training loss: 0.0106496
DEBUG:root:[ Iteration 72 ] Training loss: 0.00621761
DEBUG:root:[ Iteration 75 ] Training loss: 0.00857181
DEBUG:root:[ Iteration 78 ] Training loss: 0.00723003
DEBUG:root:[ Iteration 80 ] Test loss: 0.00968403
DEBUG:root:[ Iteration 81 ] Training loss: 0.00696809
DEBUG:root:[ Iteration 84 ] Training loss: 0.0114881
DEBUG:root:[ Iteration 87 ] Training loss: 0.00934514
DEBUG:root:[ Iteration 90 ] Training loss: 0.00881206
DEBUG:root:[ Iteration 93 ] Training loss: 0.00857551
DEBUG:root:[ Iteration 96 ] Training loss: 0.00661426
DEBUG:root:[ Iteration 99 ] Training loss: 0.00871907
DEBUG:root:[ Iteration 100 ] Test loss: 0.00609709
DEBUG:root:[ Iteration 102 ] Training loss: 0.00675585
DEBUG:root:[ Iteration 105 ] Training loss: 0.00851876
DEBUG:root:[ Iteration 108 ] Training loss: 0.00660812
DEBUG:root:[ Iteration 111 ] Training loss: 0.0087894
DEBUG:root:[ Iteration 114 ] Training loss: 0.00679377
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-27-2016_11h16m08s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0098106
DEBUG:root:[ Iteration 0 ] Test loss: 0.00845331
DEBUG:root:[ Iteration 3 ] Training loss: 0.00815163
DEBUG:root:[ Iteration 6 ] Training loss: 0.00876201
DEBUG:root:[ Iteration 9 ] Training loss: 0.00907746
DEBUG:root:[ Iteration 12 ] Training loss: 0.0123791
DEBUG:root:[ Iteration 15 ] Training loss: 0.0118845
DEBUG:root:[ Iteration 18 ] Training loss: 0.00911205
DEBUG:root:[ Iteration 20 ] Test loss: 0.00962527
DEBUG:root:[ Iteration 21 ] Training loss: 0.00841638
DEBUG:root:[ Iteration 24 ] Training loss: 0.00695509
DEBUG:root:[ Iteration 27 ] Training loss: 0.0101934
DEBUG:root:[ Iteration 30 ] Training loss: 0.00683809
DEBUG:root:[ Iteration 33 ] Training loss: 0.00642714
DEBUG:root:[ Iteration 36 ] Training loss: 0.00492525
DEBUG:root:[ Iteration 39 ] Training loss: 0.00987455
DEBUG:root:[ Iteration 40 ] Test loss: 0.00865105
DEBUG:root:[ Iteration 42 ] Training loss: 0.0073675
DEBUG:root:[ Iteration 45 ] Training loss: 0.00627747
DEBUG:root:[ Iteration 48 ] Training loss: 0.00721835
DEBUG:root:[ Iteration 51 ] Training loss: 0.00873729
DEBUG:root:[ Iteration 54 ] Training loss: 0.00827459
DEBUG:root:[ Iteration 57 ] Training loss: 0.00672746
DEBUG:root:[ Iteration 60 ] Training loss: 0.0076848
DEBUG:root:[ Iteration 60 ] Test loss: 0.00731835
DEBUG:root:[ Iteration 63 ] Training loss: 0.00797546
DEBUG:root:[ Iteration 66 ] Training loss: 0.00728659
DEBUG:root:[ Iteration 69 ] Training loss: 0.0072867
DEBUG:root:[ Iteration 72 ] Training loss: 0.00798018
DEBUG:root:[ Iteration 75 ] Training loss: 0.00745416
DEBUG:root:[ Iteration 78 ] Training loss: 0.00712089
DEBUG:root:[ Iteration 80 ] Test loss: 0.00748685
DEBUG:root:[ Iteration 81 ] Training loss: 0.0061327
DEBUG:root:[ Iteration 84 ] Training loss: 0.00527183
DEBUG:root:[ Iteration 87 ] Training loss: 0.00983052
DEBUG:root:[ Iteration 90 ] Training loss: 0.00578386
DEBUG:root:[ Iteration 93 ] Training loss: 0.00572154
DEBUG:root:[ Iteration 96 ] Training loss: 0.00508373
DEBUG:root:[ Iteration 99 ] Training loss: 0.00843412
DEBUG:root:[ Iteration 100 ] Test loss: 0.00918665
DEBUG:root:[ Iteration 102 ] Training loss: 0.00439581
DEBUG:root:[ Iteration 105 ] Training loss: 0.00690468
DEBUG:root:[ Iteration 108 ] Training loss: 0.00847386
DEBUG:root:[ Iteration 111 ] Training loss: 0.00479023
DEBUG:root:[ Iteration 114 ] Training loss: 0.00685334
DEBUG:root:[ Iteration 117 ] Training loss: 0.00951668
DEBUG:root:[ Iteration 120 ] Training loss: 0.00699612
DEBUG:root:[ Iteration 120 ] Test loss: 0.00655403
DEBUG:root:[ Iteration 123 ] Training loss: 0.00748769
DEBUG:root:[ Iteration 126 ] Training loss: 0.00499588
DEBUG:root:[ Iteration 129 ] Training loss: 0.00737334
DEBUG:root:[ Iteration 132 ] Training loss: 0.00616528
DEBUG:root:[ Iteration 135 ] Training loss: 0.00694023
DEBUG:root:[ Iteration 138 ] Training loss: 0.0059781
DEBUG:root:[ Iteration 140 ] Test loss: 0.00731256
DEBUG:root:[ Iteration 141 ] Training loss: 0.00692514
DEBUG:root:[ Iteration 144 ] Training loss: 0.0083087
DEBUG:root:[ Iteration 147 ] Training loss: 0.00514266
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-27-2016_12h04m13s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0967926
DEBUG:root:[ Iteration 0 ] Test loss: 0.110317
DEBUG:root:[ Iteration 3 ] Training loss: 0.0843805
DEBUG:root:[ Iteration 6 ] Training loss: 0.0939806
DEBUG:root:[ Iteration 9 ] Training loss: 0.0618868
DEBUG:root:[ Iteration 12 ] Training loss: 0.0513448
DEBUG:root:[ Iteration 15 ] Training loss: 0.0424039
DEBUG:root:[ Iteration 18 ] Training loss: 0.0350266
DEBUG:root:[ Iteration 20 ] Test loss: 0.0377599
DEBUG:root:[ Iteration 21 ] Training loss: 0.0290533
DEBUG:root:[ Iteration 24 ] Training loss: 0.0253153
DEBUG:root:[ Iteration 27 ] Training loss: 0.0260814
DEBUG:root:[ Iteration 30 ] Training loss: 0.0291435
DEBUG:root:[ Iteration 33 ] Training loss: 0.0261971
DEBUG:root:[ Iteration 36 ] Training loss: 0.0239104
DEBUG:root:[ Iteration 39 ] Training loss: 0.0225354
DEBUG:root:[ Iteration 40 ] Test loss: 0.0244197
DEBUG:root:[ Iteration 42 ] Training loss: 0.0195723
DEBUG:root:[ Iteration 45 ] Training loss: 0.0220855
DEBUG:root:[ Iteration 48 ] Training loss: 0.020608
DEBUG:root:[ Iteration 51 ] Training loss: 0.0227176
DEBUG:root:[ Iteration 54 ] Training loss: 0.0172986
DEBUG:root:[ Iteration 57 ] Training loss: 0.0185619
DEBUG:root:[ Iteration 60 ] Training loss: 0.0197736
DEBUG:root:[ Iteration 60 ] Test loss: 0.0180147
DEBUG:root:[ Iteration 63 ] Training loss: 0.0201158
DEBUG:root:[ Iteration 66 ] Training loss: 0.0175032
DEBUG:root:[ Iteration 69 ] Training loss: 0.021075
DEBUG:root:[ Iteration 72 ] Training loss: 0.0162295
DEBUG:root:[ Iteration 75 ] Training loss: 0.019858
DEBUG:root:[ Iteration 78 ] Training loss: 0.0163578
DEBUG:root:[ Iteration 80 ] Test loss: 0.0153809
DEBUG:root:[ Iteration 81 ] Training loss: 0.0164851
DEBUG:root:[ Iteration 84 ] Training loss: 0.0154104
DEBUG:root:[ Iteration 87 ] Training loss: 0.0153414
DEBUG:root:[ Iteration 90 ] Training loss: 0.0160917
DEBUG:root:[ Iteration 93 ] Training loss: 0.0140391
DEBUG:root:[ Iteration 96 ] Training loss: 0.0147849
DEBUG:root:[ Iteration 99 ] Training loss: 0.0151789
DEBUG:root:[ Iteration 100 ] Test loss: 0.0160816
DEBUG:root:[ Iteration 102 ] Training loss: 0.0145854
DEBUG:root:[ Iteration 105 ] Training loss: 0.0154722
DEBUG:root:[ Iteration 108 ] Training loss: 0.0140035
DEBUG:root:[ Iteration 111 ] Training loss: 0.013309
DEBUG:root:[ Iteration 114 ] Training loss: 0.0136754
DEBUG:root:[ Iteration 117 ] Training loss: 0.0149827
DEBUG:root:[ Iteration 120 ] Training loss: 0.012045
DEBUG:root:[ Iteration 120 ] Test loss: 0.01256
DEBUG:root:[ Iteration 123 ] Training loss: 0.0122409
DEBUG:root:[ Iteration 126 ] Training loss: 0.0133071
DEBUG:root:[ Iteration 129 ] Training loss: 0.0133851
DEBUG:root:[ Iteration 132 ] Training loss: 0.0113094
DEBUG:root:[ Iteration 135 ] Training loss: 0.011857
DEBUG:root:[ Iteration 138 ] Training loss: 0.0138434
DEBUG:root:[ Iteration 140 ] Test loss: 0.0133402
DEBUG:root:[ Iteration 141 ] Training loss: 0.0125101
DEBUG:root:[ Iteration 144 ] Training loss: 0.0128438
DEBUG:root:[ Iteration 147 ] Training loss: 0.0117264
DEBUG:root:[ Iteration 150 ] Training loss: 0.0108812
DEBUG:root:[ Iteration 153 ] Training loss: 0.0125603
DEBUG:root:[ Iteration 156 ] Training loss: 0.0116881
DEBUG:root:[ Iteration 159 ] Training loss: 0.0119799
DEBUG:root:[ Iteration 160 ] Test loss: 0.0133603
DEBUG:root:[ Iteration 162 ] Training loss: 0.0107021
DEBUG:root:[ Iteration 165 ] Training loss: 0.0115008
DEBUG:root:[ Iteration 168 ] Training loss: 0.0100547
DEBUG:root:[ Iteration 171 ] Training loss: 0.010958
DEBUG:root:[ Iteration 174 ] Training loss: 0.0123439
DEBUG:root:[ Iteration 177 ] Training loss: 0.00841865
DEBUG:root:[ Iteration 180 ] Training loss: 0.0113038
DEBUG:root:[ Iteration 180 ] Test loss: 0.011782
DEBUG:root:[ Iteration 183 ] Training loss: 0.0112356
DEBUG:root:[ Iteration 186 ] Training loss: 0.00857321
DEBUG:root:[ Iteration 189 ] Training loss: 0.00970646
DEBUG:root:[ Iteration 192 ] Training loss: 0.0106229
DEBUG:root:[ Iteration 195 ] Training loss: 0.0102387
DEBUG:root:[ Iteration 198 ] Training loss: 0.012164
DEBUG:root:[ Iteration 200 ] Test loss: 0.0131176
DEBUG:root:[ Iteration 201 ] Training loss: 0.0092511
DEBUG:root:[ Iteration 204 ] Training loss: 0.00762221
DEBUG:root:[ Iteration 207 ] Training loss: 0.0090558
DEBUG:root:[ Iteration 210 ] Training loss: 0.00923161
DEBUG:root:[ Iteration 213 ] Training loss: 0.0101087
DEBUG:root:[ Iteration 216 ] Training loss: 0.00824989
DEBUG:root:[ Iteration 219 ] Training loss: 0.0109167
DEBUG:root:[ Iteration 220 ] Test loss: 0.0105813
DEBUG:root:[ Iteration 222 ] Training loss: 0.0103655
DEBUG:root:[ Iteration 225 ] Training loss: 0.0103794
DEBUG:root:[ Iteration 228 ] Training loss: 0.0108715
DEBUG:root:[ Iteration 231 ] Training loss: 0.00912887
DEBUG:root:[ Iteration 234 ] Training loss: 0.00888374
DEBUG:root:[ Iteration 237 ] Training loss: 0.00920064
DEBUG:root:[ Iteration 240 ] Training loss: 0.00707144
DEBUG:root:[ Iteration 240 ] Test loss: 0.00911323
DEBUG:root:[ Iteration 243 ] Training loss: 0.00821984
DEBUG:root:[ Iteration 246 ] Training loss: 0.00959198
DEBUG:root:[ Iteration 249 ] Training loss: 0.0103025
DEBUG:root:[ Iteration 252 ] Training loss: 0.00942345
DEBUG:root:[ Iteration 255 ] Training loss: 0.00952705
DEBUG:root:[ Iteration 258 ] Training loss: 0.0101821
DEBUG:root:[ Iteration 260 ] Test loss: 0.00877426
DEBUG:root:[ Iteration 261 ] Training loss: 0.0076054
DEBUG:root:[ Iteration 264 ] Training loss: 0.00743121
DEBUG:root:[ Iteration 267 ] Training loss: 0.00857643
DEBUG:root:[ Iteration 270 ] Training loss: 0.00932772
DEBUG:root:[ Iteration 273 ] Training loss: 0.00774629
DEBUG:root:[ Iteration 276 ] Training loss: 0.0093563
DEBUG:root:[ Iteration 279 ] Training loss: 0.00704625
DEBUG:root:[ Iteration 280 ] Test loss: 0.00956422
DEBUG:root:[ Iteration 282 ] Training loss: 0.00582353
DEBUG:root:[ Iteration 285 ] Training loss: 0.00687972
DEBUG:root:[ Iteration 288 ] Training loss: 0.00683098
DEBUG:root:[ Iteration 291 ] Training loss: 0.00899639
DEBUG:root:[ Iteration 294 ] Training loss: 0.00935829
DEBUG:root:[ Iteration 297 ] Training loss: 0.00732609
DEBUG:root:[ Iteration 300 ] Training loss: 0.0103077
DEBUG:root:[ Iteration 300 ] Test loss: 0.00955849
DEBUG:root:[ Iteration 303 ] Training loss: 0.00657674
DEBUG:root:[ Iteration 306 ] Training loss: 0.0090754
DEBUG:root:[ Iteration 309 ] Training loss: 0.0091673
DEBUG:root:[ Iteration 312 ] Training loss: 0.00784063
DEBUG:root:[ Iteration 315 ] Training loss: 0.00946209
DEBUG:root:[ Iteration 318 ] Training loss: 0.00827408
DEBUG:root:[ Iteration 320 ] Test loss: 0.00896183
DEBUG:root:[ Iteration 321 ] Training loss: 0.00634306
DEBUG:root:[ Iteration 324 ] Training loss: 0.00827456
DEBUG:root:[ Iteration 327 ] Training loss: 0.00608788
DEBUG:root:[ Iteration 330 ] Training loss: 0.00701857
DEBUG:root:[ Iteration 333 ] Training loss: 0.00544618
DEBUG:root:[ Iteration 336 ] Training loss: 0.0059576
DEBUG:root:[ Iteration 339 ] Training loss: 0.00703097
DEBUG:root:[ Iteration 340 ] Test loss: 0.00941811
DEBUG:root:[ Iteration 342 ] Training loss: 0.00852676
DEBUG:root:[ Iteration 345 ] Training loss: 0.0070244
DEBUG:root:[ Iteration 348 ] Training loss: 0.0075386
DEBUG:root:[ Iteration 351 ] Training loss: 0.0070929
DEBUG:root:[ Iteration 354 ] Training loss: 0.00681846
DEBUG:root:[ Iteration 357 ] Training loss: 0.00833834
DEBUG:root:[ Iteration 360 ] Training loss: 0.00703083
DEBUG:root:[ Iteration 360 ] Test loss: 0.00803819
DEBUG:root:[ Iteration 363 ] Training loss: 0.0078774
DEBUG:root:[ Iteration 366 ] Training loss: 0.0094233
DEBUG:root:[ Iteration 369 ] Training loss: 0.00674741
DEBUG:root:[ Iteration 372 ] Training loss: 0.00559711
DEBUG:root:[ Iteration 375 ] Training loss: 0.00606752
DEBUG:root:[ Iteration 378 ] Training loss: 0.00548238
DEBUG:root:[ Iteration 380 ] Test loss: 0.00773967
DEBUG:root:[ Iteration 381 ] Training loss: 0.00634155
DEBUG:root:[ Iteration 384 ] Training loss: 0.00639774
DEBUG:root:[ Iteration 387 ] Training loss: 0.00667827
DEBUG:root:[ Iteration 390 ] Training loss: 0.00566617
DEBUG:root:[ Iteration 393 ] Training loss: 0.00729605
DEBUG:root:[ Iteration 396 ] Training loss: 0.00723488
DEBUG:root:[ Iteration 399 ] Training loss: 0.00622016
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_03-28-2016_14h51m12s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.147724
DEBUG:root:[ Iteration 0 ] Test loss: 0.153351
DEBUG:root:[ Iteration 3 ] Training loss: 0.144435
DEBUG:root:[ Iteration 6 ] Training loss: 0.147156
DEBUG:root:[ Iteration 9 ] Training loss: 0.150099
DEBUG:root:[ Iteration 12 ] Training loss: 0.14344
DEBUG:root:[ Iteration 15 ] Training loss: 0.133972
DEBUG:root:[ Iteration 18 ] Training loss: 0.0926041
DEBUG:root:[ Iteration 20 ] Test loss: 0.0574835
DEBUG:root:[ Iteration 21 ] Training loss: 0.0508789
DEBUG:root:[ Iteration 24 ] Training loss: 0.0492942
DEBUG:root:[ Iteration 27 ] Training loss: 0.0440388
DEBUG:root:[ Iteration 30 ] Training loss: 0.043264
DEBUG:root:[ Iteration 33 ] Training loss: 0.0412253
DEBUG:root:[ Iteration 36 ] Training loss: 0.0477088
DEBUG:root:[ Iteration 39 ] Training loss: 0.0480088
DEBUG:root:[ Iteration 40 ] Test loss: 0.0602251
DEBUG:root:[ Iteration 42 ] Training loss: 0.0494997
DEBUG:root:[ Iteration 45 ] Training loss: 0.0764999
DEBUG:root:[ Iteration 48 ] Training loss: 0.0354362
DEBUG:root:[ Iteration 51 ] Training loss: 0.0493422
DEBUG:root:[ Iteration 54 ] Training loss: 0.0375496
DEBUG:root:[ Iteration 57 ] Training loss: 0.0440183
DEBUG:root:[ Iteration 60 ] Training loss: 0.0436649
DEBUG:root:[ Iteration 60 ] Test loss: 0.0365682
DEBUG:root:[ Iteration 63 ] Training loss: 0.0354964
DEBUG:root:[ Iteration 66 ] Training loss: 0.0334921
DEBUG:root:[ Iteration 69 ] Training loss: 0.0397676
DEBUG:root:[ Iteration 72 ] Training loss: 0.0413751
DEBUG:root:[ Iteration 75 ] Training loss: 0.0350797
DEBUG:root:[ Iteration 78 ] Training loss: 0.0349875
DEBUG:root:[ Iteration 80 ] Test loss: 0.0307494
DEBUG:root:[ Iteration 81 ] Training loss: 0.0364168
DEBUG:root:[ Iteration 84 ] Training loss: 0.0337593
DEBUG:root:[ Iteration 87 ] Training loss: 0.033515
DEBUG:root:[ Iteration 90 ] Training loss: 0.0378028
DEBUG:root:[ Iteration 93 ] Training loss: 0.0330195
DEBUG:root:[ Iteration 96 ] Training loss: 0.0378568
DEBUG:root:[ Iteration 99 ] Training loss: 0.0298012
DEBUG:root:[ Iteration 100 ] Test loss: 0.0324568
DEBUG:root:[ Iteration 102 ] Training loss: 0.0315265
DEBUG:root:[ Iteration 105 ] Training loss: 0.030738
DEBUG:root:[ Iteration 108 ] Training loss: 0.0353922
DEBUG:root:[ Iteration 111 ] Training loss: 0.0316932
DEBUG:root:[ Iteration 114 ] Training loss: 0.0329814
DEBUG:root:[ Iteration 117 ] Training loss: 0.0362297
DEBUG:root:[ Iteration 120 ] Training loss: 0.0292211
DEBUG:root:[ Iteration 120 ] Test loss: 0.0285416
DEBUG:root:[ Iteration 123 ] Training loss: 0.0415246
DEBUG:root:[ Iteration 126 ] Training loss: 0.0358828
DEBUG:root:[ Iteration 129 ] Training loss: 0.0258429
DEBUG:root:[ Iteration 132 ] Training loss: 0.033125
DEBUG:root:[ Iteration 135 ] Training loss: 0.0289906
DEBUG:root:[ Iteration 138 ] Training loss: 0.0327949
DEBUG:root:[ Iteration 140 ] Test loss: 0.0271195
DEBUG:root:[ Iteration 141 ] Training loss: 0.0272255
DEBUG:root:[ Iteration 144 ] Training loss: 0.0372901
DEBUG:root:[ Iteration 147 ] Training loss: 0.0318667
DEBUG:root:[ Iteration 150 ] Training loss: 0.0251841
DEBUG:root:[ Iteration 153 ] Training loss: 0.0301013
DEBUG:root:[ Iteration 156 ] Training loss: 0.0283633
DEBUG:root:[ Iteration 159 ] Training loss: 0.0324797
DEBUG:root:[ Iteration 160 ] Test loss: 0.0273055
DEBUG:root:[ Iteration 162 ] Training loss: 0.0256819
DEBUG:root:[ Iteration 165 ] Training loss: 0.026173
DEBUG:root:[ Iteration 168 ] Training loss: 0.0270119
DEBUG:root:[ Iteration 171 ] Training loss: 0.0323153
DEBUG:root:[ Iteration 174 ] Training loss: 0.028098
DEBUG:root:[ Iteration 177 ] Training loss: 0.0289068
DEBUG:root:[ Iteration 180 ] Training loss: 0.0234095
DEBUG:root:[ Iteration 180 ] Test loss: 0.0260629
DEBUG:root:[ Iteration 183 ] Training loss: 0.0285042
DEBUG:root:[ Iteration 186 ] Training loss: 0.0303322
DEBUG:root:[ Iteration 189 ] Training loss: 0.0333683
DEBUG:root:[ Iteration 192 ] Training loss: 0.0269262
DEBUG:root:[ Iteration 195 ] Training loss: 0.0324573
DEBUG:root:[ Iteration 198 ] Training loss: 0.0225152
DEBUG:root:[ Iteration 200 ] Test loss: 0.0257752
DEBUG:root:[ Iteration 201 ] Training loss: 0.0263494
DEBUG:root:[ Iteration 204 ] Training loss: 0.0249292
DEBUG:root:[ Iteration 207 ] Training loss: 0.0241701
DEBUG:root:[ Iteration 210 ] Training loss: 0.0253307
DEBUG:root:[ Iteration 213 ] Training loss: 0.0234291
DEBUG:root:[ Iteration 216 ] Training loss: 0.0195605
DEBUG:root:[ Iteration 219 ] Training loss: 0.0223866
DEBUG:root:[ Iteration 220 ] Test loss: 0.021185
DEBUG:root:[ Iteration 222 ] Training loss: 0.0223661
DEBUG:root:[ Iteration 225 ] Training loss: 0.0232439
DEBUG:root:[ Iteration 228 ] Training loss: 0.0227043
DEBUG:root:[ Iteration 231 ] Training loss: 0.0234698
DEBUG:root:[ Iteration 234 ] Training loss: 0.027393
DEBUG:root:[ Iteration 237 ] Training loss: 0.0225652
DEBUG:root:[ Iteration 240 ] Training loss: 0.0192403
DEBUG:root:[ Iteration 240 ] Test loss: 0.0200071
DEBUG:root:[ Iteration 243 ] Training loss: 0.0236556
DEBUG:root:[ Iteration 246 ] Training loss: 0.0213657
DEBUG:root:[ Iteration 249 ] Training loss: 0.0202836
DEBUG:root:[ Iteration 252 ] Training loss: 0.0198887
DEBUG:root:[ Iteration 255 ] Training loss: 0.0202665
DEBUG:root:[ Iteration 258 ] Training loss: 0.0218221
DEBUG:root:[ Iteration 260 ] Test loss: 0.0259627
DEBUG:root:[ Iteration 261 ] Training loss: 0.0210308
DEBUG:root:[ Iteration 264 ] Training loss: 0.0190706
DEBUG:root:[ Iteration 267 ] Training loss: 0.0255588
DEBUG:root:[ Iteration 270 ] Training loss: 0.0271458
DEBUG:root:[ Iteration 273 ] Training loss: 0.034698
DEBUG:root:[ Iteration 276 ] Training loss: 0.0335658
DEBUG:root:[ Iteration 279 ] Training loss: 0.0230164
DEBUG:root:[ Iteration 280 ] Test loss: 0.0211713
DEBUG:root:[ Iteration 282 ] Training loss: 0.0190695
DEBUG:root:[ Iteration 285 ] Training loss: 0.0236945
DEBUG:root:[ Iteration 288 ] Training loss: 0.0182361
DEBUG:root:[ Iteration 291 ] Training loss: 0.0242266
DEBUG:root:[ Iteration 294 ] Training loss: 0.019683
DEBUG:root:[ Iteration 297 ] Training loss: 0.0160704
DEBUG:root:[ Iteration 300 ] Training loss: 0.0199979
DEBUG:root:[ Iteration 300 ] Test loss: 0.0198179
DEBUG:root:[ Iteration 303 ] Training loss: 0.0194791
DEBUG:root:[ Iteration 306 ] Training loss: 0.0227736
DEBUG:root:[ Iteration 309 ] Training loss: 0.0173716
DEBUG:root:[ Iteration 312 ] Training loss: 0.0183201
DEBUG:root:[ Iteration 315 ] Training loss: 0.0164013
DEBUG:root:[ Iteration 318 ] Training loss: 0.0176229
DEBUG:root:[ Iteration 320 ] Test loss: 0.0165412
DEBUG:root:[ Iteration 321 ] Training loss: 0.0166186
DEBUG:root:[ Iteration 324 ] Training loss: 0.0195062
DEBUG:root:[ Iteration 327 ] Training loss: 0.0141971
DEBUG:root:[ Iteration 330 ] Training loss: 0.019615
DEBUG:root:[ Iteration 333 ] Training loss: 0.0192892
DEBUG:root:[ Iteration 336 ] Training loss: 0.0169794
DEBUG:root:[ Iteration 339 ] Training loss: 0.0301261
DEBUG:root:[ Iteration 340 ] Test loss: 0.0181317
DEBUG:root:[ Iteration 342 ] Training loss: 0.0197142
DEBUG:root:[ Iteration 345 ] Training loss: 0.0164029
DEBUG:root:[ Iteration 348 ] Training loss: 0.0183768
DEBUG:root:[ Iteration 351 ] Training loss: 0.01591
DEBUG:root:[ Iteration 354 ] Training loss: 0.0148489
DEBUG:root:[ Iteration 357 ] Training loss: 0.0177453
DEBUG:root:[ Iteration 360 ] Training loss: 0.0139206
DEBUG:root:[ Iteration 360 ] Test loss: 0.0167973
DEBUG:root:[ Iteration 363 ] Training loss: 0.0186794
DEBUG:root:[ Iteration 366 ] Training loss: 0.0172682
DEBUG:root:[ Iteration 369 ] Training loss: 0.0164573
DEBUG:root:[ Iteration 372 ] Training loss: 0.0134619
DEBUG:root:[ Iteration 375 ] Training loss: 0.0169838
DEBUG:root:[ Iteration 378 ] Training loss: 0.0180106
DEBUG:root:[ Iteration 380 ] Test loss: 0.0140127
DEBUG:root:[ Iteration 381 ] Training loss: 0.0156413
DEBUG:root:[ Iteration 384 ] Training loss: 0.024451
DEBUG:root:[ Iteration 387 ] Training loss: 0.0196588
DEBUG:root:[ Iteration 390 ] Training loss: 0.0147408
DEBUG:root:[ Iteration 393 ] Training loss: 0.0139811
DEBUG:root:[ Iteration 396 ] Training loss: 0.0174578
DEBUG:root:[ Iteration 399 ] Training loss: 0.0169021
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_04-08-2016_14h46m42s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.147377
DEBUG:root:[ Iteration 0 ] Test loss: 0.146047
DEBUG:root:[ Iteration 3 ] Training loss: 0.143295
DEBUG:root:[ Iteration 6 ] Training loss: 0.133209
DEBUG:root:[ Iteration 9 ] Training loss: 0.086913
DEBUG:root:[ Iteration 12 ] Training loss: 0.0533629
DEBUG:root:[ Iteration 15 ] Training loss: 0.0473263
DEBUG:root:[ Iteration 18 ] Training loss: 0.0473409
DEBUG:root:[ Iteration 20 ] Test loss: 0.0430995
DEBUG:root:[ Iteration 21 ] Training loss: 0.0484994
DEBUG:root:[ Iteration 24 ] Training loss: 0.046925
DEBUG:root:[ Iteration 27 ] Training loss: 0.0464094
DEBUG:root:[ Iteration 30 ] Training loss: 0.0401679
DEBUG:root:[ Iteration 33 ] Training loss: 0.0552283
DEBUG:root:[ Iteration 36 ] Training loss: 0.0330744
DEBUG:root:[ Iteration 39 ] Training loss: 0.047395
DEBUG:root:[ Iteration 40 ] Test loss: 0.0384535
DEBUG:root:[ Iteration 42 ] Training loss: 0.0377831
DEBUG:root:[ Iteration 45 ] Training loss: 0.0444841
DEBUG:root:[ Iteration 48 ] Training loss: 0.0448752
DEBUG:root:[ Iteration 51 ] Training loss: 0.0359206
DEBUG:root:[ Iteration 54 ] Training loss: 0.0355481
DEBUG:root:[ Iteration 57 ] Training loss: 0.0415371
DEBUG:root:[ Iteration 60 ] Training loss: 0.0339235
DEBUG:root:[ Iteration 60 ] Test loss: 0.0311401
DEBUG:root:[ Iteration 63 ] Training loss: 0.0376329
DEBUG:root:[ Iteration 66 ] Training loss: 0.0389543
DEBUG:root:[ Iteration 69 ] Training loss: 0.0388846
DEBUG:root:[ Iteration 72 ] Training loss: 0.0349778
DEBUG:root:[ Iteration 75 ] Training loss: 0.0316165
DEBUG:root:[ Iteration 78 ] Training loss: 0.029001
DEBUG:root:[ Iteration 80 ] Test loss: 0.0324599
DEBUG:root:[ Iteration 81 ] Training loss: 0.0286507
DEBUG:root:[ Iteration 84 ] Training loss: 0.0327043
DEBUG:root:[ Iteration 87 ] Training loss: 0.0323734
DEBUG:root:[ Iteration 90 ] Training loss: 0.0342051
DEBUG:root:[ Iteration 93 ] Training loss: 0.0393072
DEBUG:root:[ Iteration 96 ] Training loss: 0.0339895
DEBUG:root:[ Iteration 99 ] Training loss: 0.0374036
DEBUG:root:[ Iteration 100 ] Test loss: 0.0388759
DEBUG:root:[ Iteration 102 ] Training loss: 0.0409743
DEBUG:root:[ Iteration 105 ] Training loss: 0.0322371
DEBUG:root:[ Iteration 108 ] Training loss: 0.0317036
DEBUG:root:[ Iteration 111 ] Training loss: 0.0304492
DEBUG:root:[ Iteration 114 ] Training loss: 0.032906
DEBUG:root:[ Iteration 117 ] Training loss: 0.0291972
DEBUG:root:[ Iteration 120 ] Training loss: 0.0268668
DEBUG:root:[ Iteration 120 ] Test loss: 0.0304663
DEBUG:root:[ Iteration 123 ] Training loss: 0.0325389
DEBUG:root:[ Iteration 126 ] Training loss: 0.0276403
DEBUG:root:[ Iteration 129 ] Training loss: 0.0297525
DEBUG:root:[ Iteration 132 ] Training loss: 0.0282639
DEBUG:root:[ Iteration 135 ] Training loss: 0.0280766
DEBUG:root:[ Iteration 138 ] Training loss: 0.0279037
DEBUG:root:[ Iteration 140 ] Test loss: 0.0281085
DEBUG:root:[ Iteration 141 ] Training loss: 0.0288209
DEBUG:root:[ Iteration 144 ] Training loss: 0.0285555
DEBUG:root:[ Iteration 147 ] Training loss: 0.0265973
DEBUG:root:[ Iteration 150 ] Training loss: 0.0287565
DEBUG:root:[ Iteration 153 ] Training loss: 0.0267253
DEBUG:root:[ Iteration 156 ] Training loss: 0.0262718
DEBUG:root:[ Iteration 159 ] Training loss: 0.0290307
DEBUG:root:[ Iteration 160 ] Test loss: 0.0307641
DEBUG:root:[ Iteration 162 ] Training loss: 0.0267444
DEBUG:root:[ Iteration 165 ] Training loss: 0.0282254
DEBUG:root:[ Iteration 168 ] Training loss: 0.0234711
DEBUG:root:[ Iteration 171 ] Training loss: 0.0219229
DEBUG:root:[ Iteration 174 ] Training loss: 0.021384
DEBUG:root:[ Iteration 177 ] Training loss: 0.0259773
DEBUG:root:[ Iteration 180 ] Training loss: 0.0242858
DEBUG:root:[ Iteration 180 ] Test loss: 0.0223786
DEBUG:root:[ Iteration 183 ] Training loss: 0.0270952
DEBUG:root:[ Iteration 186 ] Training loss: 0.0235009
DEBUG:root:[ Iteration 189 ] Training loss: 0.0200183
DEBUG:root:[ Iteration 192 ] Training loss: 0.0215226
DEBUG:root:[ Iteration 195 ] Training loss: 0.024164
DEBUG:root:[ Iteration 198 ] Training loss: 0.0205656
DEBUG:root:[ Iteration 200 ] Test loss: 0.0233073
DEBUG:root:[ Iteration 201 ] Training loss: 0.0277802
DEBUG:root:[ Iteration 204 ] Training loss: 0.0222994
DEBUG:root:[ Iteration 207 ] Training loss: 0.0251975
DEBUG:root:[ Iteration 210 ] Training loss: 0.0212646
DEBUG:root:[ Iteration 213 ] Training loss: 0.0251259
DEBUG:root:[ Iteration 216 ] Training loss: 0.0182599
DEBUG:root:[ Iteration 219 ] Training loss: 0.0204968
DEBUG:root:[ Iteration 220 ] Test loss: 0.0174166
DEBUG:root:[ Iteration 222 ] Training loss: 0.0234192
DEBUG:root:[ Iteration 225 ] Training loss: 0.0259519
DEBUG:root:[ Iteration 228 ] Training loss: 0.0207638
DEBUG:root:[ Iteration 231 ] Training loss: 0.021667
DEBUG:root:[ Iteration 234 ] Training loss: 0.0228574
DEBUG:root:[ Iteration 237 ] Training loss: 0.0204874
DEBUG:root:[ Iteration 240 ] Training loss: 0.0208886
DEBUG:root:[ Iteration 240 ] Test loss: 0.017218
DEBUG:root:[ Iteration 243 ] Training loss: 0.0166856
DEBUG:root:[ Iteration 246 ] Training loss: 0.0191269
DEBUG:root:[ Iteration 249 ] Training loss: 0.017453
DEBUG:root:[ Iteration 252 ] Training loss: 0.0204793
DEBUG:root:[ Iteration 255 ] Training loss: 0.0219036
DEBUG:root:[ Iteration 258 ] Training loss: 0.021677
DEBUG:root:[ Iteration 260 ] Test loss: 0.0190803
DEBUG:root:[ Iteration 261 ] Training loss: 0.0167393
DEBUG:root:[ Iteration 264 ] Training loss: 0.0180019
DEBUG:root:[ Iteration 267 ] Training loss: 0.018842
DEBUG:root:[ Iteration 270 ] Training loss: 0.0163768
DEBUG:root:[ Iteration 273 ] Training loss: 0.0177432
DEBUG:root:[ Iteration 276 ] Training loss: 0.0189071
DEBUG:root:[ Iteration 279 ] Training loss: 0.0183471
DEBUG:root:[ Iteration 280 ] Test loss: 0.0207776
DEBUG:root:[ Iteration 282 ] Training loss: 0.0201076
DEBUG:root:[ Iteration 285 ] Training loss: 0.0174688
DEBUG:root:[ Iteration 288 ] Training loss: 0.0212671
DEBUG:root:[ Iteration 291 ] Training loss: 0.017832
DEBUG:root:[ Iteration 294 ] Training loss: 0.0172115
DEBUG:root:[ Iteration 297 ] Training loss: 0.0183773
DEBUG:root:[ Iteration 300 ] Training loss: 0.0150391
DEBUG:root:[ Iteration 300 ] Test loss: 0.013557
DEBUG:root:[ Iteration 303 ] Training loss: 0.0179074
DEBUG:root:[ Iteration 306 ] Training loss: 0.016602
DEBUG:root:[ Iteration 309 ] Training loss: 0.0174847
DEBUG:root:[ Iteration 312 ] Training loss: 0.0167525
DEBUG:root:[ Iteration 315 ] Training loss: 0.0164408
DEBUG:root:[ Iteration 318 ] Training loss: 0.0185126
DEBUG:root:[ Iteration 320 ] Test loss: 0.0139978
DEBUG:root:[ Iteration 321 ] Training loss: 0.0167689
DEBUG:root:[ Iteration 324 ] Training loss: 0.0135585
DEBUG:root:[ Iteration 327 ] Training loss: 0.0170206
DEBUG:root:[ Iteration 330 ] Training loss: 0.0171429
DEBUG:root:[ Iteration 333 ] Training loss: 0.014926
DEBUG:root:[ Iteration 336 ] Training loss: 0.0128185
DEBUG:root:[ Iteration 339 ] Training loss: 0.0156684
DEBUG:root:[ Iteration 340 ] Test loss: 0.0148341
DEBUG:root:[ Iteration 342 ] Training loss: 0.0143644
DEBUG:root:[ Iteration 345 ] Training loss: 0.0164857
DEBUG:root:[ Iteration 348 ] Training loss: 0.0117257
DEBUG:root:[ Iteration 351 ] Training loss: 0.0126191
DEBUG:root:[ Iteration 354 ] Training loss: 0.0173858
DEBUG:root:[ Iteration 357 ] Training loss: 0.0132302
DEBUG:root:[ Iteration 360 ] Training loss: 0.0122716
DEBUG:root:[ Iteration 360 ] Test loss: 0.0149289
DEBUG:root:[ Iteration 363 ] Training loss: 0.0141302
DEBUG:root:[ Iteration 366 ] Training loss: 0.0120511
DEBUG:root:[ Iteration 369 ] Training loss: 0.0127688
DEBUG:root:[ Iteration 372 ] Training loss: 0.011597
DEBUG:root:[ Iteration 375 ] Training loss: 0.013789
DEBUG:root:[ Iteration 378 ] Training loss: 0.0141656
DEBUG:root:[ Iteration 380 ] Test loss: 0.0129688
DEBUG:root:[ Iteration 381 ] Training loss: 0.0149138
DEBUG:root:[ Iteration 384 ] Training loss: 0.0127352
DEBUG:root:[ Iteration 387 ] Training loss: 0.0127427
DEBUG:root:[ Iteration 390 ] Training loss: 0.0117106
DEBUG:root:[ Iteration 393 ] Training loss: 0.0140624
DEBUG:root:[ Iteration 396 ] Training loss: 0.0150266
DEBUG:root:[ Iteration 399 ] Training loss: 0.0132026
DEBUG:root:[ Iteration 400 ] Test loss: 0.0104198
DEBUG:root:[ Iteration 402 ] Training loss: 0.00987748
DEBUG:root:[ Iteration 405 ] Training loss: 0.0108559
DEBUG:root:[ Iteration 408 ] Training loss: 0.0126576
DEBUG:root:[ Iteration 411 ] Training loss: 0.00978726
DEBUG:root:[ Iteration 414 ] Training loss: 0.011809
DEBUG:root:[ Iteration 417 ] Training loss: 0.0153671
DEBUG:root:[ Iteration 420 ] Training loss: 0.0147198
DEBUG:root:[ Iteration 420 ] Test loss: 0.0168146
DEBUG:root:[ Iteration 423 ] Training loss: 0.0182492
DEBUG:root:[ Iteration 426 ] Training loss: 0.0207476
DEBUG:root:[ Iteration 429 ] Training loss: 0.0166841
DEBUG:root:[ Iteration 432 ] Training loss: 0.0120101
DEBUG:root:[ Iteration 435 ] Training loss: 0.0149781
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_04-19-2016_11h27m55s.ckpt
DEBUG:root:Optimization done.
<<<<<<< HEAD
DEBUG:root:[ Iteration 0 ] Training loss: 0.17526
DEBUG:root:[ Iteration 0 ] Test loss: 0.168949
DEBUG:root:[ Iteration 3 ] Training loss: 0.166066
DEBUG:root:[ Iteration 6 ] Training loss: 0.163896
DEBUG:root:[ Iteration 9 ] Training loss: 0.159827
DEBUG:root:[ Iteration 12 ] Training loss: 0.151449
DEBUG:root:[ Iteration 15 ] Training loss: 0.135454
DEBUG:root:[ Iteration 18 ] Training loss: 0.122574
DEBUG:root:[ Iteration 20 ] Test loss: 0.101583
DEBUG:root:[ Iteration 21 ] Training loss: 0.0895894
DEBUG:root:[ Iteration 24 ] Training loss: 0.0608386
DEBUG:root:[ Iteration 27 ] Training loss: 0.0493191
DEBUG:root:[ Iteration 30 ] Training loss: 0.0450434
DEBUG:root:[ Iteration 33 ] Training loss: 0.0423959
DEBUG:root:[ Iteration 36 ] Training loss: 0.0414651
DEBUG:root:[ Iteration 39 ] Training loss: 0.0420264
DEBUG:root:[ Iteration 40 ] Test loss: 0.0435734
DEBUG:root:[ Iteration 42 ] Training loss: 0.0376668
DEBUG:root:[ Iteration 45 ] Training loss: 0.041871
DEBUG:root:[ Iteration 48 ] Training loss: 0.0340972
DEBUG:root:[ Iteration 51 ] Training loss: 0.0338166
DEBUG:root:[ Iteration 54 ] Training loss: 0.041709
DEBUG:root:[ Iteration 57 ] Training loss: 0.0346794
DEBUG:root:[ Iteration 60 ] Training loss: 0.0349698
DEBUG:root:[ Iteration 60 ] Test loss: 0.0389279
DEBUG:root:[ Iteration 63 ] Training loss: 0.0367406
DEBUG:root:[ Iteration 66 ] Training loss: 0.0309679
DEBUG:root:[ Iteration 69 ] Training loss: 0.02728
DEBUG:root:[ Iteration 72 ] Training loss: 0.0324386
DEBUG:root:[ Iteration 75 ] Training loss: 0.0235444
DEBUG:root:[ Iteration 78 ] Training loss: 0.0266365
DEBUG:root:[ Iteration 80 ] Test loss: 0.0253072
DEBUG:root:[ Iteration 81 ] Training loss: 0.0217114
DEBUG:root:[ Iteration 84 ] Training loss: 0.0234454
DEBUG:root:[ Iteration 87 ] Training loss: 0.0208763
DEBUG:root:[ Iteration 90 ] Training loss: 0.0239193
DEBUG:root:[ Iteration 93 ] Training loss: 0.0252391
DEBUG:root:[ Iteration 96 ] Training loss: 0.0217953
DEBUG:root:[ Iteration 99 ] Training loss: 0.0182485
DEBUG:root:[ Iteration 100 ] Test loss: 0.0191556
DEBUG:root:[ Iteration 102 ] Training loss: 0.0187816
DEBUG:root:[ Iteration 105 ] Training loss: 0.0208842
DEBUG:root:[ Iteration 108 ] Training loss: 0.0219828
DEBUG:root:[ Iteration 111 ] Training loss: 0.0158296
DEBUG:root:[ Iteration 114 ] Training loss: 0.0155531
DEBUG:root:[ Iteration 117 ] Training loss: 0.0187707
DEBUG:root:[ Iteration 120 ] Training loss: 0.0140355
DEBUG:root:[ Iteration 120 ] Test loss: 0.0191862
DEBUG:root:[ Iteration 123 ] Training loss: 0.0199312
DEBUG:root:[ Iteration 126 ] Training loss: 0.0158143
DEBUG:root:[ Iteration 129 ] Training loss: 0.016118
DEBUG:root:[ Iteration 132 ] Training loss: 0.0182552
DEBUG:root:[ Iteration 135 ] Training loss: 0.0170694
DEBUG:root:[ Iteration 138 ] Training loss: 0.0173291
DEBUG:root:[ Iteration 140 ] Test loss: 0.0178154
DEBUG:root:[ Iteration 141 ] Training loss: 0.0138302
DEBUG:root:[ Iteration 144 ] Training loss: 0.011915
DEBUG:root:[ Iteration 147 ] Training loss: 0.013185
DEBUG:root:[ Iteration 150 ] Training loss: 0.0115374
DEBUG:root:[ Iteration 153 ] Training loss: 0.0125261
DEBUG:root:[ Iteration 156 ] Training loss: 0.0148169
DEBUG:root:[ Iteration 159 ] Training loss: 0.0135945
DEBUG:root:[ Iteration 160 ] Test loss: 0.016606
DEBUG:root:[ Iteration 162 ] Training loss: 0.0111894
DEBUG:root:[ Iteration 165 ] Training loss: 0.0127557
DEBUG:root:[ Iteration 168 ] Training loss: 0.0130609
DEBUG:root:[ Iteration 171 ] Training loss: 0.0110386
DEBUG:root:[ Iteration 174 ] Training loss: 0.0117807
DEBUG:root:[ Iteration 177 ] Training loss: 0.0138365
DEBUG:root:[ Iteration 180 ] Training loss: 0.0130558
DEBUG:root:[ Iteration 180 ] Test loss: 0.0139762
DEBUG:root:[ Iteration 183 ] Training loss: 0.011073
DEBUG:root:[ Iteration 186 ] Training loss: 0.0120399
DEBUG:root:[ Iteration 189 ] Training loss: 0.0170037
DEBUG:root:[ Iteration 192 ] Training loss: 0.0116833
DEBUG:root:[ Iteration 195 ] Training loss: 0.0103145
DEBUG:root:[ Iteration 198 ] Training loss: 0.0105924
DEBUG:root:[ Iteration 200 ] Test loss: 0.0121327
DEBUG:root:[ Iteration 201 ] Training loss: 0.00835213
DEBUG:root:[ Iteration 204 ] Training loss: 0.0136645
DEBUG:root:[ Iteration 207 ] Training loss: 0.0111851
DEBUG:root:[ Iteration 210 ] Training loss: 0.0172484
DEBUG:root:[ Iteration 213 ] Training loss: 0.0106113
DEBUG:root:[ Iteration 216 ] Training loss: 0.0101375
DEBUG:root:[ Iteration 219 ] Training loss: 0.011827
DEBUG:root:[ Iteration 220 ] Test loss: 0.0119255
DEBUG:root:[ Iteration 222 ] Training loss: 0.0107077
DEBUG:root:[ Iteration 225 ] Training loss: 0.0122732
DEBUG:root:[ Iteration 228 ] Training loss: 0.012422
DEBUG:root:[ Iteration 231 ] Training loss: 0.00892253
DEBUG:root:[ Iteration 234 ] Training loss: 0.014724
DEBUG:root:[ Iteration 237 ] Training loss: 0.00889565
DEBUG:root:[ Iteration 240 ] Training loss: 0.0116181
DEBUG:root:[ Iteration 240 ] Test loss: 0.00925873
DEBUG:root:[ Iteration 243 ] Training loss: 0.0120365
DEBUG:root:[ Iteration 246 ] Training loss: 0.0123169
DEBUG:root:[ Iteration 249 ] Training loss: 0.0102713
DEBUG:root:[ Iteration 252 ] Training loss: 0.0107227
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_19h29m06s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.17234
DEBUG:root:[ Iteration 0 ] Test loss: 0.171881
DEBUG:root:[ Iteration 3 ] Training loss: 0.170116
DEBUG:root:[ Iteration 6 ] Training loss: 0.157814
DEBUG:root:[ Iteration 9 ] Training loss: 0.12223
DEBUG:root:[ Iteration 12 ] Training loss: 0.0502634
DEBUG:root:[ Iteration 15 ] Training loss: 0.0479147
DEBUG:root:[ Iteration 18 ] Training loss: 0.0460249
DEBUG:root:[ Iteration 20 ] Test loss: 0.0448234
DEBUG:root:[ Iteration 21 ] Training loss: 0.0423563
DEBUG:root:[ Iteration 24 ] Training loss: 0.0463869
DEBUG:root:[ Iteration 27 ] Training loss: 0.0539404
DEBUG:root:[ Iteration 30 ] Training loss: 0.0506153
DEBUG:root:[ Iteration 33 ] Training loss: 0.0460177
DEBUG:root:[ Iteration 36 ] Training loss: 0.0428715
DEBUG:root:[ Iteration 39 ] Training loss: 0.0425224
DEBUG:root:[ Iteration 40 ] Test loss: 0.0474004
DEBUG:root:[ Iteration 42 ] Training loss: 0.0470348
DEBUG:root:[ Iteration 45 ] Training loss: 0.0450137
DEBUG:root:[ Iteration 48 ] Training loss: 0.048964
DEBUG:root:[ Iteration 51 ] Training loss: 0.0463898
DEBUG:root:[ Iteration 54 ] Training loss: 0.0422351
DEBUG:root:[ Iteration 57 ] Training loss: 0.0523925
DEBUG:root:[ Iteration 60 ] Training loss: 0.0605273
DEBUG:root:[ Iteration 60 ] Test loss: 0.0527604
DEBUG:root:[ Iteration 63 ] Training loss: 0.0673969
DEBUG:root:[ Iteration 66 ] Training loss: 0.0518377
DEBUG:root:[ Iteration 69 ] Training loss: 0.0592006
DEBUG:root:[ Iteration 72 ] Training loss: 0.049936
DEBUG:root:[ Iteration 75 ] Training loss: 0.0413341
DEBUG:root:[ Iteration 78 ] Training loss: 0.0460305
DEBUG:root:[ Iteration 80 ] Test loss: 0.0449786
DEBUG:root:[ Iteration 81 ] Training loss: 0.0446866
DEBUG:root:[ Iteration 84 ] Training loss: 0.0434541
DEBUG:root:[ Iteration 87 ] Training loss: 0.0431799
DEBUG:root:[ Iteration 90 ] Training loss: 0.0420307
DEBUG:root:[ Iteration 93 ] Training loss: 0.0461703
DEBUG:root:[ Iteration 96 ] Training loss: 0.0456599
DEBUG:root:[ Iteration 99 ] Training loss: 0.0407241
DEBUG:root:[ Iteration 100 ] Test loss: 0.0446869
DEBUG:root:[ Iteration 102 ] Training loss: 0.0429989
DEBUG:root:[ Iteration 105 ] Training loss: 0.0402592
DEBUG:root:[ Iteration 108 ] Training loss: 0.0433527
DEBUG:root:[ Iteration 111 ] Training loss: 0.0414547
DEBUG:root:[ Iteration 114 ] Training loss: 0.0406582
DEBUG:root:[ Iteration 117 ] Training loss: 0.0453184
DEBUG:root:[ Iteration 120 ] Training loss: 0.0441467
DEBUG:root:[ Iteration 120 ] Test loss: 0.0433627
DEBUG:root:[ Iteration 123 ] Training loss: 0.0429267
DEBUG:root:[ Iteration 126 ] Training loss: 0.0320402
DEBUG:root:[ Iteration 129 ] Training loss: 0.0348043
DEBUG:root:[ Iteration 132 ] Training loss: 0.0455878
DEBUG:root:[ Iteration 135 ] Training loss: 0.0407984
DEBUG:root:[ Iteration 138 ] Training loss: 0.0404305
DEBUG:root:[ Iteration 140 ] Test loss: 0.0428664
DEBUG:root:[ Iteration 141 ] Training loss: 0.0382785
DEBUG:root:[ Iteration 144 ] Training loss: 0.0372992
DEBUG:root:[ Iteration 147 ] Training loss: 0.0368233
DEBUG:root:[ Iteration 150 ] Training loss: 0.0325341
DEBUG:root:[ Iteration 153 ] Training loss: 0.0370848
DEBUG:root:[ Iteration 156 ] Training loss: 0.0353097
DEBUG:root:[ Iteration 159 ] Training loss: 0.0373522
DEBUG:root:[ Iteration 160 ] Test loss: 0.0427483
DEBUG:root:[ Iteration 162 ] Training loss: 0.0394887
DEBUG:root:[ Iteration 165 ] Training loss: 0.036702
DEBUG:root:[ Iteration 168 ] Training loss: 0.0343293
DEBUG:root:[ Iteration 171 ] Training loss: 0.0394087
DEBUG:root:[ Iteration 174 ] Training loss: 0.0304821
DEBUG:root:[ Iteration 177 ] Training loss: 0.0336519
DEBUG:root:[ Iteration 180 ] Training loss: 0.0332277
DEBUG:root:[ Iteration 180 ] Test loss: 0.0362122
DEBUG:root:[ Iteration 183 ] Training loss: 0.0330781
DEBUG:root:[ Iteration 186 ] Training loss: 0.0386691
DEBUG:root:[ Iteration 189 ] Training loss: 0.0333907
DEBUG:root:[ Iteration 192 ] Training loss: 0.0459196
DEBUG:root:[ Iteration 195 ] Training loss: 0.0446105
DEBUG:root:[ Iteration 198 ] Training loss: 0.0322901
DEBUG:root:[ Iteration 200 ] Test loss: 0.0391321
DEBUG:root:[ Iteration 201 ] Training loss: 0.0381122
DEBUG:root:[ Iteration 204 ] Training loss: 0.0305825
DEBUG:root:[ Iteration 207 ] Training loss: 0.0360797
DEBUG:root:[ Iteration 210 ] Training loss: 0.0302358
DEBUG:root:[ Iteration 213 ] Training loss: 0.029511
DEBUG:root:[ Iteration 216 ] Training loss: 0.0304802
DEBUG:root:[ Iteration 219 ] Training loss: 0.0337875
DEBUG:root:[ Iteration 220 ] Test loss: 0.0358054
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_19h38m56s.ckpt
=======
DEBUG:root:[ Iteration 0 ] Training loss: 0.140889
DEBUG:root:[ Iteration 0 ] Test loss: 0.148815
DEBUG:root:[ Iteration 3 ] Training loss: 0.136932
DEBUG:root:[ Iteration 6 ] Training loss: 0.134529
DEBUG:root:[ Iteration 9 ] Training loss: 0.138712
DEBUG:root:[ Iteration 12 ] Training loss: 0.139512
DEBUG:root:[ Iteration 15 ] Training loss: 0.136396
DEBUG:root:[ Iteration 18 ] Training loss: 0.127939
DEBUG:root:[ Iteration 20 ] Test loss: 0.138883
DEBUG:root:[ Iteration 21 ] Training loss: 0.126778
DEBUG:root:[ Iteration 24 ] Training loss: 0.126215
DEBUG:root:[ Iteration 27 ] Training loss: 0.135579
DEBUG:root:[ Iteration 30 ] Training loss: 0.1272
DEBUG:root:[ Iteration 33 ] Training loss: 0.117713
DEBUG:root:[ Iteration 36 ] Training loss: 0.117858
DEBUG:root:[ Iteration 39 ] Training loss: 0.121552
DEBUG:root:[ Iteration 40 ] Test loss: 0.123469
DEBUG:root:[ Iteration 42 ] Training loss: 0.110755
DEBUG:root:[ Iteration 45 ] Training loss: 0.104928
DEBUG:root:[ Iteration 48 ] Training loss: 0.100351
DEBUG:root:[ Iteration 51 ] Training loss: 0.0867321
DEBUG:root:[ Iteration 54 ] Training loss: 0.0929302
DEBUG:root:[ Iteration 57 ] Training loss: 0.0775955
DEBUG:root:[ Iteration 60 ] Training loss: 0.0697906
DEBUG:root:[ Iteration 60 ] Test loss: 0.0908191
DEBUG:root:[ Iteration 63 ] Training loss: 0.0666559
DEBUG:root:[ Iteration 66 ] Training loss: 0.0705796
DEBUG:root:[ Iteration 69 ] Training loss: 0.0626824
DEBUG:root:[ Iteration 72 ] Training loss: 0.0643912
DEBUG:root:[ Iteration 75 ] Training loss: 0.0701199
DEBUG:root:[ Iteration 78 ] Training loss: 0.0607343
DEBUG:root:[ Iteration 80 ] Test loss: 0.0731308
DEBUG:root:[ Iteration 81 ] Training loss: 0.0523219
DEBUG:root:[ Iteration 84 ] Training loss: 0.0513598
DEBUG:root:[ Iteration 87 ] Training loss: 0.045113
DEBUG:root:[ Iteration 90 ] Training loss: 0.0497469
DEBUG:root:[ Iteration 93 ] Training loss: 0.046069
DEBUG:root:[ Iteration 96 ] Training loss: 0.0416479
DEBUG:root:[ Iteration 99 ] Training loss: 0.0429927
DEBUG:root:[ Iteration 100 ] Test loss: 0.0521829
DEBUG:root:[ Iteration 102 ] Training loss: 0.0428414
DEBUG:root:[ Iteration 105 ] Training loss: 0.043768
DEBUG:root:[ Iteration 108 ] Training loss: 0.0371984
DEBUG:root:[ Iteration 111 ] Training loss: 0.0317612
DEBUG:root:[ Iteration 114 ] Training loss: 0.0380859
DEBUG:root:[ Iteration 117 ] Training loss: 0.0344711
DEBUG:root:[ Iteration 120 ] Training loss: 0.0271591
DEBUG:root:[ Iteration 120 ] Test loss: 0.039052
DEBUG:root:[ Iteration 123 ] Training loss: 0.0296273
DEBUG:root:[ Iteration 126 ] Training loss: 0.0294955
DEBUG:root:[ Iteration 129 ] Training loss: 0.0253326
DEBUG:root:[ Iteration 132 ] Training loss: 0.0315953
DEBUG:root:[ Iteration 135 ] Training loss: 0.0293209
DEBUG:root:[ Iteration 138 ] Training loss: 0.0226747
DEBUG:root:[ Iteration 140 ] Test loss: 0.0295475
DEBUG:root:[ Iteration 141 ] Training loss: 0.0284574
DEBUG:root:[ Iteration 144 ] Training loss: 0.0265758
DEBUG:root:[ Iteration 147 ] Training loss: 0.0264387
DEBUG:root:[ Iteration 150 ] Training loss: 0.0219241
DEBUG:root:[ Iteration 153 ] Training loss: 0.0230004
DEBUG:root:[ Iteration 156 ] Training loss: 0.0212308
DEBUG:root:[ Iteration 159 ] Training loss: 0.0278202
DEBUG:root:[ Iteration 160 ] Test loss: 0.0246697
DEBUG:root:[ Iteration 162 ] Training loss: 0.022021
DEBUG:root:[ Iteration 165 ] Training loss: 0.0211818
DEBUG:root:[ Iteration 168 ] Training loss: 0.024118
DEBUG:root:[ Iteration 171 ] Training loss: 0.0224921
DEBUG:root:[ Iteration 174 ] Training loss: 0.0177349
DEBUG:root:[ Iteration 177 ] Training loss: 0.0213176
DEBUG:root:[ Iteration 180 ] Training loss: 0.0179706
DEBUG:root:[ Iteration 180 ] Test loss: 0.02082
DEBUG:root:[ Iteration 183 ] Training loss: 0.0154516
DEBUG:root:[ Iteration 186 ] Training loss: 0.0168531
DEBUG:root:[ Iteration 189 ] Training loss: 0.0212087
DEBUG:root:[ Iteration 192 ] Training loss: 0.0163563
DEBUG:root:[ Iteration 195 ] Training loss: 0.0188839
DEBUG:root:[ Iteration 198 ] Training loss: 0.0175441
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_11h57m27s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0948347
DEBUG:root:[ Iteration 0 ] Test loss: 0.135698
DEBUG:root:[ Iteration 3 ] Training loss: 0.071163
DEBUG:root:[ Iteration 6 ] Training loss: 0.0510629
DEBUG:root:[ Iteration 9 ] Training loss: 0.0370937
DEBUG:root:[ Iteration 12 ] Training loss: 0.0366352
DEBUG:root:[ Iteration 15 ] Training loss: 0.0327682
DEBUG:root:[ Iteration 18 ] Training loss: 0.024256
DEBUG:root:[ Iteration 20 ] Test loss: 0.0298609
DEBUG:root:[ Iteration 21 ] Training loss: 0.0252704
DEBUG:root:[ Iteration 24 ] Training loss: 0.0230918
DEBUG:root:[ Iteration 27 ] Training loss: 0.0205494
DEBUG:root:[ Iteration 30 ] Training loss: 0.0193482
DEBUG:root:[ Iteration 33 ] Training loss: 0.017503
DEBUG:root:[ Iteration 36 ] Training loss: 0.0180342
DEBUG:root:[ Iteration 39 ] Training loss: 0.0185189
DEBUG:root:[ Iteration 40 ] Test loss: 0.0174109
DEBUG:root:[ Iteration 42 ] Training loss: 0.0146156
DEBUG:root:[ Iteration 45 ] Training loss: 0.01506
DEBUG:root:[ Iteration 48 ] Training loss: 0.0136726
DEBUG:root:[ Iteration 51 ] Training loss: 0.0127508
DEBUG:root:[ Iteration 54 ] Training loss: 0.0107435
DEBUG:root:[ Iteration 57 ] Training loss: 0.0139718
DEBUG:root:[ Iteration 60 ] Training loss: 0.011609
DEBUG:root:[ Iteration 60 ] Test loss: 0.0140107
DEBUG:root:[ Iteration 63 ] Training loss: 0.0106625
DEBUG:root:[ Iteration 66 ] Training loss: 0.0126398
DEBUG:root:[ Iteration 69 ] Training loss: 0.0119526
DEBUG:root:[ Iteration 72 ] Training loss: 0.0113539
DEBUG:root:[ Iteration 75 ] Training loss: 0.0104885
DEBUG:root:[ Iteration 78 ] Training loss: 0.0099041
DEBUG:root:[ Iteration 80 ] Test loss: 0.0120535
DEBUG:root:[ Iteration 81 ] Training loss: 0.0109959
DEBUG:root:[ Iteration 84 ] Training loss: 0.00939423
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_12h05m01s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0659558
DEBUG:root:[ Iteration 0 ] Test loss: 0.0637647
DEBUG:root:[ Iteration 3 ] Training loss: 0.0466873
DEBUG:root:[ Iteration 6 ] Training loss: 0.0482024
DEBUG:root:[ Iteration 9 ] Training loss: 0.0431288
DEBUG:root:[ Iteration 12 ] Training loss: 0.0248725
DEBUG:root:[ Iteration 15 ] Training loss: 0.0296837
DEBUG:root:[ Iteration 18 ] Training loss: 0.0366157
DEBUG:root:[ Iteration 20 ] Test loss: 0.0215255
DEBUG:root:[ Iteration 21 ] Training loss: 0.0293745
DEBUG:root:[ Iteration 24 ] Training loss: 0.0254976
DEBUG:root:[ Iteration 27 ] Training loss: 0.0190918
DEBUG:root:[ Iteration 30 ] Training loss: 0.0234058
DEBUG:root:[ Iteration 33 ] Training loss: 0.0223346
DEBUG:root:[ Iteration 36 ] Training loss: 0.0250513
DEBUG:root:[ Iteration 39 ] Training loss: 0.0154285
DEBUG:root:[ Iteration 40 ] Test loss: 0.0166624
DEBUG:root:[ Iteration 42 ] Training loss: 0.0126696
DEBUG:root:[ Iteration 45 ] Training loss: 0.0131559
DEBUG:root:[ Iteration 48 ] Training loss: 0.0182379
DEBUG:root:[ Iteration 51 ] Training loss: 0.0157296
DEBUG:root:[ Iteration 54 ] Training loss: 0.0164171
DEBUG:root:[ Iteration 57 ] Training loss: 0.0163812
DEBUG:root:[ Iteration 60 ] Training loss: 0.0142443
DEBUG:root:[ Iteration 60 ] Test loss: 0.0153145
DEBUG:root:[ Iteration 63 ] Training loss: 0.0144522
DEBUG:root:[ Iteration 66 ] Training loss: 0.0115321
DEBUG:root:[ Iteration 69 ] Training loss: 0.0124104
DEBUG:root:[ Iteration 72 ] Training loss: 0.0170619
DEBUG:root:[ Iteration 75 ] Training loss: 0.00899689
DEBUG:root:[ Iteration 78 ] Training loss: 0.00965733
DEBUG:root:[ Iteration 80 ] Test loss: 0.014398
DEBUG:root:[ Iteration 81 ] Training loss: 0.0124576
DEBUG:root:[ Iteration 84 ] Training loss: 0.0121863
DEBUG:root:[ Iteration 87 ] Training loss: 0.00787454
DEBUG:root:[ Iteration 90 ] Training loss: 0.012653
DEBUG:root:[ Iteration 93 ] Training loss: 0.00888207
DEBUG:root:[ Iteration 96 ] Training loss: 0.00721658
DEBUG:root:[ Iteration 99 ] Training loss: 0.00903758
DEBUG:root:[ Iteration 100 ] Test loss: 0.0119264
DEBUG:root:[ Iteration 102 ] Training loss: 0.0109904
DEBUG:root:[ Iteration 105 ] Training loss: 0.00769188
DEBUG:root:[ Iteration 108 ] Training loss: 0.0105556
DEBUG:root:[ Iteration 111 ] Training loss: 0.0111556
DEBUG:root:[ Iteration 114 ] Training loss: 0.00804335
DEBUG:root:[ Iteration 117 ] Training loss: 0.0123858
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_13h19m21s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.116038
DEBUG:root:[ Iteration 0 ] Test loss: 0.105694
DEBUG:root:[ Iteration 3 ] Training loss: 0.0692978
DEBUG:root:[ Iteration 6 ] Training loss: 0.0456391
DEBUG:root:[ Iteration 9 ] Training loss: 0.0519241
DEBUG:root:[ Iteration 12 ] Training loss: 0.0512931
DEBUG:root:[ Iteration 15 ] Training loss: 0.0534621
DEBUG:root:[ Iteration 18 ] Training loss: 0.0313811
DEBUG:root:[ Iteration 20 ] Test loss: 0.0399919
DEBUG:root:[ Iteration 21 ] Training loss: 0.0359819
DEBUG:root:[ Iteration 24 ] Training loss: 0.0405519
DEBUG:root:[ Iteration 27 ] Training loss: 0.0299366
DEBUG:root:[ Iteration 30 ] Training loss: 0.0226666
DEBUG:root:[ Iteration 33 ] Training loss: 0.0304915
DEBUG:root:[ Iteration 36 ] Training loss: 0.0219264
DEBUG:root:[ Iteration 39 ] Training loss: 0.0219363
DEBUG:root:[ Iteration 40 ] Test loss: 0.0319086
DEBUG:root:[ Iteration 42 ] Training loss: 0.0258836
DEBUG:root:[ Iteration 45 ] Training loss: 0.020301
DEBUG:root:[ Iteration 48 ] Training loss: 0.0208911
DEBUG:root:[ Iteration 51 ] Training loss: 0.017709
DEBUG:root:[ Iteration 54 ] Training loss: 0.0171806
DEBUG:root:[ Iteration 57 ] Training loss: 0.0243857
DEBUG:root:[ Iteration 60 ] Training loss: 0.0232713
DEBUG:root:[ Iteration 60 ] Test loss: 0.023228
DEBUG:root:[ Iteration 63 ] Training loss: 0.016725
DEBUG:root:[ Iteration 66 ] Training loss: 0.0249535
DEBUG:root:[ Iteration 69 ] Training loss: 0.0156339
DEBUG:root:[ Iteration 72 ] Training loss: 0.0179506
DEBUG:root:[ Iteration 75 ] Training loss: 0.0168519
DEBUG:root:[ Iteration 78 ] Training loss: 0.0215393
DEBUG:root:[ Iteration 80 ] Test loss: 0.0269642
DEBUG:root:[ Iteration 81 ] Training loss: 0.0183757
DEBUG:root:[ Iteration 84 ] Training loss: 0.0169018
DEBUG:root:[ Iteration 87 ] Training loss: 0.0106659
DEBUG:root:[ Iteration 90 ] Training loss: 0.0128376
DEBUG:root:[ Iteration 93 ] Training loss: 0.0136057
DEBUG:root:[ Iteration 96 ] Training loss: 0.0151933
DEBUG:root:[ Iteration 99 ] Training loss: 0.0174302
DEBUG:root:[ Iteration 100 ] Test loss: 0.0211913
DEBUG:root:[ Iteration 102 ] Training loss: 0.0113466
DEBUG:root:[ Iteration 105 ] Training loss: 0.014787
DEBUG:root:[ Iteration 108 ] Training loss: 0.0111856
DEBUG:root:[ Iteration 111 ] Training loss: 0.0124709
DEBUG:root:[ Iteration 114 ] Training loss: 0.0116387
DEBUG:root:[ Iteration 117 ] Training loss: 0.011415
DEBUG:root:[ Iteration 120 ] Training loss: 0.0100489
DEBUG:root:[ Iteration 120 ] Test loss: 0.0258572
DEBUG:root:[ Iteration 123 ] Training loss: 0.00917041
DEBUG:root:[ Iteration 126 ] Training loss: 0.00762512
DEBUG:root:[ Iteration 129 ] Training loss: 0.0152313
DEBUG:root:[ Iteration 132 ] Training loss: 0.0131369
DEBUG:root:[ Iteration 135 ] Training loss: 0.0126007
DEBUG:root:[ Iteration 138 ] Training loss: 0.00963093
DEBUG:root:[ Iteration 140 ] Test loss: 0.0170151
DEBUG:root:[ Iteration 141 ] Training loss: 0.00807502
DEBUG:root:[ Iteration 144 ] Training loss: 0.00981563
DEBUG:root:[ Iteration 147 ] Training loss: 0.010455
DEBUG:root:[ Iteration 150 ] Training loss: 0.0106479
DEBUG:root:[ Iteration 153 ] Training loss: 0.00966589
DEBUG:root:[ Iteration 156 ] Training loss: 0.011772
DEBUG:root:[ Iteration 159 ] Training loss: 0.0122792
DEBUG:root:[ Iteration 160 ] Test loss: 0.0162105
DEBUG:root:[ Iteration 162 ] Training loss: 0.00863705
DEBUG:root:[ Iteration 165 ] Training loss: 0.0134493
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_14h10m59s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.105568
DEBUG:root:[ Iteration 0 ] Test loss: 0.104936
DEBUG:root:[ Iteration 3 ] Training loss: 0.0776188
DEBUG:root:[ Iteration 6 ] Training loss: 0.0651905
DEBUG:root:[ Iteration 9 ] Training loss: 0.0497777
DEBUG:root:[ Iteration 12 ] Training loss: 0.059547
DEBUG:root:[ Iteration 15 ] Training loss: 0.049086
DEBUG:root:[ Iteration 18 ] Training loss: 0.0521047
DEBUG:root:[ Iteration 20 ] Test loss: 0.0421633
DEBUG:root:[ Iteration 21 ] Training loss: 0.0584943
DEBUG:root:[ Iteration 24 ] Training loss: 0.0409848
DEBUG:root:[ Iteration 27 ] Training loss: 0.0396611
DEBUG:root:[ Iteration 30 ] Training loss: 0.0447704
DEBUG:root:[ Iteration 33 ] Training loss: 0.0430272
DEBUG:root:[ Iteration 36 ] Training loss: 0.0299254
DEBUG:root:[ Iteration 39 ] Training loss: 0.0292383
DEBUG:root:[ Iteration 40 ] Test loss: 0.0300569
DEBUG:root:[ Iteration 42 ] Training loss: 0.0386773
DEBUG:root:[ Iteration 45 ] Training loss: 0.0283668
DEBUG:root:[ Iteration 48 ] Training loss: 0.0289405
DEBUG:root:[ Iteration 51 ] Training loss: 0.0247144
DEBUG:root:[ Iteration 54 ] Training loss: 0.0278307
DEBUG:root:[ Iteration 57 ] Training loss: 0.0211846
DEBUG:root:[ Iteration 60 ] Training loss: 0.0275969
DEBUG:root:[ Iteration 60 ] Test loss: 0.0292712
DEBUG:root:[ Iteration 63 ] Training loss: 0.0282015
DEBUG:root:[ Iteration 66 ] Training loss: 0.0221611
DEBUG:root:[ Iteration 69 ] Training loss: 0.0207883
DEBUG:root:[ Iteration 72 ] Training loss: 0.0191536
DEBUG:root:[ Iteration 75 ] Training loss: 0.0229348
DEBUG:root:[ Iteration 78 ] Training loss: 0.0256303
DEBUG:root:[ Iteration 80 ] Test loss: 0.0252377
DEBUG:root:[ Iteration 81 ] Training loss: 0.0238358
DEBUG:root:[ Iteration 84 ] Training loss: 0.0190076
DEBUG:root:[ Iteration 87 ] Training loss: 0.0209417
DEBUG:root:[ Iteration 90 ] Training loss: 0.0227293
DEBUG:root:[ Iteration 93 ] Training loss: 0.0257258
DEBUG:root:[ Iteration 96 ] Training loss: 0.0252874
DEBUG:root:[ Iteration 99 ] Training loss: 0.0229383
DEBUG:root:[ Iteration 100 ] Test loss: 0.0236673
DEBUG:root:[ Iteration 102 ] Training loss: 0.0248017
DEBUG:root:[ Iteration 105 ] Training loss: 0.0165006
DEBUG:root:[ Iteration 108 ] Training loss: 0.0155121
DEBUG:root:[ Iteration 111 ] Training loss: 0.0210247
DEBUG:root:[ Iteration 114 ] Training loss: 0.016969
DEBUG:root:[ Iteration 117 ] Training loss: 0.0186298
DEBUG:root:[ Iteration 120 ] Training loss: 0.0178274
DEBUG:root:[ Iteration 120 ] Test loss: 0.0166394
DEBUG:root:[ Iteration 123 ] Training loss: 0.0204611
DEBUG:root:[ Iteration 126 ] Training loss: 0.0155467
DEBUG:root:[ Iteration 129 ] Training loss: 0.0115906
DEBUG:root:[ Iteration 132 ] Training loss: 0.0198806
DEBUG:root:[ Iteration 135 ] Training loss: 0.0154516
DEBUG:root:[ Iteration 138 ] Training loss: 0.0172528
DEBUG:root:[ Iteration 140 ] Test loss: 0.0181022
DEBUG:root:[ Iteration 141 ] Training loss: 0.0123482
DEBUG:root:[ Iteration 144 ] Training loss: 0.0164675
DEBUG:root:[ Iteration 147 ] Training loss: 0.01412
DEBUG:root:[ Iteration 150 ] Training loss: 0.0223989
DEBUG:root:[ Iteration 153 ] Training loss: 0.0157325
DEBUG:root:[ Iteration 156 ] Training loss: 0.0132539
DEBUG:root:[ Iteration 159 ] Training loss: 0.0108527
DEBUG:root:[ Iteration 160 ] Test loss: 0.0189289
DEBUG:root:[ Iteration 162 ] Training loss: 0.0170559
DEBUG:root:[ Iteration 165 ] Training loss: 0.0108837
DEBUG:root:[ Iteration 168 ] Training loss: 0.0147961
DEBUG:root:[ Iteration 171 ] Training loss: 0.0166207
DEBUG:root:[ Iteration 174 ] Training loss: 0.0189536
DEBUG:root:[ Iteration 177 ] Training loss: 0.0128674
DEBUG:root:[ Iteration 180 ] Training loss: 0.0139459
DEBUG:root:[ Iteration 180 ] Test loss: 0.0152972
DEBUG:root:[ Iteration 183 ] Training loss: 0.0115061
DEBUG:root:[ Iteration 186 ] Training loss: 0.0206984
DEBUG:root:[ Iteration 189 ] Training loss: 0.010986
DEBUG:root:[ Iteration 192 ] Training loss: 0.0154905
DEBUG:root:[ Iteration 195 ] Training loss: 0.0164892
DEBUG:root:[ Iteration 198 ] Training loss: 0.016912
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_14h55m03s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.108027
DEBUG:root:[ Iteration 0 ] Test loss: 0.10509
DEBUG:root:[ Iteration 3 ] Training loss: 0.0647406
DEBUG:root:[ Iteration 6 ] Training loss: 0.0608484
DEBUG:root:[ Iteration 9 ] Training loss: 0.0642339
DEBUG:root:[ Iteration 12 ] Training loss: 0.0534225
DEBUG:root:[ Iteration 15 ] Training loss: 0.0588711
DEBUG:root:[ Iteration 18 ] Training loss: 0.0504682
DEBUG:root:[ Iteration 20 ] Test loss: 0.0625252
DEBUG:root:[ Iteration 21 ] Training loss: 0.043418
DEBUG:root:[ Iteration 24 ] Training loss: 0.0456408
DEBUG:root:[ Iteration 27 ] Training loss: 0.0410712
DEBUG:root:[ Iteration 30 ] Training loss: 0.0504527
DEBUG:root:[ Iteration 33 ] Training loss: 0.0259826
DEBUG:root:[ Iteration 36 ] Training loss: 0.0444938
DEBUG:root:[ Iteration 39 ] Training loss: 0.0518034
DEBUG:root:[ Iteration 40 ] Test loss: 0.0359459
DEBUG:root:[ Iteration 42 ] Training loss: 0.0423739
DEBUG:root:[ Iteration 45 ] Training loss: 0.0331518
DEBUG:root:[ Iteration 48 ] Training loss: 0.0276468
DEBUG:root:[ Iteration 51 ] Training loss: 0.0399068
DEBUG:root:[ Iteration 54 ] Training loss: 0.0476698
DEBUG:root:[ Iteration 57 ] Training loss: 0.0221885
DEBUG:root:[ Iteration 60 ] Training loss: 0.0239022
DEBUG:root:[ Iteration 60 ] Test loss: 0.0368343
DEBUG:root:[ Iteration 63 ] Training loss: 0.0275235
DEBUG:root:[ Iteration 66 ] Training loss: 0.036596
DEBUG:root:[ Iteration 69 ] Training loss: 0.0306573
DEBUG:root:[ Iteration 72 ] Training loss: 0.0314627
DEBUG:root:[ Iteration 75 ] Training loss: 0.0233895
DEBUG:root:[ Iteration 78 ] Training loss: 0.024313
DEBUG:root:[ Iteration 80 ] Test loss: 0.0317607
DEBUG:root:[ Iteration 81 ] Training loss: 0.0291329
DEBUG:root:[ Iteration 84 ] Training loss: 0.0191887
DEBUG:root:[ Iteration 87 ] Training loss: 0.0287305
DEBUG:root:[ Iteration 90 ] Training loss: 0.026731
DEBUG:root:[ Iteration 93 ] Training loss: 0.0273538
DEBUG:root:[ Iteration 96 ] Training loss: 0.0258729
DEBUG:root:[ Iteration 99 ] Training loss: 0.0187981
DEBUG:root:[ Iteration 100 ] Test loss: 0.0333455
DEBUG:root:[ Iteration 102 ] Training loss: 0.0196002
DEBUG:root:[ Iteration 105 ] Training loss: 0.024367
DEBUG:root:[ Iteration 108 ] Training loss: 0.0136254
DEBUG:root:[ Iteration 111 ] Training loss: 0.0226454
DEBUG:root:[ Iteration 114 ] Training loss: 0.0248271
DEBUG:root:[ Iteration 117 ] Training loss: 0.0230877
DEBUG:root:[ Iteration 120 ] Training loss: 0.0220881
DEBUG:root:[ Iteration 120 ] Test loss: 0.0327126
DEBUG:root:[ Iteration 123 ] Training loss: 0.0232283
DEBUG:root:[ Iteration 126 ] Training loss: 0.0191517
DEBUG:root:[ Iteration 129 ] Training loss: 0.0188754
DEBUG:root:[ Iteration 132 ] Training loss: 0.0233889
DEBUG:root:[ Iteration 135 ] Training loss: 0.0186011
DEBUG:root:[ Iteration 138 ] Training loss: 0.0181069
DEBUG:root:[ Iteration 140 ] Test loss: 0.0177141
DEBUG:root:[ Iteration 141 ] Training loss: 0.0162376
DEBUG:root:[ Iteration 144 ] Training loss: 0.0179319
DEBUG:root:[ Iteration 147 ] Training loss: 0.0131945
DEBUG:root:[ Iteration 150 ] Training loss: 0.0141864
DEBUG:root:[ Iteration 153 ] Training loss: 0.0206902
DEBUG:root:[ Iteration 156 ] Training loss: 0.0166238
DEBUG:root:[ Iteration 159 ] Training loss: 0.0141061
DEBUG:root:[ Iteration 160 ] Test loss: 0.0240105
DEBUG:root:[ Iteration 162 ] Training loss: 0.0166744
DEBUG:root:[ Iteration 165 ] Training loss: 0.018133
DEBUG:root:[ Iteration 168 ] Training loss: 0.020875
DEBUG:root:[ Iteration 171 ] Training loss: 0.0214927
DEBUG:root:[ Iteration 174 ] Training loss: 0.0133421
DEBUG:root:[ Iteration 177 ] Training loss: 0.00941272
DEBUG:root:[ Iteration 180 ] Training loss: 0.0172374
DEBUG:root:[ Iteration 180 ] Test loss: 0.0203607
DEBUG:root:[ Iteration 183 ] Training loss: 0.0136415
DEBUG:root:[ Iteration 186 ] Training loss: 0.0185148
DEBUG:root:[ Iteration 189 ] Training loss: 0.0107837
DEBUG:root:[ Iteration 192 ] Training loss: 0.0168721
DEBUG:root:[ Iteration 195 ] Training loss: 0.0150773
DEBUG:root:[ Iteration 198 ] Training loss: 0.0177815
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_15h33m28s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0376712
DEBUG:root:[ Iteration 0 ] Test loss: 0.0370502
DEBUG:root:[ Iteration 3 ] Training loss: 0.0356516
DEBUG:root:[ Iteration 6 ] Training loss: 0.0338472
DEBUG:root:[ Iteration 9 ] Training loss: 0.0366591
DEBUG:root:[ Iteration 12 ] Training loss: 0.0250531
DEBUG:root:[ Iteration 15 ] Training loss: 0.0186285
DEBUG:root:[ Iteration 18 ] Training loss: 0.0225331
DEBUG:root:[ Iteration 20 ] Test loss: 0.0198433
DEBUG:root:[ Iteration 21 ] Training loss: 0.0215994
DEBUG:root:[ Iteration 24 ] Training loss: 0.0202981
DEBUG:root:[ Iteration 27 ] Training loss: 0.0170149
DEBUG:root:[ Iteration 30 ] Training loss: 0.0197147
DEBUG:root:[ Iteration 33 ] Training loss: 0.0207256
DEBUG:root:[ Iteration 36 ] Training loss: 0.0144921
DEBUG:root:[ Iteration 39 ] Training loss: 0.0180212
DEBUG:root:[ Iteration 40 ] Test loss: 0.0203975
DEBUG:root:[ Iteration 42 ] Training loss: 0.016618
DEBUG:root:[ Iteration 45 ] Training loss: 0.0167335
DEBUG:root:[ Iteration 48 ] Training loss: 0.0131061
DEBUG:root:[ Iteration 51 ] Training loss: 0.0176368
DEBUG:root:[ Iteration 54 ] Training loss: 0.0224743
DEBUG:root:[ Iteration 57 ] Training loss: 0.0142342
DEBUG:root:[ Iteration 60 ] Training loss: 0.0162611
DEBUG:root:[ Iteration 60 ] Test loss: 0.0163087
DEBUG:root:[ Iteration 63 ] Training loss: 0.0137835
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_15h59m29s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.034389
DEBUG:root:[ Iteration 0 ] Test loss: 0.0302168
DEBUG:root:[ Iteration 3 ] Training loss: 0.0138487
DEBUG:root:[ Iteration 6 ] Training loss: 0.0113197
DEBUG:root:[ Iteration 9 ] Training loss: 0.00857791
DEBUG:root:[ Iteration 12 ] Training loss: 0.00886274
DEBUG:root:[ Iteration 15 ] Training loss: 0.00688141
DEBUG:root:[ Iteration 18 ] Training loss: 0.0072213
DEBUG:root:[ Iteration 20 ] Test loss: 0.00938767
DEBUG:root:[ Iteration 21 ] Training loss: 0.00786514
DEBUG:root:[ Iteration 24 ] Training loss: 0.00790941
DEBUG:root:[ Iteration 27 ] Training loss: 0.00734649
DEBUG:root:[ Iteration 30 ] Training loss: 0.0110546
DEBUG:root:[ Iteration 33 ] Training loss: 0.00531401
DEBUG:root:[ Iteration 36 ] Training loss: 0.00613344
DEBUG:root:[ Iteration 39 ] Training loss: 0.00889608
DEBUG:root:[ Iteration 40 ] Test loss: 0.00786445
DEBUG:root:[ Iteration 42 ] Training loss: 0.00496276
DEBUG:root:[ Iteration 45 ] Training loss: 0.00433869
DEBUG:root:[ Iteration 48 ] Training loss: 0.00701535
DEBUG:root:[ Iteration 51 ] Training loss: 0.0037363
DEBUG:root:[ Iteration 54 ] Training loss: 0.00709618
DEBUG:root:[ Iteration 57 ] Training loss: 0.00736029
DEBUG:root:[ Iteration 60 ] Training loss: 0.00701864
DEBUG:root:[ Iteration 60 ] Test loss: 0.00751358
DEBUG:root:[ Iteration 63 ] Training loss: 0.00393876
DEBUG:root:[ Iteration 66 ] Training loss: 0.00432771
DEBUG:root:[ Iteration 69 ] Training loss: 0.00646892
DEBUG:root:[ Iteration 72 ] Training loss: 0.00797166
DEBUG:root:[ Iteration 75 ] Training loss: 0.0067124
DEBUG:root:[ Iteration 78 ] Training loss: 0.00433549
DEBUG:root:[ Iteration 80 ] Test loss: 0.006013
DEBUG:root:[ Iteration 81 ] Training loss: 0.00687779
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_16h10m11s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.461345
DEBUG:root:[ Iteration 0 ] Test loss: 0.460326
DEBUG:root:[ Iteration 3 ] Training loss: 0.104678
DEBUG:root:[ Iteration 6 ] Training loss: 0.141254
DEBUG:root:[ Iteration 9 ] Training loss: 0.368815
DEBUG:root:[ Iteration 12 ] Training loss: 0.399438
DEBUG:root:[ Iteration 15 ] Training loss: 0.400189
DEBUG:root:[ Iteration 18 ] Training loss: 0.38571
DEBUG:root:[ Iteration 20 ] Test loss: 0.298713
DEBUG:root:[ Iteration 21 ] Training loss: 0.362284
DEBUG:root:[ Iteration 24 ] Training loss: 0.295749
DEBUG:root:[ Iteration 27 ] Training loss: 0.127209
DEBUG:root:[ Iteration 30 ] Training loss: 0.122368
DEBUG:root:[ Iteration 33 ] Training loss: 0.0860827
DEBUG:root:[ Iteration 36 ] Training loss: 0.0440586
DEBUG:root:[ Iteration 39 ] Training loss: 0.0646971
DEBUG:root:[ Iteration 40 ] Test loss: 0.0634028
DEBUG:root:[ Iteration 42 ] Training loss: 0.0494458
DEBUG:root:[ Iteration 45 ] Training loss: 0.046336
DEBUG:root:[ Iteration 48 ] Training loss: 0.0519103
DEBUG:root:[ Iteration 51 ] Training loss: 0.0453088
DEBUG:root:[ Iteration 54 ] Training loss: 0.040143
DEBUG:root:[ Iteration 57 ] Training loss: 0.0449793
DEBUG:root:[ Iteration 60 ] Training loss: 0.0373218
DEBUG:root:[ Iteration 60 ] Test loss: 0.0381106
DEBUG:root:[ Iteration 63 ] Training loss: 0.0354797
DEBUG:root:[ Iteration 66 ] Training loss: 0.0399012
DEBUG:root:[ Iteration 69 ] Training loss: 0.0324408
DEBUG:root:[ Iteration 72 ] Training loss: 0.0350885
DEBUG:root:[ Iteration 75 ] Training loss: 0.0293488
DEBUG:root:[ Iteration 78 ] Training loss: 0.0356757
DEBUG:root:[ Iteration 80 ] Test loss: 0.0301171
DEBUG:root:[ Iteration 81 ] Training loss: 0.0330263
DEBUG:root:[ Iteration 84 ] Training loss: 0.0283796
DEBUG:root:[ Iteration 87 ] Training loss: 0.0265465
DEBUG:root:[ Iteration 90 ] Training loss: 0.0249491
DEBUG:root:[ Iteration 93 ] Training loss: 0.0239943
DEBUG:root:[ Iteration 96 ] Training loss: 0.0256924
DEBUG:root:[ Iteration 99 ] Training loss: 0.0216903
DEBUG:root:[ Iteration 100 ] Test loss: 0.0251323
DEBUG:root:[ Iteration 102 ] Training loss: 0.0220423
DEBUG:root:[ Iteration 105 ] Training loss: 0.0226218
DEBUG:root:[ Iteration 108 ] Training loss: 0.0186735
DEBUG:root:[ Iteration 111 ] Training loss: 0.0208403
DEBUG:root:[ Iteration 114 ] Training loss: 0.0189608
DEBUG:root:[ Iteration 117 ] Training loss: 0.019536
DEBUG:root:[ Iteration 120 ] Training loss: 0.0158599
DEBUG:root:[ Iteration 120 ] Test loss: 0.0187443
DEBUG:root:[ Iteration 123 ] Training loss: 0.0215151
DEBUG:root:[ Iteration 126 ] Training loss: 0.0173654
DEBUG:root:[ Iteration 129 ] Training loss: 0.0164352
DEBUG:root:[ Iteration 132 ] Training loss: 0.0145013
DEBUG:root:[ Iteration 135 ] Training loss: 0.0204938
DEBUG:root:[ Iteration 138 ] Training loss: 0.0180356
DEBUG:root:[ Iteration 140 ] Test loss: 0.0179572
DEBUG:root:[ Iteration 141 ] Training loss: 0.0155164
DEBUG:root:[ Iteration 144 ] Training loss: 0.0164096
DEBUG:root:[ Iteration 147 ] Training loss: 0.0151054
DEBUG:root:[ Iteration 150 ] Training loss: 0.0190139
DEBUG:root:[ Iteration 153 ] Training loss: 0.0174109
DEBUG:root:[ Iteration 156 ] Training loss: 0.0132425
DEBUG:root:[ Iteration 159 ] Training loss: 0.014105
DEBUG:root:[ Iteration 160 ] Test loss: 0.018994
DEBUG:root:[ Iteration 162 ] Training loss: 0.016923
DEBUG:root:[ Iteration 165 ] Training loss: 0.0144328
DEBUG:root:[ Iteration 168 ] Training loss: 0.0124019
DEBUG:root:[ Iteration 171 ] Training loss: 0.0159589
DEBUG:root:[ Iteration 174 ] Training loss: 0.0122277
DEBUG:root:[ Iteration 177 ] Training loss: 0.0146795
DEBUG:root:[ Iteration 180 ] Training loss: 0.0131314
DEBUG:root:[ Iteration 180 ] Test loss: 0.0161786
DEBUG:root:[ Iteration 183 ] Training loss: 0.0135517
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_17h33m51s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.172613
DEBUG:root:[ Iteration 0 ] Test loss: 0.165173
DEBUG:root:[ Iteration 3 ] Training loss: 0.163664
DEBUG:root:[ Iteration 6 ] Training loss: 0.152291
DEBUG:root:[ Iteration 9 ] Training loss: 0.102729
DEBUG:root:[ Iteration 12 ] Training loss: 0.0470364
DEBUG:root:[ Iteration 15 ] Training loss: 0.0475904
DEBUG:root:[ Iteration 18 ] Training loss: 0.0444981
DEBUG:root:[ Iteration 20 ] Test loss: 0.0473364
DEBUG:root:[ Iteration 21 ] Training loss: 0.0429063
DEBUG:root:[ Iteration 24 ] Training loss: 0.0419786
DEBUG:root:[ Iteration 27 ] Training loss: 0.0532836
DEBUG:root:[ Iteration 30 ] Training loss: 0.0529293
DEBUG:root:[ Iteration 33 ] Training loss: 0.0557198
DEBUG:root:[ Iteration 36 ] Training loss: 0.0558129
DEBUG:root:[ Iteration 39 ] Training loss: 0.0445003
DEBUG:root:[ Iteration 40 ] Test loss: 0.0418308
DEBUG:root:[ Iteration 42 ] Training loss: 0.0508186
DEBUG:root:[ Iteration 45 ] Training loss: 0.0606823
DEBUG:root:[ Iteration 48 ] Training loss: 0.0548284
DEBUG:root:[ Iteration 51 ] Training loss: 0.0459345
DEBUG:root:[ Iteration 54 ] Training loss: 0.0448512
DEBUG:root:[ Iteration 57 ] Training loss: 0.0453422
DEBUG:root:[ Iteration 60 ] Training loss: 0.0404478
DEBUG:root:[ Iteration 60 ] Test loss: 0.0448174
DEBUG:root:[ Iteration 63 ] Training loss: 0.0517125
DEBUG:root:[ Iteration 66 ] Training loss: 0.0515553
DEBUG:root:[ Iteration 69 ] Training loss: 0.0390563
DEBUG:root:[ Iteration 72 ] Training loss: 0.0387906
DEBUG:root:[ Iteration 75 ] Training loss: 0.0410618
DEBUG:root:[ Iteration 78 ] Training loss: 0.0444826
DEBUG:root:[ Iteration 80 ] Test loss: 0.0424785
DEBUG:root:[ Iteration 81 ] Training loss: 0.0449966
DEBUG:root:[ Iteration 84 ] Training loss: 0.0380938
DEBUG:root:[ Iteration 87 ] Training loss: 0.0409474
DEBUG:root:[ Iteration 90 ] Training loss: 0.0388994
DEBUG:root:[ Iteration 93 ] Training loss: 0.0388853
DEBUG:root:[ Iteration 96 ] Training loss: 0.0409522
DEBUG:root:[ Iteration 99 ] Training loss: 0.0468332
DEBUG:root:[ Iteration 100 ] Test loss: 0.0373862
DEBUG:root:[ Iteration 102 ] Training loss: 0.0387085
DEBUG:root:[ Iteration 105 ] Training loss: 0.0385554
DEBUG:root:[ Iteration 108 ] Training loss: 0.0417507
DEBUG:root:[ Iteration 111 ] Training loss: 0.0377688
DEBUG:root:[ Iteration 114 ] Training loss: 0.0417427
DEBUG:root:[ Iteration 117 ] Training loss: 0.0371396
DEBUG:root:[ Iteration 120 ] Training loss: 0.0360298
DEBUG:root:[ Iteration 120 ] Test loss: 0.0398025
DEBUG:root:[ Iteration 123 ] Training loss: 0.0355506
DEBUG:root:[ Iteration 126 ] Training loss: 0.0420716
DEBUG:root:[ Iteration 129 ] Training loss: 0.0369382
DEBUG:root:[ Iteration 132 ] Training loss: 0.0392902
DEBUG:root:[ Iteration 135 ] Training loss: 0.0387111
DEBUG:root:[ Iteration 138 ] Training loss: 0.0374393
DEBUG:root:[ Iteration 140 ] Test loss: 0.0371005
DEBUG:root:[ Iteration 141 ] Training loss: 0.0354399
DEBUG:root:[ Iteration 144 ] Training loss: 0.0358288
DEBUG:root:[ Iteration 147 ] Training loss: 0.0373808
DEBUG:root:[ Iteration 150 ] Training loss: 0.0361012
DEBUG:root:[ Iteration 153 ] Training loss: 0.038618
DEBUG:root:[ Iteration 156 ] Training loss: 0.0319592
DEBUG:root:[ Iteration 159 ] Training loss: 0.035512
DEBUG:root:[ Iteration 160 ] Test loss: 0.0362742
DEBUG:root:[ Iteration 162 ] Training loss: 0.0381099
DEBUG:root:[ Iteration 165 ] Training loss: 0.0322379
DEBUG:root:[ Iteration 168 ] Training loss: 0.0430231
DEBUG:root:[ Iteration 171 ] Training loss: 0.0470224
DEBUG:root:[ Iteration 174 ] Training loss: 0.0410369
DEBUG:root:[ Iteration 177 ] Training loss: 0.0350229
DEBUG:root:[ Iteration 180 ] Training loss: 0.0414887
DEBUG:root:[ Iteration 180 ] Test loss: 0.0330991
DEBUG:root:[ Iteration 183 ] Training loss: 0.0456817
DEBUG:root:[ Iteration 186 ] Training loss: 0.041676
DEBUG:root:[ Iteration 189 ] Training loss: 0.035728
DEBUG:root:[ Iteration 192 ] Training loss: 0.0307679
DEBUG:root:[ Iteration 195 ] Training loss: 0.0369673
DEBUG:root:[ Iteration 198 ] Training loss: 0.0336495
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_17h56m50s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.170705
DEBUG:root:[ Iteration 0 ] Test loss: 0.167647
DEBUG:root:[ Iteration 3 ] Training loss: 0.159272
DEBUG:root:[ Iteration 6 ] Training loss: 0.130282
DEBUG:root:[ Iteration 9 ] Training loss: 0.0629723
DEBUG:root:[ Iteration 12 ] Training loss: 0.0458376
DEBUG:root:[ Iteration 15 ] Training loss: 0.0510392
DEBUG:root:[ Iteration 18 ] Training loss: 0.0496005
DEBUG:root:[ Iteration 20 ] Test loss: 0.0422239
DEBUG:root:[ Iteration 21 ] Training loss: 0.0434417
DEBUG:root:[ Iteration 24 ] Training loss: 0.0478977
DEBUG:root:[ Iteration 27 ] Training loss: 0.0440624
DEBUG:root:[ Iteration 30 ] Training loss: 0.0501308
DEBUG:root:[ Iteration 33 ] Training loss: 0.0824765
DEBUG:root:[ Iteration 36 ] Training loss: 0.0524932
DEBUG:root:[ Iteration 39 ] Training loss: 0.0643532
DEBUG:root:[ Iteration 40 ] Test loss: 0.0484347
DEBUG:root:[ Iteration 42 ] Training loss: 0.0497141
DEBUG:root:[ Iteration 45 ] Training loss: 0.0587397
DEBUG:root:[ Iteration 48 ] Training loss: 0.0443839
DEBUG:root:[ Iteration 51 ] Training loss: 0.0491441
DEBUG:root:[ Iteration 54 ] Training loss: 0.0463456
DEBUG:root:[ Iteration 57 ] Training loss: 0.0545436
DEBUG:root:[ Iteration 60 ] Training loss: 0.0416741
DEBUG:root:[ Iteration 60 ] Test loss: 0.050351
DEBUG:root:[ Iteration 63 ] Training loss: 0.0431109
DEBUG:root:[ Iteration 66 ] Training loss: 0.0427068
DEBUG:root:[ Iteration 69 ] Training loss: 0.0417639
DEBUG:root:[ Iteration 72 ] Training loss: 0.0461535
DEBUG:root:[ Iteration 75 ] Training loss: 0.0424846
DEBUG:root:[ Iteration 78 ] Training loss: 0.0426306
DEBUG:root:[ Iteration 80 ] Test loss: 0.042103
DEBUG:root:[ Iteration 81 ] Training loss: 0.0393897
DEBUG:root:[ Iteration 84 ] Training loss: 0.047984
DEBUG:root:[ Iteration 87 ] Training loss: 0.046785
DEBUG:root:[ Iteration 90 ] Training loss: 0.0459312
DEBUG:root:[ Iteration 93 ] Training loss: 0.0438977
DEBUG:root:[ Iteration 96 ] Training loss: 0.0407793
DEBUG:root:[ Iteration 99 ] Training loss: 0.0421558
DEBUG:root:[ Iteration 100 ] Test loss: 0.0418537
DEBUG:root:[ Iteration 102 ] Training loss: 0.0429134
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_18h03m39s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.172351
DEBUG:root:[ Iteration 0 ] Test loss: 0.170976
DEBUG:root:[ Iteration 3 ] Training loss: 0.168782
DEBUG:root:[ Iteration 6 ] Training loss: 0.166516
DEBUG:root:[ Iteration 9 ] Training loss: 0.167324
DEBUG:root:[ Iteration 12 ] Training loss: 0.159543
DEBUG:root:[ Iteration 15 ] Training loss: 0.155854
DEBUG:root:[ Iteration 18 ] Training loss: 0.152806
DEBUG:root:[ Iteration 20 ] Test loss: 0.144996
DEBUG:root:[ Iteration 21 ] Training loss: 0.144814
DEBUG:root:[ Iteration 24 ] Training loss: 0.145023
DEBUG:root:[ Iteration 27 ] Training loss: 0.131494
DEBUG:root:[ Iteration 30 ] Training loss: 0.132011
DEBUG:root:[ Iteration 33 ] Training loss: 0.129929
DEBUG:root:[ Iteration 36 ] Training loss: 0.119351
DEBUG:root:[ Iteration 39 ] Training loss: 0.112601
DEBUG:root:[ Iteration 40 ] Test loss: 0.103924
DEBUG:root:[ Iteration 42 ] Training loss: 0.0897423
DEBUG:root:[ Iteration 45 ] Training loss: 0.0747887
DEBUG:root:[ Iteration 48 ] Training loss: 0.0606811
DEBUG:root:[ Iteration 51 ] Training loss: 0.0478024
DEBUG:root:[ Iteration 54 ] Training loss: 0.0414729
DEBUG:root:[ Iteration 57 ] Training loss: 0.0383798
DEBUG:root:[ Iteration 60 ] Training loss: 0.0399097
DEBUG:root:[ Iteration 60 ] Test loss: 0.042327
DEBUG:root:[ Iteration 63 ] Training loss: 0.0455741
DEBUG:root:[ Iteration 66 ] Training loss: 0.0464571
DEBUG:root:[ Iteration 69 ] Training loss: 0.0339154
DEBUG:root:[ Iteration 72 ] Training loss: 0.0390449
DEBUG:root:[ Iteration 75 ] Training loss: 0.0407999
DEBUG:root:[ Iteration 78 ] Training loss: 0.0347465
DEBUG:root:[ Iteration 80 ] Test loss: 0.0383016
DEBUG:root:[ Iteration 81 ] Training loss: 0.0380864
DEBUG:root:[ Iteration 84 ] Training loss: 0.0317046
DEBUG:root:[ Iteration 87 ] Training loss: 0.0341327
DEBUG:root:[ Iteration 90 ] Training loss: 0.0343155
DEBUG:root:[ Iteration 93 ] Training loss: 0.0250603
DEBUG:root:[ Iteration 96 ] Training loss: 0.0339552
DEBUG:root:[ Iteration 99 ] Training loss: 0.0333432
DEBUG:root:[ Iteration 100 ] Test loss: 0.028398
DEBUG:root:[ Iteration 102 ] Training loss: 0.0218474
DEBUG:root:[ Iteration 105 ] Training loss: 0.0238569
DEBUG:root:[ Iteration 108 ] Training loss: 0.0295879
DEBUG:root:[ Iteration 111 ] Training loss: 0.0281432
DEBUG:root:[ Iteration 114 ] Training loss: 0.0262206
DEBUG:root:[ Iteration 117 ] Training loss: 0.0222782
DEBUG:root:[ Iteration 120 ] Training loss: 0.0224732
DEBUG:root:[ Iteration 120 ] Test loss: 0.0221573
DEBUG:root:[ Iteration 123 ] Training loss: 0.0215601
DEBUG:root:[ Iteration 126 ] Training loss: 0.0218792
DEBUG:root:[ Iteration 129 ] Training loss: 0.0187468
DEBUG:root:[ Iteration 132 ] Training loss: 0.0159429
DEBUG:root:[ Iteration 135 ] Training loss: 0.0151487
DEBUG:root:[ Iteration 138 ] Training loss: 0.0188721
DEBUG:root:[ Iteration 140 ] Test loss: 0.0217244
DEBUG:root:[ Iteration 141 ] Training loss: 0.01469
DEBUG:root:[ Iteration 144 ] Training loss: 0.0163493
DEBUG:root:[ Iteration 147 ] Training loss: 0.0197859
DEBUG:root:[ Iteration 150 ] Training loss: 0.0164392
DEBUG:root:[ Iteration 153 ] Training loss: 0.0152898
DEBUG:root:[ Iteration 156 ] Training loss: 0.0132664
DEBUG:root:[ Iteration 159 ] Training loss: 0.0155424
DEBUG:root:[ Iteration 160 ] Test loss: 0.0190574
DEBUG:root:[ Iteration 162 ] Training loss: 0.0168128
DEBUG:root:[ Iteration 165 ] Training loss: 0.0167632
DEBUG:root:[ Iteration 168 ] Training loss: 0.0128439
DEBUG:root:[ Iteration 171 ] Training loss: 0.0128831
DEBUG:root:[ Iteration 174 ] Training loss: 0.0170644
DEBUG:root:[ Iteration 177 ] Training loss: 0.0155544
DEBUG:root:[ Iteration 180 ] Training loss: 0.0182259
DEBUG:root:[ Iteration 180 ] Test loss: 0.0129652
DEBUG:root:[ Iteration 183 ] Training loss: 0.0158128
DEBUG:root:[ Iteration 186 ] Training loss: 0.0167283
DEBUG:root:[ Iteration 189 ] Training loss: 0.0143207
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_18h15m48s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.160282
DEBUG:root:[ Iteration 0 ] Test loss: 0.164996
DEBUG:root:[ Iteration 3 ] Training loss: 0.15972
DEBUG:root:[ Iteration 6 ] Training loss: 0.157359
DEBUG:root:[ Iteration 9 ] Training loss: 0.158262
DEBUG:root:[ Iteration 12 ] Training loss: 0.158374
DEBUG:root:[ Iteration 15 ] Training loss: 0.147461
DEBUG:root:[ Iteration 18 ] Training loss: 0.147915
DEBUG:root:[ Iteration 20 ] Test loss: 0.136694
DEBUG:root:[ Iteration 21 ] Training loss: 0.134584
DEBUG:root:[ Iteration 24 ] Training loss: 0.126337
DEBUG:root:[ Iteration 27 ] Training loss: 0.108802
DEBUG:root:[ Iteration 30 ] Training loss: 0.0879817
DEBUG:root:[ Iteration 33 ] Training loss: 0.0664695
DEBUG:root:[ Iteration 36 ] Training loss: 0.0456492
DEBUG:root:[ Iteration 39 ] Training loss: 0.0460881
DEBUG:root:[ Iteration 40 ] Test loss: 0.0428647
DEBUG:root:[ Iteration 42 ] Training loss: 0.0394845
DEBUG:root:[ Iteration 45 ] Training loss: 0.0444296
DEBUG:root:[ Iteration 48 ] Training loss: 0.0395458
DEBUG:root:[ Iteration 51 ] Training loss: 0.0439892
DEBUG:root:[ Iteration 54 ] Training loss: 0.0416551
DEBUG:root:[ Iteration 57 ] Training loss: 0.0343697
DEBUG:root:[ Iteration 60 ] Training loss: 0.0343877
DEBUG:root:[ Iteration 60 ] Test loss: 0.0389073
DEBUG:root:[ Iteration 63 ] Training loss: 0.0373257
DEBUG:root:[ Iteration 66 ] Training loss: 0.0307533
DEBUG:root:[ Iteration 69 ] Training loss: 0.0304548
DEBUG:root:[ Iteration 72 ] Training loss: 0.0341644
DEBUG:root:[ Iteration 75 ] Training loss: 0.0304572
DEBUG:root:[ Iteration 78 ] Training loss: 0.0256891
DEBUG:root:[ Iteration 80 ] Test loss: 0.0273111
DEBUG:root:[ Iteration 81 ] Training loss: 0.0285
DEBUG:root:[ Iteration 84 ] Training loss: 0.0229929
DEBUG:root:[ Iteration 87 ] Training loss: 0.0260094
DEBUG:root:[ Iteration 90 ] Training loss: 0.0267979
DEBUG:root:[ Iteration 93 ] Training loss: 0.0219334
DEBUG:root:[ Iteration 96 ] Training loss: 0.0223876
DEBUG:root:[ Iteration 99 ] Training loss: 0.0220248
DEBUG:root:[ Iteration 100 ] Test loss: 0.0216912
DEBUG:root:[ Iteration 102 ] Training loss: 0.0190881
DEBUG:root:[ Iteration 105 ] Training loss: 0.0170476
DEBUG:root:[ Iteration 108 ] Training loss: 0.0166831
DEBUG:root:[ Iteration 111 ] Training loss: 0.0148611
DEBUG:root:[ Iteration 114 ] Training loss: 0.0185271
DEBUG:root:[ Iteration 117 ] Training loss: 0.017761
DEBUG:root:[ Iteration 120 ] Training loss: 0.0183504
DEBUG:root:[ Iteration 120 ] Test loss: 0.0165383
DEBUG:root:[ Iteration 123 ] Training loss: 0.0216987
DEBUG:root:[ Iteration 126 ] Training loss: 0.015053
DEBUG:root:[ Iteration 129 ] Training loss: 0.0164259
DEBUG:root:[ Iteration 132 ] Training loss: 0.0160995
DEBUG:root:[ Iteration 135 ] Training loss: 0.0176847
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_18h53m51s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.167648
DEBUG:root:[ Iteration 0 ] Test loss: 0.164946
DEBUG:root:[ Iteration 3 ] Training loss: 0.16849
DEBUG:root:[ Iteration 6 ] Training loss: 0.154607
DEBUG:root:[ Iteration 9 ] Training loss: 0.093274
DEBUG:root:[ Iteration 12 ] Training loss: 0.0455956
DEBUG:root:[ Iteration 15 ] Training loss: 0.0521717
DEBUG:root:[ Iteration 18 ] Training loss: 0.0415848
DEBUG:root:[ Iteration 20 ] Test loss: 0.0482733
DEBUG:root:[ Iteration 21 ] Training loss: 0.0451568
DEBUG:root:[ Iteration 24 ] Training loss: 0.047395
DEBUG:root:[ Iteration 27 ] Training loss: 0.040591
DEBUG:root:[ Iteration 30 ] Training loss: 0.0496752
DEBUG:root:[ Iteration 33 ] Training loss: 0.0452488
DEBUG:root:[ Iteration 36 ] Training loss: 0.0431703
DEBUG:root:[ Iteration 39 ] Training loss: 0.0680544
DEBUG:root:[ Iteration 40 ] Test loss: 0.0475598
DEBUG:root:[ Iteration 42 ] Training loss: 0.0502386
DEBUG:root:[ Iteration 45 ] Training loss: 0.0555608
DEBUG:root:[ Iteration 48 ] Training loss: 0.0511438
DEBUG:root:[ Iteration 51 ] Training loss: 0.0393601
DEBUG:root:[ Iteration 54 ] Training loss: 0.0401697
DEBUG:root:[ Iteration 57 ] Training loss: 0.044743
DEBUG:root:[ Iteration 60 ] Training loss: 0.0424715
DEBUG:root:[ Iteration 60 ] Test loss: 0.0422914
DEBUG:root:[ Iteration 63 ] Training loss: 0.0422588
DEBUG:root:[ Iteration 66 ] Training loss: 0.0420849
DEBUG:root:[ Iteration 69 ] Training loss: 0.046122
DEBUG:root:[ Iteration 72 ] Training loss: 0.0355878
DEBUG:root:[ Iteration 75 ] Training loss: 0.0430735
DEBUG:root:[ Iteration 78 ] Training loss: 0.0481179
DEBUG:root:[ Iteration 80 ] Test loss: 0.0427098
DEBUG:root:[ Iteration 81 ] Training loss: 0.0399286
DEBUG:root:[ Iteration 84 ] Training loss: 0.0393744
DEBUG:root:[ Iteration 87 ] Training loss: 0.0383961
DEBUG:root:[ Iteration 90 ] Training loss: 0.0403765
DEBUG:root:[ Iteration 93 ] Training loss: 0.0456296
DEBUG:root:[ Iteration 96 ] Training loss: 0.042773
DEBUG:root:[ Iteration 99 ] Training loss: 0.0429618
DEBUG:root:[ Iteration 100 ] Test loss: 0.0415965
DEBUG:root:[ Iteration 102 ] Training loss: 0.0370667
DEBUG:root:[ Iteration 105 ] Training loss: 0.04062
DEBUG:root:[ Iteration 108 ] Training loss: 0.0386772
DEBUG:root:[ Iteration 111 ] Training loss: 0.0348
DEBUG:root:[ Iteration 114 ] Training loss: 0.0399073
DEBUG:root:[ Iteration 117 ] Training loss: 0.042122
DEBUG:root:[ Iteration 120 ] Training loss: 0.0361096
DEBUG:root:[ Iteration 120 ] Test loss: 0.0379257
DEBUG:root:[ Iteration 123 ] Training loss: 0.0387301
DEBUG:root:[ Iteration 126 ] Training loss: 0.0443447
DEBUG:root:[ Iteration 129 ] Training loss: 0.0317597
DEBUG:root:[ Iteration 132 ] Training loss: 0.0432541
DEBUG:root:[ Iteration 135 ] Training loss: 0.0631449
DEBUG:root:[ Iteration 138 ] Training loss: 0.0480463
DEBUG:root:[ Iteration 140 ] Test loss: 0.0394371
DEBUG:root:[ Iteration 141 ] Training loss: 0.0526431
DEBUG:root:[ Iteration 144 ] Training loss: 0.0481229
DEBUG:root:[ Iteration 147 ] Training loss: 0.0394594
DEBUG:root:[ Iteration 150 ] Training loss: 0.0430373
DEBUG:root:[ Iteration 153 ] Training loss: 0.0415118
DEBUG:root:[ Iteration 156 ] Training loss: 0.0365838
DEBUG:root:[ Iteration 159 ] Training loss: 0.0366075
DEBUG:root:[ Iteration 160 ] Test loss: 0.0350473
DEBUG:root:[ Iteration 162 ] Training loss: 0.0385954
DEBUG:root:[ Iteration 165 ] Training loss: 0.0348267
DEBUG:root:[ Iteration 168 ] Training loss: 0.0300507
DEBUG:root:[ Iteration 171 ] Training loss: 0.0327151
DEBUG:root:[ Iteration 174 ] Training loss: 0.0423007
DEBUG:root:[ Iteration 177 ] Training loss: 0.035925
DEBUG:root:[ Iteration 180 ] Training loss: 0.0351782
DEBUG:root:[ Iteration 180 ] Test loss: 0.0367176
DEBUG:root:[ Iteration 183 ] Training loss: 0.0380993
DEBUG:root:[ Iteration 186 ] Training loss: 0.0318221
DEBUG:root:[ Iteration 189 ] Training loss: 0.0296706
DEBUG:root:[ Iteration 192 ] Training loss: 0.0312108
DEBUG:root:[ Iteration 195 ] Training loss: 0.0343786
DEBUG:root:[ Iteration 198 ] Training loss: 0.0307132
DEBUG:root:[ Iteration 200 ] Test loss: 0.0329653
DEBUG:root:[ Iteration 201 ] Training loss: 0.0317763
DEBUG:root:[ Iteration 204 ] Training loss: 0.0316398
DEBUG:root:[ Iteration 207 ] Training loss: 0.0330976
DEBUG:root:[ Iteration 210 ] Training loss: 0.0292508
DEBUG:root:[ Iteration 213 ] Training loss: 0.0365912
DEBUG:root:[ Iteration 216 ] Training loss: 0.037516
DEBUG:root:[ Iteration 219 ] Training loss: 0.0370952
DEBUG:root:[ Iteration 220 ] Test loss: 0.0277932
DEBUG:root:[ Iteration 222 ] Training loss: 0.0435607
DEBUG:root:[ Iteration 225 ] Training loss: 0.0344842
DEBUG:root:[ Iteration 228 ] Training loss: 0.0362886
DEBUG:root:[ Iteration 231 ] Training loss: 0.0348588
DEBUG:root:[ Iteration 234 ] Training loss: 0.0284442
DEBUG:root:[ Iteration 237 ] Training loss: 0.0341124
DEBUG:root:[ Iteration 240 ] Training loss: 0.0412894
DEBUG:root:[ Iteration 240 ] Test loss: 0.0423171
DEBUG:root:[ Iteration 243 ] Training loss: 0.0260844
DEBUG:root:[ Iteration 246 ] Training loss: 0.0384991
DEBUG:root:[ Iteration 249 ] Training loss: 0.0328251
DEBUG:root:[ Iteration 252 ] Training loss: 0.0337838
DEBUG:root:[ Iteration 255 ] Training loss: 0.0240647
DEBUG:root:[ Iteration 258 ] Training loss: 0.0308119
DEBUG:root:[ Iteration 260 ] Test loss: 0.0269711
DEBUG:root:[ Iteration 261 ] Training loss: 0.0277713
DEBUG:root:[ Iteration 264 ] Training loss: 0.025095
DEBUG:root:[ Iteration 267 ] Training loss: 0.0303311
DEBUG:root:[ Iteration 270 ] Training loss: 0.0301751
DEBUG:root:[ Iteration 273 ] Training loss: 0.0259538
DEBUG:root:[ Iteration 276 ] Training loss: 0.0342916
DEBUG:root:[ Iteration 279 ] Training loss: 0.0230486
DEBUG:root:[ Iteration 280 ] Test loss: 0.023352
DEBUG:root:[ Iteration 282 ] Training loss: 0.0312322
DEBUG:root:[ Iteration 285 ] Training loss: 0.0228742
DEBUG:root:[ Iteration 288 ] Training loss: 0.0286887
DEBUG:root:[ Iteration 291 ] Training loss: 0.0229413
DEBUG:root:[ Iteration 294 ] Training loss: 0.0280159
DEBUG:root:[ Iteration 297 ] Training loss: 0.0365981
DEBUG:root:[ Iteration 300 ] Training loss: 0.0304142
DEBUG:root:[ Iteration 300 ] Test loss: 0.039339
DEBUG:root:[ Iteration 303 ] Training loss: 0.0329046
DEBUG:root:[ Iteration 306 ] Training loss: 0.0272852
DEBUG:root:[ Iteration 309 ] Training loss: 0.0228407
DEBUG:root:[ Iteration 312 ] Training loss: 0.0268991
DEBUG:root:[ Iteration 315 ] Training loss: 0.0280458
DEBUG:root:[ Iteration 318 ] Training loss: 0.0262898
DEBUG:root:[ Iteration 320 ] Test loss: 0.0260641
DEBUG:root:[ Iteration 321 ] Training loss: 0.032914
DEBUG:root:[ Iteration 324 ] Training loss: 0.0289411
DEBUG:root:[ Iteration 327 ] Training loss: 0.0282464
DEBUG:root:[ Iteration 330 ] Training loss: 0.0230329
DEBUG:root:[ Iteration 333 ] Training loss: 0.0264529
DEBUG:root:[ Iteration 336 ] Training loss: 0.0261766
DEBUG:root:[ Iteration 339 ] Training loss: 0.0233262
DEBUG:root:[ Iteration 340 ] Test loss: 0.0272594
DEBUG:root:[ Iteration 342 ] Training loss: 0.0206788
DEBUG:root:[ Iteration 345 ] Training loss: 0.0241821
DEBUG:root:[ Iteration 348 ] Training loss: 0.0192926
DEBUG:root:[ Iteration 351 ] Training loss: 0.01849
DEBUG:root:[ Iteration 354 ] Training loss: 0.0229918
DEBUG:root:[ Iteration 357 ] Training loss: 0.0192451
DEBUG:root:[ Iteration 360 ] Training loss: 0.0249605
DEBUG:root:[ Iteration 360 ] Test loss: 0.0298366
DEBUG:root:[ Iteration 363 ] Training loss: 0.03039
DEBUG:root:[ Iteration 366 ] Training loss: 0.0367214
DEBUG:root:[ Iteration 369 ] Training loss: 0.0234194
DEBUG:root:[ Iteration 372 ] Training loss: 0.0236267
DEBUG:root:[ Iteration 375 ] Training loss: 0.0247084
DEBUG:root:[ Iteration 378 ] Training loss: 0.0252754
DEBUG:root:[ Iteration 380 ] Test loss: 0.0265053
DEBUG:root:[ Iteration 381 ] Training loss: 0.0198509
DEBUG:root:[ Iteration 384 ] Training loss: 0.0269531
DEBUG:root:[ Iteration 387 ] Training loss: 0.0181784
DEBUG:root:[ Iteration 390 ] Training loss: 0.0183629
DEBUG:root:[ Iteration 393 ] Training loss: 0.0176325
DEBUG:root:[ Iteration 396 ] Training loss: 0.01876
DEBUG:root:[ Iteration 399 ] Training loss: 0.022547
DEBUG:root:[ Iteration 400 ] Test loss: 0.0192505
DEBUG:root:[ Iteration 402 ] Training loss: 0.0228565
DEBUG:root:[ Iteration 405 ] Training loss: 0.0205906
DEBUG:root:[ Iteration 408 ] Training loss: 0.0206547
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-04-2016_19h10m03s.ckpt
>>>>>>> 0daad527908f81485d6324e240f2f43e3f4b5b0f
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 5.56426
DEBUG:root:[ Iteration 0 ] Test loss: 5.56372
DEBUG:root:[ Iteration 3 ] Training loss: 5.52074
DEBUG:root:[ Iteration 6 ] Training loss: 5.35969
DEBUG:root:[ Iteration 9 ] Training loss: 4.17103
DEBUG:root:[ Iteration 12 ] Training loss: 3.48429
DEBUG:root:[ Iteration 15 ] Training loss: 2.96729
DEBUG:root:[ Iteration 18 ] Training loss: 2.61104
DEBUG:root:[ Iteration 20 ] Test loss: 2.68551
DEBUG:root:[ Iteration 21 ] Training loss: 2.59386
DEBUG:root:[ Iteration 24 ] Training loss: 2.42387
DEBUG:root:[ Iteration 27 ] Training loss: 2.31728
DEBUG:root:[ Iteration 30 ] Training loss: 2.32348
DEBUG:root:[ Iteration 33 ] Training loss: 2.23314
DEBUG:root:[ Iteration 36 ] Training loss: 2.08951
DEBUG:root:[ Iteration 39 ] Training loss: 2.09287
DEBUG:root:[ Iteration 40 ] Test loss: 2.11894
DEBUG:root:[ Iteration 42 ] Training loss: 1.87674
DEBUG:root:[ Iteration 45 ] Training loss: 2.10885
DEBUG:root:[ Iteration 48 ] Training loss: 1.85778
DEBUG:root:[ Iteration 51 ] Training loss: 1.92117
DEBUG:root:[ Iteration 54 ] Training loss: 1.75392
DEBUG:root:[ Iteration 57 ] Training loss: 1.8681
DEBUG:root:[ Iteration 60 ] Training loss: 1.80266
DEBUG:root:[ Iteration 60 ] Test loss: 1.8876
DEBUG:root:[ Iteration 63 ] Training loss: 1.84291
DEBUG:root:[ Iteration 66 ] Training loss: 1.65437
DEBUG:root:[ Iteration 69 ] Training loss: 1.5883
DEBUG:root:[ Iteration 72 ] Training loss: 1.73504
DEBUG:root:[ Iteration 75 ] Training loss: 1.5615
DEBUG:root:[ Iteration 78 ] Training loss: 1.80934
DEBUG:root:[ Iteration 80 ] Test loss: 1.73799
DEBUG:root:[ Iteration 81 ] Training loss: 1.8481
DEBUG:root:[ Iteration 84 ] Training loss: 1.77009
DEBUG:root:[ Iteration 87 ] Training loss: 1.61897
DEBUG:root:[ Iteration 90 ] Training loss: 1.55281
DEBUG:root:[ Iteration 93 ] Training loss: 1.49536
DEBUG:root:[ Iteration 96 ] Training loss: 1.59104
DEBUG:root:[ Iteration 99 ] Training loss: 1.42988
DEBUG:root:[ Iteration 100 ] Test loss: 1.86902
DEBUG:root:[ Iteration 102 ] Training loss: 1.70574
DEBUG:root:[ Iteration 105 ] Training loss: 1.53358
DEBUG:root:[ Iteration 108 ] Training loss: 1.50939
DEBUG:root:[ Iteration 111 ] Training loss: 1.4987
DEBUG:root:[ Iteration 114 ] Training loss: 1.58061
DEBUG:root:[ Iteration 117 ] Training loss: 1.59885
DEBUG:root:[ Iteration 120 ] Training loss: 1.5729
DEBUG:root:[ Iteration 120 ] Test loss: 1.71327
DEBUG:root:[ Iteration 123 ] Training loss: 1.57971
DEBUG:root:[ Iteration 126 ] Training loss: 1.55986
DEBUG:root:[ Iteration 129 ] Training loss: 1.47832
DEBUG:root:[ Iteration 132 ] Training loss: 1.51725
DEBUG:root:[ Iteration 135 ] Training loss: 1.51001
DEBUG:root:[ Iteration 138 ] Training loss: 1.48497
DEBUG:root:[ Iteration 140 ] Test loss: 1.58567
DEBUG:root:[ Iteration 141 ] Training loss: 1.52205
DEBUG:root:[ Iteration 144 ] Training loss: 1.47459
DEBUG:root:[ Iteration 147 ] Training loss: 1.4586
DEBUG:root:[ Iteration 150 ] Training loss: 1.41551
DEBUG:root:[ Iteration 153 ] Training loss: 1.36226
DEBUG:root:[ Iteration 156 ] Training loss: 1.39602
DEBUG:root:[ Iteration 159 ] Training loss: 1.4217
DEBUG:root:[ Iteration 160 ] Test loss: 1.55382
DEBUG:root:[ Iteration 162 ] Training loss: 1.38672
DEBUG:root:[ Iteration 165 ] Training loss: 1.47394
DEBUG:root:[ Iteration 168 ] Training loss: 1.41699
DEBUG:root:[ Iteration 171 ] Training loss: 1.45026
DEBUG:root:[ Iteration 174 ] Training loss: 1.41892
DEBUG:root:[ Iteration 177 ] Training loss: 1.49895
DEBUG:root:[ Iteration 180 ] Training loss: 1.41743
DEBUG:root:[ Iteration 180 ] Test loss: 1.62677
DEBUG:root:[ Iteration 183 ] Training loss: 1.38529
DEBUG:root:[ Iteration 186 ] Training loss: 1.4402
DEBUG:root:[ Iteration 189 ] Training loss: 1.49926
DEBUG:root:[ Iteration 192 ] Training loss: 1.46741
DEBUG:root:[ Iteration 195 ] Training loss: 1.47483
DEBUG:root:[ Iteration 198 ] Training loss: 1.29914
DEBUG:root:[ Iteration 200 ] Test loss: 1.63112
DEBUG:root:[ Iteration 201 ] Training loss: 1.46211
DEBUG:root:[ Iteration 204 ] Training loss: 1.39962
DEBUG:root:[ Iteration 207 ] Training loss: 1.35844
DEBUG:root:[ Iteration 210 ] Training loss: 1.4016
DEBUG:root:[ Iteration 213 ] Training loss: 1.45996
DEBUG:root:[ Iteration 216 ] Training loss: 1.317
DEBUG:root:[ Iteration 219 ] Training loss: 1.51646
DEBUG:root:[ Iteration 220 ] Test loss: 1.57829
DEBUG:root:[ Iteration 222 ] Training loss: 1.37068
DEBUG:root:[ Iteration 225 ] Training loss: 1.43892
DEBUG:root:[ Iteration 228 ] Training loss: 1.25658
DEBUG:root:[ Iteration 231 ] Training loss: 1.3507
DEBUG:root:[ Iteration 234 ] Training loss: 1.24144
DEBUG:root:[ Iteration 237 ] Training loss: 1.33817
DEBUG:root:[ Iteration 240 ] Training loss: 1.45077
DEBUG:root:[ Iteration 240 ] Test loss: 1.56161
DEBUG:root:[ Iteration 243 ] Training loss: 1.30612
DEBUG:root:[ Iteration 246 ] Training loss: 1.38265
DEBUG:root:[ Iteration 249 ] Training loss: 1.37499
DEBUG:root:[ Iteration 252 ] Training loss: 1.35654
DEBUG:root:[ Iteration 255 ] Training loss: 1.39267
DEBUG:root:[ Iteration 258 ] Training loss: 1.40212
DEBUG:root:[ Iteration 260 ] Test loss: 1.38869
DEBUG:root:[ Iteration 261 ] Training loss: 1.35165
DEBUG:root:[ Iteration 264 ] Training loss: 1.30543
DEBUG:root:[ Iteration 267 ] Training loss: 1.41325
DEBUG:root:[ Iteration 270 ] Training loss: 1.34118
DEBUG:root:[ Iteration 273 ] Training loss: 1.43936
DEBUG:root:[ Iteration 276 ] Training loss: 1.32807
DEBUG:root:[ Iteration 279 ] Training loss: 1.35422
DEBUG:root:[ Iteration 280 ] Test loss: 1.63195
DEBUG:root:[ Iteration 282 ] Training loss: 1.31589
DEBUG:root:[ Iteration 285 ] Training loss: 1.32407
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-10-2016_16h39m11s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 6.43147
DEBUG:root:[ Iteration 0 ] Test loss: 6.43077
DEBUG:root:[ Iteration 3 ] Training loss: 6.38506
DEBUG:root:[ Iteration 6 ] Training loss: 6.24359
DEBUG:root:[ Iteration 9 ] Training loss: 5.11795
DEBUG:root:[ Iteration 12 ] Training loss: 3.06486
DEBUG:root:[ Iteration 15 ] Training loss: 6.12469
DEBUG:root:[ Iteration 18 ] Training loss: 3.94291
DEBUG:root:[ Iteration 20 ] Test loss: 5.48742
DEBUG:root:[ Iteration 21 ] Training loss: 4.78284
DEBUG:root:[ Iteration 24 ] Training loss: 3.85548
DEBUG:root:[ Iteration 27 ] Training loss: 3.03152
DEBUG:root:[ Iteration 30 ] Training loss: 2.15224
DEBUG:root:[ Iteration 33 ] Training loss: 2.12501
DEBUG:root:[ Iteration 36 ] Training loss: 2.0327
DEBUG:root:[ Iteration 39 ] Training loss: 1.92516
DEBUG:root:[ Iteration 40 ] Test loss: 1.8894
DEBUG:root:[ Iteration 42 ] Training loss: 1.72511
DEBUG:root:[ Iteration 45 ] Training loss: 1.65393
DEBUG:root:[ Iteration 48 ] Training loss: 1.44696
DEBUG:root:[ Iteration 51 ] Training loss: 1.50622
DEBUG:root:[ Iteration 54 ] Training loss: 1.36227
DEBUG:root:[ Iteration 57 ] Training loss: 1.45924
DEBUG:root:[ Iteration 60 ] Training loss: 1.25398
DEBUG:root:[ Iteration 60 ] Test loss: 1.45248
DEBUG:root:[ Iteration 63 ] Training loss: 1.3893
DEBUG:root:[ Iteration 66 ] Training loss: 1.17935
DEBUG:root:[ Iteration 69 ] Training loss: 1.06823
DEBUG:root:[ Iteration 72 ] Training loss: 1.2467
DEBUG:root:[ Iteration 75 ] Training loss: 1.09382
DEBUG:root:[ Iteration 78 ] Training loss: 1.19484
DEBUG:root:[ Iteration 80 ] Test loss: 1.28469
DEBUG:root:[ Iteration 81 ] Training loss: 1.11003
DEBUG:root:[ Iteration 84 ] Training loss: 1.20988
DEBUG:root:[ Iteration 87 ] Training loss: 1.15198
DEBUG:root:[ Iteration 90 ] Training loss: 1.00883
DEBUG:root:[ Iteration 93 ] Training loss: 0.889152
DEBUG:root:[ Iteration 96 ] Training loss: 0.951053
DEBUG:root:[ Iteration 99 ] Training loss: 0.960551
DEBUG:root:[ Iteration 100 ] Test loss: 0.936604
DEBUG:root:[ Iteration 102 ] Training loss: 0.911271
DEBUG:root:[ Iteration 105 ] Training loss: 0.701675
DEBUG:root:[ Iteration 108 ] Training loss: 0.922108
DEBUG:root:[ Iteration 111 ] Training loss: 0.823374
DEBUG:root:[ Iteration 114 ] Training loss: 0.88086
DEBUG:root:[ Iteration 117 ] Training loss: 0.9398
DEBUG:root:[ Iteration 120 ] Training loss: 0.759852
DEBUG:root:[ Iteration 120 ] Test loss: 1.01598
DEBUG:root:[ Iteration 123 ] Training loss: 0.801382
DEBUG:root:[ Iteration 126 ] Training loss: 0.861811
DEBUG:root:[ Iteration 129 ] Training loss: 0.748496
DEBUG:root:[ Iteration 132 ] Training loss: 0.584759
DEBUG:root:[ Iteration 135 ] Training loss: 0.768816
DEBUG:root:[ Iteration 138 ] Training loss: 0.548419
DEBUG:root:[ Iteration 140 ] Test loss: 0.935375
DEBUG:root:[ Iteration 141 ] Training loss: 0.824115
DEBUG:root:[ Iteration 144 ] Training loss: 0.715873
DEBUG:root:[ Iteration 147 ] Training loss: 0.747296
DEBUG:root:[ Iteration 150 ] Training loss: 0.75247
DEBUG:root:[ Iteration 153 ] Training loss: 0.726019
DEBUG:root:[ Iteration 156 ] Training loss: 0.959493
DEBUG:root:[ Iteration 159 ] Training loss: 0.72931
DEBUG:root:[ Iteration 160 ] Test loss: 0.718545
DEBUG:root:[ Iteration 162 ] Training loss: 0.668852
DEBUG:root:[ Iteration 165 ] Training loss: 0.594762
DEBUG:root:[ Iteration 168 ] Training loss: 0.852499
DEBUG:root:[ Iteration 171 ] Training loss: 0.777361
DEBUG:root:[ Iteration 174 ] Training loss: 0.82163
DEBUG:root:[ Iteration 177 ] Training loss: 0.662787
DEBUG:root:[ Iteration 180 ] Training loss: 0.740968
DEBUG:root:[ Iteration 180 ] Test loss: 0.755878
DEBUG:root:[ Iteration 183 ] Training loss: 0.652659
DEBUG:root:[ Iteration 186 ] Training loss: 0.612269
DEBUG:root:[ Iteration 189 ] Training loss: 0.698356
DEBUG:root:[ Iteration 192 ] Training loss: 0.623787
DEBUG:root:[ Iteration 195 ] Training loss: 0.666706
DEBUG:root:[ Iteration 198 ] Training loss: 0.665076
DEBUG:root:[ Iteration 200 ] Test loss: 0.764551
DEBUG:root:[ Iteration 201 ] Training loss: 0.811466
DEBUG:root:[ Iteration 204 ] Training loss: 0.69296
DEBUG:root:[ Iteration 207 ] Training loss: 0.6353
DEBUG:root:[ Iteration 210 ] Training loss: 0.564277
DEBUG:root:[ Iteration 213 ] Training loss: 0.782908
DEBUG:root:[ Iteration 216 ] Training loss: 0.77384
DEBUG:root:[ Iteration 219 ] Training loss: 0.717358
DEBUG:root:[ Iteration 220 ] Test loss: 0.885324
DEBUG:root:[ Iteration 222 ] Training loss: 0.620613
DEBUG:root:[ Iteration 225 ] Training loss: 0.565926
DEBUG:root:[ Iteration 228 ] Training loss: 0.560774
DEBUG:root:[ Iteration 231 ] Training loss: 0.540381
DEBUG:root:[ Iteration 234 ] Training loss: 0.633735
DEBUG:root:[ Iteration 237 ] Training loss: 0.671699
DEBUG:root:[ Iteration 240 ] Training loss: 0.593349
DEBUG:root:[ Iteration 240 ] Test loss: 0.928334
DEBUG:root:[ Iteration 243 ] Training loss: 0.451169
DEBUG:root:[ Iteration 246 ] Training loss: 0.573061
DEBUG:root:[ Iteration 249 ] Training loss: 0.536873
DEBUG:root:[ Iteration 252 ] Training loss: 0.653503
DEBUG:root:[ Iteration 255 ] Training loss: 0.610193
DEBUG:root:[ Iteration 258 ] Training loss: 0.531719
DEBUG:root:[ Iteration 260 ] Test loss: 0.834667
DEBUG:root:[ Iteration 261 ] Training loss: 0.547106
DEBUG:root:[ Iteration 264 ] Training loss: 0.40391
DEBUG:root:[ Iteration 267 ] Training loss: 0.400689
DEBUG:root:[ Iteration 270 ] Training loss: 0.441456
DEBUG:root:[ Iteration 273 ] Training loss: 0.467001
DEBUG:root:[ Iteration 276 ] Training loss: 0.52836
DEBUG:root:[ Iteration 279 ] Training loss: 0.585666
DEBUG:root:[ Iteration 280 ] Test loss: 0.854703
DEBUG:root:[ Iteration 282 ] Training loss: 0.531715
DEBUG:root:[ Iteration 285 ] Training loss: 0.547027
DEBUG:root:[ Iteration 288 ] Training loss: 0.478839
DEBUG:root:[ Iteration 291 ] Training loss: 0.587338
DEBUG:root:[ Iteration 294 ] Training loss: 0.695687
DEBUG:root:[ Iteration 297 ] Training loss: 0.57269
DEBUG:root:[ Iteration 300 ] Training loss: 0.451365
DEBUG:root:[ Iteration 300 ] Test loss: 0.622223
DEBUG:root:[ Iteration 303 ] Training loss: 0.553643
DEBUG:root:[ Iteration 306 ] Training loss: 0.423642
DEBUG:root:[ Iteration 309 ] Training loss: 0.439126
DEBUG:root:[ Iteration 312 ] Training loss: 0.46713
DEBUG:root:[ Iteration 315 ] Training loss: 0.379099
DEBUG:root:[ Iteration 318 ] Training loss: 0.362436
DEBUG:root:[ Iteration 320 ] Test loss: 0.651024
DEBUG:root:[ Iteration 321 ] Training loss: 0.555581
DEBUG:root:[ Iteration 324 ] Training loss: 0.572583
DEBUG:root:[ Iteration 327 ] Training loss: 0.640491
DEBUG:root:[ Iteration 330 ] Training loss: 0.447745
DEBUG:root:[ Iteration 333 ] Training loss: 0.648544
DEBUG:root:[ Iteration 336 ] Training loss: 0.385531
DEBUG:root:[ Iteration 339 ] Training loss: 0.453119
DEBUG:root:[ Iteration 340 ] Test loss: 0.743369
DEBUG:root:[ Iteration 342 ] Training loss: 0.6004
DEBUG:root:[ Iteration 345 ] Training loss: 0.491775
DEBUG:root:[ Iteration 348 ] Training loss: 0.561806
DEBUG:root:[ Iteration 351 ] Training loss: 0.44823
DEBUG:root:[ Iteration 354 ] Training loss: 0.533424
DEBUG:root:[ Iteration 357 ] Training loss: 0.517018
DEBUG:root:[ Iteration 360 ] Training loss: 0.343407
DEBUG:root:[ Iteration 360 ] Test loss: 0.927112
DEBUG:root:[ Iteration 363 ] Training loss: 0.39627
DEBUG:root:[ Iteration 366 ] Training loss: 0.539061
DEBUG:root:[ Iteration 369 ] Training loss: 0.441719
DEBUG:root:[ Iteration 372 ] Training loss: 0.457361
DEBUG:root:[ Iteration 375 ] Training loss: 0.620389
DEBUG:root:[ Iteration 378 ] Training loss: 0.50879
DEBUG:root:[ Iteration 380 ] Test loss: 0.685786
DEBUG:root:[ Iteration 381 ] Training loss: 0.474776
DEBUG:root:[ Iteration 384 ] Training loss: 0.513834
DEBUG:root:[ Iteration 387 ] Training loss: 0.459257
DEBUG:root:[ Iteration 390 ] Training loss: 0.476819
DEBUG:root:[ Iteration 393 ] Training loss: 0.513644
DEBUG:root:[ Iteration 396 ] Training loss: 0.422319
DEBUG:root:[ Iteration 399 ] Training loss: 0.503701
DEBUG:root:[ Iteration 400 ] Test loss: 0.763746
DEBUG:root:[ Iteration 402 ] Training loss: 0.417647
DEBUG:root:[ Iteration 405 ] Training loss: 0.391722
DEBUG:root:[ Iteration 408 ] Training loss: 0.385101
DEBUG:root:[ Iteration 411 ] Training loss: 0.43876
DEBUG:root:[ Iteration 414 ] Training loss: 0.556044
DEBUG:root:[ Iteration 417 ] Training loss: 0.349482
DEBUG:root:[ Iteration 420 ] Training loss: 0.371593
DEBUG:root:[ Iteration 420 ] Test loss: 0.864303
DEBUG:root:[ Iteration 423 ] Training loss: 0.40942
DEBUG:root:[ Iteration 426 ] Training loss: 0.50884
DEBUG:root:[ Iteration 429 ] Training loss: 0.37299
DEBUG:root:[ Iteration 432 ] Training loss: 0.324207
DEBUG:root:[ Iteration 435 ] Training loss: 0.580493
DEBUG:root:[ Iteration 438 ] Training loss: 0.478111
DEBUG:root:[ Iteration 440 ] Test loss: 0.865091
DEBUG:root:[ Iteration 441 ] Training loss: 0.363237
DEBUG:root:[ Iteration 444 ] Training loss: 0.415353
DEBUG:root:[ Iteration 447 ] Training loss: 0.462347
DEBUG:root:[ Iteration 450 ] Training loss: 0.404885
DEBUG:root:[ Iteration 453 ] Training loss: 0.466324
DEBUG:root:[ Iteration 456 ] Training loss: 0.407886
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-10-2016_16h51m48s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.15
DEBUG:root:[ Iteration 0 ] Test loss: 0.123
DEBUG:root:[ Iteration 3 ] Training loss: 0.61
DEBUG:root:[ Iteration 6 ] Training loss: 0.606
DEBUG:root:[ Iteration 9 ] Training loss: 0.643
DEBUG:root:[ Iteration 12 ] Training loss: 0.641
DEBUG:root:[ Iteration 15 ] Training loss: 0.638
DEBUG:root:[ Iteration 18 ] Training loss: 0.6
DEBUG:root:[ Iteration 20 ] Test loss: 0.63
DEBUG:root:[ Iteration 21 ] Training loss: 0.598
DEBUG:root:[ Iteration 24 ] Training loss: 0.608
DEBUG:root:[ Iteration 27 ] Training loss: 0.607
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-11-2016_11h41m51s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.769
DEBUG:root:[ Iteration 0 ] Test loss: 0.75
DEBUG:root:[ Iteration 3 ] Training loss: 0.763
DEBUG:root:[ Iteration 6 ] Training loss: 0.778
DEBUG:root:[ Iteration 9 ] Training loss: 0.763
DEBUG:root:[ Iteration 12 ] Training loss: 0.761
DEBUG:root:[ Iteration 15 ] Training loss: 0.771
DEBUG:root:[ Iteration 18 ] Training loss: 0.767
DEBUG:root:[ Iteration 20 ] Test loss: 0.761
DEBUG:root:[ Iteration 21 ] Training loss: 0.769
DEBUG:root:[ Iteration 24 ] Training loss: 0.769
DEBUG:root:[ Iteration 27 ] Training loss: 0.763
DEBUG:root:[ Iteration 30 ] Training loss: 0.764
DEBUG:root:[ Iteration 33 ] Training loss: 0.772
DEBUG:root:[ Iteration 36 ] Training loss: 0.771
DEBUG:root:[ Iteration 39 ] Training loss: 0.77
DEBUG:root:[ Iteration 40 ] Test loss: 0.743
DEBUG:root:[ Iteration 42 ] Training loss: 0.771
DEBUG:root:[ Iteration 45 ] Training loss: 0.78
DEBUG:root:[ Iteration 48 ] Training loss: 0.772
DEBUG:root:[ Iteration 51 ] Training loss: 0.772
DEBUG:root:[ Iteration 54 ] Training loss: 0.767
DEBUG:root:[ Iteration 57 ] Training loss: 0.766
DEBUG:root:[ Iteration 60 ] Training loss: 0.771
DEBUG:root:[ Iteration 60 ] Test loss: 0.754
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-11-2016_11h44m22s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.274
DEBUG:root:[ Iteration 0 ] Test loss: 0.282
DEBUG:root:[ Iteration 3 ] Training loss: 0.606
DEBUG:root:[ Iteration 6 ] Training loss: 0.642
DEBUG:root:[ Iteration 9 ] Training loss: 0.644
DEBUG:root:[ Iteration 12 ] Training loss: 0.646
DEBUG:root:[ Iteration 15 ] Training loss: 0.648
DEBUG:root:[ Iteration 18 ] Training loss: 0.643
DEBUG:root:[ Iteration 20 ] Test loss: 0.636
DEBUG:root:[ Iteration 21 ] Training loss: 0.643
DEBUG:root:[ Iteration 24 ] Training loss: 0.652
DEBUG:root:[ Iteration 27 ] Training loss: 0.649
DEBUG:root:[ Iteration 30 ] Training loss: 0.649
DEBUG:root:[ Iteration 33 ] Training loss: 0.635
DEBUG:root:[ Iteration 36 ] Training loss: 0.65
DEBUG:root:[ Iteration 39 ] Training loss: 0.663
DEBUG:root:[ Iteration 40 ] Test loss: 0.65
DEBUG:root:[ Iteration 42 ] Training loss: 0.659
DEBUG:root:[ Iteration 45 ] Training loss: 0.665
DEBUG:root:[ Iteration 48 ] Training loss: 0.658
DEBUG:root:[ Iteration 51 ] Training loss: 0.661
DEBUG:root:[ Iteration 54 ] Training loss: 0.675
DEBUG:root:[ Iteration 57 ] Training loss: 0.661
DEBUG:root:[ Iteration 60 ] Training loss: 0.666
DEBUG:root:[ Iteration 60 ] Test loss: 0.668
DEBUG:root:[ Iteration 63 ] Training loss: 0.679
DEBUG:root:[ Iteration 66 ] Training loss: 0.685
DEBUG:root:[ Iteration 69 ] Training loss: 0.684
DEBUG:root:[ Iteration 72 ] Training loss: 0.689
DEBUG:root:[ Iteration 75 ] Training loss: 0.696
DEBUG:root:[ Iteration 78 ] Training loss: 0.701
DEBUG:root:[ Iteration 80 ] Test loss: 0.686
DEBUG:root:[ Iteration 81 ] Training loss: 0.694
DEBUG:root:[ Iteration 84 ] Training loss: 0.701
DEBUG:root:[ Iteration 87 ] Training loss: 0.7
DEBUG:root:[ Iteration 90 ] Training loss: 0.698
DEBUG:root:[ Iteration 93 ] Training loss: 0.7
DEBUG:root:[ Iteration 96 ] Training loss: 0.687
DEBUG:root:[ Iteration 99 ] Training loss: 0.705
DEBUG:root:[ Iteration 100 ] Test loss: 0.7
DEBUG:root:[ Iteration 102 ] Training loss: 0.709
DEBUG:root:[ Iteration 105 ] Training loss: 0.723
DEBUG:root:[ Iteration 108 ] Training loss: 0.703
DEBUG:root:[ Iteration 111 ] Training loss: 0.71
DEBUG:root:[ Iteration 114 ] Training loss: 0.728
DEBUG:root:[ Iteration 117 ] Training loss: 0.71
DEBUG:root:[ Iteration 120 ] Training loss: 0.729
DEBUG:root:[ Iteration 120 ] Test loss: 0.711
DEBUG:root:[ Iteration 123 ] Training loss: 0.714
DEBUG:root:[ Iteration 126 ] Training loss: 0.71
DEBUG:root:[ Iteration 129 ] Training loss: 0.718
DEBUG:root:[ Iteration 132 ] Training loss: 0.707
DEBUG:root:[ Iteration 135 ] Training loss: 0.718
DEBUG:root:[ Iteration 138 ] Training loss: 0.72
DEBUG:root:[ Iteration 140 ] Test loss: 0.715
DEBUG:root:[ Iteration 141 ] Training loss: 0.713
DEBUG:root:[ Iteration 144 ] Training loss: 0.719
DEBUG:root:[ Iteration 147 ] Training loss: 0.724
DEBUG:root:[ Iteration 150 ] Training loss: 0.728
DEBUG:root:[ Iteration 153 ] Training loss: 0.725
DEBUG:root:[ Iteration 156 ] Training loss: 0.713
DEBUG:root:[ Iteration 159 ] Training loss: 0.716
DEBUG:root:[ Iteration 160 ] Test loss: 0.718
DEBUG:root:[ Iteration 162 ] Training loss: 0.712
DEBUG:root:[ Iteration 165 ] Training loss: 0.736
DEBUG:root:[ Iteration 168 ] Training loss: 0.732
DEBUG:root:[ Iteration 171 ] Training loss: 0.721
DEBUG:root:[ Iteration 174 ] Training loss: 0.734
DEBUG:root:[ Iteration 177 ] Training loss: 0.727
DEBUG:root:[ Iteration 180 ] Training loss: 0.726
DEBUG:root:[ Iteration 180 ] Test loss: 0.715
DEBUG:root:[ Iteration 183 ] Training loss: 0.73
DEBUG:root:[ Iteration 186 ] Training loss: 0.732
DEBUG:root:[ Iteration 189 ] Training loss: 0.73
DEBUG:root:[ Iteration 192 ] Training loss: 0.721
DEBUG:root:[ Iteration 195 ] Training loss: 0.737
DEBUG:root:[ Iteration 198 ] Training loss: 0.725
DEBUG:root:[ Iteration 200 ] Test loss: 0.71
DEBUG:root:[ Iteration 201 ] Training loss: 0.73
DEBUG:root:[ Iteration 204 ] Training loss: 0.72
DEBUG:root:[ Iteration 207 ] Training loss: 0.728
DEBUG:root:[ Iteration 210 ] Training loss: 0.727
DEBUG:root:[ Iteration 213 ] Training loss: 0.72
DEBUG:root:[ Iteration 216 ] Training loss: 0.728
DEBUG:root:[ Iteration 219 ] Training loss: 0.716
DEBUG:root:[ Iteration 220 ] Test loss: 0.712
DEBUG:root:[ Iteration 222 ] Training loss: 0.735
DEBUG:root:[ Iteration 225 ] Training loss: 0.727
DEBUG:root:[ Iteration 228 ] Training loss: 0.729
DEBUG:root:[ Iteration 231 ] Training loss: 0.737
DEBUG:root:[ Iteration 234 ] Training loss: 0.731
DEBUG:root:[ Iteration 237 ] Training loss: 0.731
DEBUG:root:[ Iteration 240 ] Training loss: 0.745
DEBUG:root:[ Iteration 240 ] Test loss: 0.724
DEBUG:root:[ Iteration 243 ] Training loss: 0.731
DEBUG:root:[ Iteration 246 ] Training loss: 0.737
DEBUG:root:[ Iteration 249 ] Training loss: 0.737
DEBUG:root:[ Iteration 252 ] Training loss: 0.745
DEBUG:root:[ Iteration 255 ] Training loss: 0.735
DEBUG:root:[ Iteration 258 ] Training loss: 0.726
DEBUG:root:[ Iteration 260 ] Test loss: 0.729
DEBUG:root:[ Iteration 261 ] Training loss: 0.745
DEBUG:root:[ Iteration 264 ] Training loss: 0.747
DEBUG:root:[ Iteration 267 ] Training loss: 0.724
DEBUG:root:[ Iteration 270 ] Training loss: 0.734
DEBUG:root:[ Iteration 273 ] Training loss: 0.736
DEBUG:root:[ Iteration 276 ] Training loss: 0.748
DEBUG:root:[ Iteration 279 ] Training loss: 0.734
DEBUG:root:[ Iteration 280 ] Test loss: 0.718
DEBUG:root:[ Iteration 282 ] Training loss: 0.744
DEBUG:root:[ Iteration 285 ] Training loss: 0.743
DEBUG:root:[ Iteration 288 ] Training loss: 0.735
DEBUG:root:[ Iteration 291 ] Training loss: 0.743
DEBUG:root:[ Iteration 294 ] Training loss: 0.735
DEBUG:root:[ Iteration 297 ] Training loss: 0.732
DEBUG:root:[ Iteration 300 ] Training loss: 0.722
DEBUG:root:[ Iteration 300 ] Test loss: 0.725
DEBUG:root:[ Iteration 303 ] Training loss: 0.738
DEBUG:root:[ Iteration 306 ] Training loss: 0.742
DEBUG:root:[ Iteration 309 ] Training loss: 0.754
DEBUG:root:[ Iteration 312 ] Training loss: 0.743
DEBUG:root:[ Iteration 315 ] Training loss: 0.736
DEBUG:root:[ Iteration 318 ] Training loss: 0.749
DEBUG:root:[ Iteration 320 ] Test loss: 0.72
DEBUG:root:[ Iteration 321 ] Training loss: 0.737
DEBUG:root:[ Iteration 324 ] Training loss: 0.748
DEBUG:root:[ Iteration 327 ] Training loss: 0.736
DEBUG:root:[ Iteration 330 ] Training loss: 0.739
DEBUG:root:[ Iteration 333 ] Training loss: 0.734
DEBUG:root:[ Iteration 336 ] Training loss: 0.743
DEBUG:root:[ Iteration 339 ] Training loss: 0.74
DEBUG:root:[ Iteration 340 ] Test loss: 0.724
DEBUG:root:[ Iteration 342 ] Training loss: 0.73
DEBUG:root:[ Iteration 345 ] Training loss: 0.738
DEBUG:root:[ Iteration 348 ] Training loss: 0.747
DEBUG:root:[ Iteration 351 ] Training loss: 0.737
DEBUG:root:[ Iteration 354 ] Training loss: 0.743
DEBUG:root:[ Iteration 357 ] Training loss: 0.749
DEBUG:root:[ Iteration 360 ] Training loss: 0.739
DEBUG:root:[ Iteration 360 ] Test loss: 0.745
DEBUG:root:[ Iteration 363 ] Training loss: 0.749
DEBUG:root:[ Iteration 366 ] Training loss: 0.749
DEBUG:root:[ Iteration 369 ] Training loss: 0.747
DEBUG:root:[ Iteration 372 ] Training loss: 0.744
DEBUG:root:[ Iteration 375 ] Training loss: 0.745
DEBUG:root:[ Iteration 378 ] Training loss: 0.747
DEBUG:root:[ Iteration 380 ] Test loss: 0.725
DEBUG:root:[ Iteration 381 ] Training loss: 0.732
DEBUG:root:[ Iteration 384 ] Training loss: 0.746
DEBUG:root:[ Iteration 387 ] Training loss: 0.738
DEBUG:root:[ Iteration 390 ] Training loss: 0.737
DEBUG:root:[ Iteration 393 ] Training loss: 0.744
DEBUG:root:[ Iteration 396 ] Training loss: 0.751
DEBUG:root:[ Iteration 399 ] Training loss: 0.741
DEBUG:root:[ Iteration 400 ] Test loss: 0.728
DEBUG:root:[ Iteration 402 ] Training loss: 0.751
DEBUG:root:[ Iteration 405 ] Training loss: 0.751
DEBUG:root:[ Iteration 408 ] Training loss: 0.748
DEBUG:root:[ Iteration 411 ] Training loss: 0.758
DEBUG:root:[ Iteration 414 ] Training loss: 0.751
DEBUG:root:[ Iteration 417 ] Training loss: 0.736
DEBUG:root:[ Iteration 420 ] Training loss: 0.744
DEBUG:root:[ Iteration 420 ] Test loss: 0.747
DEBUG:root:[ Iteration 423 ] Training loss: 0.747
DEBUG:root:[ Iteration 426 ] Training loss: 0.738
DEBUG:root:[ Iteration 429 ] Training loss: 0.741
DEBUG:root:[ Iteration 432 ] Training loss: 0.751
DEBUG:root:[ Iteration 435 ] Training loss: 0.742
DEBUG:root:[ Iteration 438 ] Training loss: 0.742
DEBUG:root:[ Iteration 440 ] Test loss: 0.725
DEBUG:root:[ Iteration 441 ] Training loss: 0.74
DEBUG:root:[ Iteration 444 ] Training loss: 0.746
DEBUG:root:[ Iteration 447 ] Training loss: 0.752
DEBUG:root:[ Iteration 450 ] Training loss: 0.753
DEBUG:root:[ Iteration 453 ] Training loss: 0.765
DEBUG:root:[ Iteration 456 ] Training loss: 0.752
DEBUG:root:[ Iteration 459 ] Training loss: 0.754
DEBUG:root:[ Iteration 460 ] Test loss: 0.73
DEBUG:root:[ Iteration 462 ] Training loss: 0.738
DEBUG:root:[ Iteration 465 ] Training loss: 0.737
DEBUG:root:[ Iteration 468 ] Training loss: 0.742
DEBUG:root:[ Iteration 471 ] Training loss: 0.759
DEBUG:root:[ Iteration 474 ] Training loss: 0.74
DEBUG:root:[ Iteration 477 ] Training loss: 0.74
DEBUG:root:[ Iteration 480 ] Training loss: 0.748
DEBUG:root:[ Iteration 480 ] Test loss: 0.738
DEBUG:root:[ Iteration 483 ] Training loss: 0.725
DEBUG:root:[ Iteration 486 ] Training loss: 0.744
DEBUG:root:[ Iteration 489 ] Training loss: 0.745
DEBUG:root:[ Iteration 492 ] Training loss: 0.749
DEBUG:root:[ Iteration 495 ] Training loss: 0.744
DEBUG:root:[ Iteration 498 ] Training loss: 0.753
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_05-11-2016_12h09m12s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.158315
DEBUG:root:[ Iteration 0 ] Test loss: 0.153687
DEBUG:root:[ Iteration 3 ] Training loss: 0.149096
DEBUG:root:[ Iteration 6 ] Training loss: 0.162369
DEBUG:root:[ Iteration 9 ] Training loss: 0.152025
DEBUG:root:[ Iteration 12 ] Training loss: 0.1519
DEBUG:root:[ Iteration 15 ] Training loss: 0.141871
DEBUG:root:[ Iteration 18 ] Training loss: 0.132326
DEBUG:root:[ Iteration 20 ] Test loss: 0.12731
DEBUG:root:[ Iteration 21 ] Training loss: 0.126597
DEBUG:root:[ Iteration 24 ] Training loss: 0.0939266
DEBUG:root:[ Iteration 27 ] Training loss: 0.07122
DEBUG:root:[ Iteration 30 ] Training loss: 0.066863
DEBUG:root:[ Iteration 33 ] Training loss: 0.0711842
DEBUG:root:[ Iteration 36 ] Training loss: 0.074119
DEBUG:root:[ Iteration 39 ] Training loss: 0.0728575
DEBUG:root:[ Iteration 40 ] Test loss: 0.0776379
DEBUG:root:[ Iteration 42 ] Training loss: 0.0750225
DEBUG:root:[ Iteration 45 ] Training loss: 0.0813709
DEBUG:root:[ Iteration 48 ] Training loss: 0.0849663
DEBUG:root:[ Iteration 51 ] Training loss: 0.0710535
DEBUG:root:[ Iteration 54 ] Training loss: 0.0684274
DEBUG:root:[ Iteration 57 ] Training loss: 0.07266
DEBUG:root:[ Iteration 60 ] Training loss: 0.0644312
DEBUG:root:[ Iteration 60 ] Test loss: 0.0546263
DEBUG:root:[ Iteration 63 ] Training loss: 0.0605427
DEBUG:root:[ Iteration 66 ] Training loss: 0.0661824
DEBUG:root:[ Iteration 69 ] Training loss: 0.0547377
DEBUG:root:[ Iteration 72 ] Training loss: 0.0551424
DEBUG:root:[ Iteration 75 ] Training loss: 0.0493253
DEBUG:root:[ Iteration 78 ] Training loss: 0.0549679
DEBUG:root:[ Iteration 80 ] Test loss: 0.0583508
DEBUG:root:[ Iteration 81 ] Training loss: 0.0514042
DEBUG:root:[ Iteration 84 ] Training loss: 0.0467183
DEBUG:root:[ Iteration 87 ] Training loss: 0.0511498
DEBUG:root:[ Iteration 90 ] Training loss: 0.0532706
DEBUG:root:[ Iteration 93 ] Training loss: 0.0403509
DEBUG:root:[ Iteration 96 ] Training loss: 0.0525809
DEBUG:root:[ Iteration 99 ] Training loss: 0.0497991
DEBUG:root:[ Iteration 100 ] Test loss: 0.0380891
DEBUG:root:[ Iteration 102 ] Training loss: 0.0525043
DEBUG:root:[ Iteration 105 ] Training loss: 0.0575232
DEBUG:root:[ Iteration 108 ] Training loss: 0.0432405
DEBUG:root:[ Iteration 111 ] Training loss: 0.0504681
DEBUG:root:[ Iteration 114 ] Training loss: 0.0420051
DEBUG:root:[ Iteration 117 ] Training loss: 0.0541369
DEBUG:root:[ Iteration 120 ] Training loss: 0.0567226
DEBUG:root:[ Iteration 120 ] Test loss: 0.0524057
DEBUG:root:[ Iteration 123 ] Training loss: 0.0397407
DEBUG:root:[ Iteration 126 ] Training loss: 0.0582346
DEBUG:root:[ Iteration 129 ] Training loss: 0.0575705
DEBUG:root:[ Iteration 132 ] Training loss: 0.0504768
DEBUG:root:[ Iteration 135 ] Training loss: 0.0451217
DEBUG:root:[ Iteration 138 ] Training loss: 0.0446597
DEBUG:root:[ Iteration 140 ] Test loss: 0.0388435
DEBUG:root:[ Iteration 141 ] Training loss: 0.0566817
DEBUG:root:[ Iteration 144 ] Training loss: 0.0454043
DEBUG:root:[ Iteration 147 ] Training loss: 0.0426703
DEBUG:root:[ Iteration 150 ] Training loss: 0.0502277
DEBUG:root:[ Iteration 153 ] Training loss: 0.0458902
DEBUG:root:[ Iteration 156 ] Training loss: 0.042368
DEBUG:root:[ Iteration 159 ] Training loss: 0.0388706
DEBUG:root:[ Iteration 160 ] Test loss: 0.0475522
DEBUG:root:[ Iteration 162 ] Training loss: 0.0399166
DEBUG:root:[ Iteration 165 ] Training loss: 0.048399
DEBUG:root:[ Iteration 168 ] Training loss: 0.0328987
DEBUG:root:[ Iteration 171 ] Training loss: 0.0365967
DEBUG:root:[ Iteration 174 ] Training loss: 0.0362401
DEBUG:root:[ Iteration 177 ] Training loss: 0.0325791
DEBUG:root:[ Iteration 180 ] Training loss: 0.0521524
DEBUG:root:[ Iteration 180 ] Test loss: 0.038982
DEBUG:root:[ Iteration 183 ] Training loss: 0.0437754
DEBUG:root:[ Iteration 186 ] Training loss: 0.0351151
DEBUG:root:[ Iteration 189 ] Training loss: 0.0382732
DEBUG:root:[ Iteration 192 ] Training loss: 0.0359574
DEBUG:root:[ Iteration 195 ] Training loss: 0.0381217
DEBUG:root:[ Iteration 198 ] Training loss: 0.0354584
DEBUG:root:[ Iteration 200 ] Test loss: 0.0345755
DEBUG:root:[ Iteration 201 ] Training loss: 0.032067
DEBUG:root:[ Iteration 204 ] Training loss: 0.0364243
DEBUG:root:[ Iteration 207 ] Training loss: 0.0348778
DEBUG:root:[ Iteration 210 ] Training loss: 0.0375167
DEBUG:root:[ Iteration 213 ] Training loss: 0.0365296
DEBUG:root:[ Iteration 216 ] Training loss: 0.0386539
DEBUG:root:[ Iteration 219 ] Training loss: 0.0403245
DEBUG:root:[ Iteration 220 ] Test loss: 0.0334445
DEBUG:root:[ Iteration 222 ] Training loss: 0.0263142
DEBUG:root:[ Iteration 225 ] Training loss: 0.0400799
DEBUG:root:[ Iteration 228 ] Training loss: 0.031265
DEBUG:root:[ Iteration 231 ] Training loss: 0.0326953
DEBUG:root:[ Iteration 234 ] Training loss: 0.0336892
DEBUG:root:[ Iteration 237 ] Training loss: 0.0284048
DEBUG:root:[ Iteration 240 ] Training loss: 0.0450464
DEBUG:root:[ Iteration 240 ] Test loss: 0.0296261
DEBUG:root:[ Iteration 243 ] Training loss: 0.0281235
DEBUG:root:[ Iteration 246 ] Training loss: 0.0403791
DEBUG:root:[ Iteration 249 ] Training loss: 0.0280529
DEBUG:root:[ Iteration 252 ] Training loss: 0.0335482
DEBUG:root:[ Iteration 255 ] Training loss: 0.0398497
DEBUG:root:[ Iteration 258 ] Training loss: 0.0329089
DEBUG:root:[ Iteration 260 ] Test loss: 0.0392862
DEBUG:root:[ Iteration 261 ] Training loss: 0.0325541
DEBUG:root:[ Iteration 264 ] Training loss: 0.0290281
DEBUG:root:[ Iteration 267 ] Training loss: 0.043003
DEBUG:root:[ Iteration 270 ] Training loss: 0.0351587
DEBUG:root:[ Iteration 273 ] Training loss: 0.0347891
DEBUG:root:[ Iteration 276 ] Training loss: 0.0291941
DEBUG:root:[ Iteration 279 ] Training loss: 0.0286655
DEBUG:root:[ Iteration 280 ] Test loss: 0.0254738
DEBUG:root:[ Iteration 282 ] Training loss: 0.0337062
DEBUG:root:[ Iteration 285 ] Training loss: 0.0312047
DEBUG:root:[ Iteration 288 ] Training loss: 0.0291913
DEBUG:root:[ Iteration 291 ] Training loss: 0.0290573
DEBUG:root:[ Iteration 294 ] Training loss: 0.0284552
DEBUG:root:[ Iteration 297 ] Training loss: 0.0359038
DEBUG:root:[ Iteration 300 ] Training loss: 0.0275158
DEBUG:root:[ Iteration 300 ] Test loss: 0.0256786
DEBUG:root:[ Iteration 303 ] Training loss: 0.0281904
DEBUG:root:[ Iteration 306 ] Training loss: 0.0300396
DEBUG:root:[ Iteration 309 ] Training loss: 0.0316996
DEBUG:root:[ Iteration 312 ] Training loss: 0.0246819
DEBUG:root:[ Iteration 315 ] Training loss: 0.0239771
DEBUG:root:[ Iteration 318 ] Training loss: 0.0296916
DEBUG:root:[ Iteration 320 ] Test loss: 0.0326072
DEBUG:root:[ Iteration 321 ] Training loss: 0.030409
DEBUG:root:[ Iteration 324 ] Training loss: 0.0251666
DEBUG:root:[ Iteration 327 ] Training loss: 0.0245723
DEBUG:root:[ Iteration 330 ] Training loss: 0.0291797
DEBUG:root:[ Iteration 333 ] Training loss: 0.0303199
DEBUG:root:[ Iteration 336 ] Training loss: 0.0242248
DEBUG:root:[ Iteration 339 ] Training loss: 0.022851
DEBUG:root:[ Iteration 340 ] Test loss: 0.0286362
DEBUG:root:[ Iteration 342 ] Training loss: 0.026031
DEBUG:root:[ Iteration 345 ] Training loss: 0.021735
DEBUG:root:[ Iteration 348 ] Training loss: 0.0239525
DEBUG:root:[ Iteration 351 ] Training loss: 0.0203476
DEBUG:root:[ Iteration 354 ] Training loss: 0.0270897
DEBUG:root:[ Iteration 357 ] Training loss: 0.0216786
DEBUG:root:[ Iteration 360 ] Training loss: 0.0241136
DEBUG:root:[ Iteration 360 ] Test loss: 0.0244749
DEBUG:root:[ Iteration 363 ] Training loss: 0.0246859
DEBUG:root:[ Iteration 366 ] Training loss: 0.026544
DEBUG:root:[ Iteration 369 ] Training loss: 0.0308127
DEBUG:root:[ Iteration 372 ] Training loss: 0.0259123
DEBUG:root:[ Iteration 375 ] Training loss: 0.023957
DEBUG:root:[ Iteration 378 ] Training loss: 0.0173889
DEBUG:root:[ Iteration 380 ] Test loss: 0.0216193
DEBUG:root:[ Iteration 381 ] Training loss: 0.0221056
DEBUG:root:[ Iteration 384 ] Training loss: 0.0205001
DEBUG:root:[ Iteration 387 ] Training loss: 0.0219474
DEBUG:root:[ Iteration 390 ] Training loss: 0.0234973
DEBUG:root:[ Iteration 393 ] Training loss: 0.0299693
DEBUG:root:[ Iteration 396 ] Training loss: 0.0176435
DEBUG:root:[ Iteration 399 ] Training loss: 0.0203286
DEBUG:root:[ Iteration 400 ] Test loss: 0.0281259
DEBUG:root:[ Iteration 402 ] Training loss: 0.0198533
DEBUG:root:[ Iteration 405 ] Training loss: 0.0256807
DEBUG:root:[ Iteration 408 ] Training loss: 0.0307884
DEBUG:root:[ Iteration 411 ] Training loss: 0.015832
DEBUG:root:[ Iteration 414 ] Training loss: 0.0233794
DEBUG:root:[ Iteration 417 ] Training loss: 0.0238226
DEBUG:root:[ Iteration 420 ] Training loss: 0.0214571
DEBUG:root:[ Iteration 420 ] Test loss: 0.0228927
DEBUG:root:[ Iteration 423 ] Training loss: 0.021654
DEBUG:root:[ Iteration 426 ] Training loss: 0.0235356
DEBUG:root:[ Iteration 429 ] Training loss: 0.0233124
DEBUG:root:[ Iteration 432 ] Training loss: 0.0170025
DEBUG:root:[ Iteration 435 ] Training loss: 0.0199805
DEBUG:root:[ Iteration 438 ] Training loss: 0.0141345
DEBUG:root:[ Iteration 440 ] Test loss: 0.0214467
DEBUG:root:[ Iteration 441 ] Training loss: 0.0166785
DEBUG:root:[ Iteration 444 ] Training loss: 0.0176209
DEBUG:root:[ Iteration 447 ] Training loss: 0.0169603
DEBUG:root:[ Iteration 450 ] Training loss: 0.0189018
DEBUG:root:[ Iteration 453 ] Training loss: 0.017282
DEBUG:root:[ Iteration 456 ] Training loss: 0.0202381
DEBUG:root:[ Iteration 459 ] Training loss: 0.0199086
DEBUG:root:[ Iteration 460 ] Test loss: 0.0191641
DEBUG:root:[ Iteration 462 ] Training loss: 0.017717
DEBUG:root:[ Iteration 465 ] Training loss: 0.0170219
DEBUG:root:[ Iteration 468 ] Training loss: 0.0227074
DEBUG:root:[ Iteration 471 ] Training loss: 0.0191076
DEBUG:root:[ Iteration 474 ] Training loss: 0.0173357
DEBUG:root:[ Iteration 477 ] Training loss: 0.0200997
DEBUG:root:[ Iteration 480 ] Training loss: 0.0166864
DEBUG:root:[ Iteration 480 ] Test loss: 0.0207602
DEBUG:root:[ Iteration 483 ] Training loss: 0.0184996
DEBUG:root:[ Iteration 486 ] Training loss: 0.0160718
DEBUG:root:[ Iteration 489 ] Training loss: 0.0187957
DEBUG:root:[ Iteration 492 ] Training loss: 0.0156509
DEBUG:root:[ Iteration 495 ] Training loss: 0.023925
DEBUG:root:[ Iteration 498 ] Training loss: 0.019301
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-06-2016_11h27m25s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.204051
DEBUG:root:[ Iteration 0 ] Test loss: 0.199858
DEBUG:root:[ Iteration 3 ] Training loss: 0.199977
DEBUG:root:[ Iteration 6 ] Training loss: 0.199833
DEBUG:root:[ Iteration 9 ] Training loss: 0.199967
DEBUG:root:[ Iteration 12 ] Training loss: 0.192815
DEBUG:root:[ Iteration 15 ] Training loss: 0.187768
DEBUG:root:[ Iteration 18 ] Training loss: 0.18427
DEBUG:root:[ Iteration 20 ] Test loss: 0.171983
DEBUG:root:[ Iteration 21 ] Training loss: 0.17505
DEBUG:root:[ Iteration 24 ] Training loss: 0.160471
DEBUG:root:[ Iteration 27 ] Training loss: 0.144596
DEBUG:root:[ Iteration 30 ] Training loss: 0.116538
DEBUG:root:[ Iteration 33 ] Training loss: 0.0893351
DEBUG:root:[ Iteration 36 ] Training loss: 0.0656532
DEBUG:root:[ Iteration 39 ] Training loss: 0.0569654
DEBUG:root:[ Iteration 40 ] Test loss: 0.0452916
DEBUG:root:[ Iteration 42 ] Training loss: 0.0403803
DEBUG:root:[ Iteration 45 ] Training loss: 0.0286939
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-06-2016_15h55m40s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.209511
DEBUG:root:[ Iteration 0 ] Test loss: 0.20354
DEBUG:root:[ Iteration 3 ] Training loss: 0.206473
DEBUG:root:[ Iteration 6 ] Training loss: 0.201205
DEBUG:root:[ Iteration 9 ] Training loss: 0.197397
DEBUG:root:[ Iteration 12 ] Training loss: 0.196801
DEBUG:root:[ Iteration 15 ] Training loss: 0.188213
DEBUG:root:[ Iteration 18 ] Training loss: 0.182197
DEBUG:root:[ Iteration 20 ] Test loss: 0.179797
DEBUG:root:[ Iteration 21 ] Training loss: 0.179503
DEBUG:root:[ Iteration 24 ] Training loss: 0.177279
DEBUG:root:[ Iteration 27 ] Training loss: 0.168193
DEBUG:root:[ Iteration 30 ] Training loss: 0.164736
DEBUG:root:[ Iteration 33 ] Training loss: 0.158221
DEBUG:root:[ Iteration 36 ] Training loss: 0.15482
DEBUG:root:[ Iteration 39 ] Training loss: 0.141135
DEBUG:root:[ Iteration 40 ] Test loss: 0.135389
DEBUG:root:[ Iteration 42 ] Training loss: 0.135895
DEBUG:root:[ Iteration 45 ] Training loss: 0.115388
DEBUG:root:[ Iteration 48 ] Training loss: 0.0978747
DEBUG:root:[ Iteration 51 ] Training loss: 0.0836301
DEBUG:root:[ Iteration 54 ] Training loss: 0.0779339
DEBUG:root:[ Iteration 57 ] Training loss: 0.075409
DEBUG:root:[ Iteration 60 ] Training loss: 0.0702867
DEBUG:root:[ Iteration 60 ] Test loss: 0.0625256
DEBUG:root:[ Iteration 63 ] Training loss: 0.0689574
DEBUG:root:[ Iteration 66 ] Training loss: 0.0591345
DEBUG:root:[ Iteration 69 ] Training loss: 0.0462848
DEBUG:root:[ Iteration 72 ] Training loss: 0.0375532
DEBUG:root:[ Iteration 75 ] Training loss: 0.0238126
DEBUG:root:[ Iteration 78 ] Training loss: 0.0140352
DEBUG:root:[ Iteration 80 ] Test loss: 0.00751168
DEBUG:root:[ Iteration 81 ] Training loss: 0.0101592
DEBUG:root:[ Iteration 84 ] Training loss: 0.00852192
DEBUG:root:[ Iteration 87 ] Training loss: 0.00708021
DEBUG:root:[ Iteration 90 ] Training loss: 0.00630668
DEBUG:root:[ Iteration 93 ] Training loss: 0.00646739
DEBUG:root:[ Iteration 96 ] Training loss: 0.00612828
DEBUG:root:[ Iteration 99 ] Training loss: 0.00654316
DEBUG:root:[ Iteration 100 ] Test loss: 0.00325727
DEBUG:root:[ Iteration 102 ] Training loss: 0.00696621
DEBUG:root:[ Iteration 105 ] Training loss: 0.0038096
DEBUG:root:[ Iteration 108 ] Training loss: 0.00519607
DEBUG:root:[ Iteration 111 ] Training loss: 0.00639164
DEBUG:root:[ Iteration 114 ] Training loss: 0.00618943
DEBUG:root:[ Iteration 117 ] Training loss: 0.00278354
DEBUG:root:[ Iteration 120 ] Training loss: 0.00222421
DEBUG:root:[ Iteration 120 ] Test loss: 0.00243336
DEBUG:root:[ Iteration 123 ] Training loss: 0.00508683
DEBUG:root:[ Iteration 126 ] Training loss: 0.0051564
DEBUG:root:[ Iteration 129 ] Training loss: 0.00443851
DEBUG:root:[ Iteration 132 ] Training loss: 0.00503568
DEBUG:root:[ Iteration 135 ] Training loss: 0.0044483
DEBUG:root:[ Iteration 138 ] Training loss: 0.00188785
DEBUG:root:[ Iteration 140 ] Test loss: 0.0023758
DEBUG:root:[ Iteration 141 ] Training loss: 0.00480542
DEBUG:root:[ Iteration 144 ] Training loss: 0.00291878
DEBUG:root:[ Iteration 147 ] Training loss: 0.00447432
DEBUG:root:[ Iteration 150 ] Training loss: 0.00458827
DEBUG:root:[ Iteration 153 ] Training loss: 0.00496309
DEBUG:root:[ Iteration 156 ] Training loss: 0.00159046
DEBUG:root:[ Iteration 159 ] Training loss: 0.00457675
DEBUG:root:[ Iteration 160 ] Test loss: 0.00218837
DEBUG:root:[ Iteration 162 ] Training loss: 0.00175398
DEBUG:root:[ Iteration 165 ] Training loss: 0.00419061
DEBUG:root:[ Iteration 168 ] Training loss: 0.002229
DEBUG:root:[ Iteration 171 ] Training loss: 0.00453628
DEBUG:root:[ Iteration 174 ] Training loss: 0.00434399
DEBUG:root:[ Iteration 177 ] Training loss: 0.00144368
DEBUG:root:[ Iteration 180 ] Training loss: 0.0042976
DEBUG:root:[ Iteration 180 ] Test loss: 0.00241486
DEBUG:root:[ Iteration 183 ] Training loss: 0.00198144
DEBUG:root:[ Iteration 186 ] Training loss: 0.00170784
DEBUG:root:[ Iteration 189 ] Training loss: 0.00383089
DEBUG:root:[ Iteration 192 ] Training loss: 0.00432589
DEBUG:root:[ Iteration 195 ] Training loss: 0.00141247
DEBUG:root:[ Iteration 198 ] Training loss: 0.00152775
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-06-2016_16h00m43s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.126241
DEBUG:root:[ Iteration 0 ] Test loss: 0.127377
DEBUG:root:[ Iteration 3 ] Training loss: 0.126117
DEBUG:root:[ Iteration 6 ] Training loss: 0.123706
DEBUG:root:[ Iteration 9 ] Training loss: 0.119635
DEBUG:root:[ Iteration 12 ] Training loss: 0.115583
DEBUG:root:[ Iteration 15 ] Training loss: 0.111157
DEBUG:root:[ Iteration 18 ] Training loss: 0.104908
DEBUG:root:[ Iteration 20 ] Test loss: 0.0998636
DEBUG:root:[ Iteration 21 ] Training loss: 0.0972668
DEBUG:root:[ Iteration 24 ] Training loss: 0.0855908
DEBUG:root:[ Iteration 27 ] Training loss: 0.0687688
DEBUG:root:[ Iteration 30 ] Training loss: 0.041851
DEBUG:root:[ Iteration 33 ] Training loss: 0.0153645
DEBUG:root:[ Iteration 36 ] Training loss: 0.00486559
DEBUG:root:[ Iteration 39 ] Training loss: 0.0033829
DEBUG:root:[ Iteration 40 ] Test loss: 0.00313483
DEBUG:root:[ Iteration 42 ] Training loss: 0.00259859
DEBUG:root:[ Iteration 45 ] Training loss: 0.00257824
DEBUG:root:[ Iteration 48 ] Training loss: 0.0016206
DEBUG:root:[ Iteration 51 ] Training loss: 0.00127772
DEBUG:root:[ Iteration 54 ] Training loss: 0.000976692
DEBUG:root:[ Iteration 57 ] Training loss: 0.000899617
DEBUG:root:[ Iteration 60 ] Training loss: 0.000788794
DEBUG:root:[ Iteration 60 ] Test loss: 0.000702268
DEBUG:root:[ Iteration 63 ] Training loss: 0.000716092
DEBUG:root:[ Iteration 66 ] Training loss: 0.000485504
DEBUG:root:[ Iteration 69 ] Training loss: 0.000617796
DEBUG:root:[ Iteration 72 ] Training loss: 0.000440117
DEBUG:root:[ Iteration 75 ] Training loss: 0.000508274
DEBUG:root:[ Iteration 78 ] Training loss: 0.000454562
DEBUG:root:[ Iteration 80 ] Test loss: 0.000439614
DEBUG:root:[ Iteration 81 ] Training loss: 0.000368263
DEBUG:root:[ Iteration 84 ] Training loss: 0.00043614
DEBUG:root:[ Iteration 87 ] Training loss: 0.000324311
DEBUG:root:[ Iteration 90 ] Training loss: 0.000394348
DEBUG:root:[ Iteration 93 ] Training loss: 0.000402781
DEBUG:root:[ Iteration 96 ] Training loss: 0.00034956
DEBUG:root:[ Iteration 99 ] Training loss: 0.0002933
DEBUG:root:[ Iteration 100 ] Test loss: 0.000400129
DEBUG:root:[ Iteration 102 ] Training loss: 0.000305432
DEBUG:root:[ Iteration 105 ] Training loss: 0.000333337
DEBUG:root:[ Iteration 108 ] Training loss: 0.000284006
DEBUG:root:[ Iteration 111 ] Training loss: 0.000390327
DEBUG:root:[ Iteration 114 ] Training loss: 0.000375778
DEBUG:root:[ Iteration 117 ] Training loss: 0.000302433
DEBUG:root:[ Iteration 120 ] Training loss: 0.000336364
DEBUG:root:[ Iteration 120 ] Test loss: 0.00038541
DEBUG:root:[ Iteration 123 ] Training loss: 0.000340399
DEBUG:root:[ Iteration 126 ] Training loss: 0.000261365
DEBUG:root:[ Iteration 129 ] Training loss: 0.000338716
DEBUG:root:[ Iteration 132 ] Training loss: 0.000231827
DEBUG:root:[ Iteration 135 ] Training loss: 0.000362854
DEBUG:root:[ Iteration 138 ] Training loss: 0.000332205
DEBUG:root:[ Iteration 140 ] Test loss: 0.000391012
DEBUG:root:[ Iteration 141 ] Training loss: 0.000350605
DEBUG:root:[ Iteration 144 ] Training loss: 0.000344698
DEBUG:root:[ Iteration 147 ] Training loss: 0.000422901
DEBUG:root:[ Iteration 150 ] Training loss: 0.000311595
DEBUG:root:[ Iteration 153 ] Training loss: 0.000397248
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-08-2016_10h47m04s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0633056
DEBUG:root:[ Iteration 0 ] Test loss: 0.0611109
DEBUG:root:[ Iteration 3 ] Training loss: 0.0393533
DEBUG:root:[ Iteration 6 ] Training loss: 0.0481903
DEBUG:root:[ Iteration 9 ] Training loss: 0.0278766
DEBUG:root:[ Iteration 12 ] Training loss: 0.0372899
DEBUG:root:[ Iteration 15 ] Training loss: 0.0242135
DEBUG:root:[ Iteration 18 ] Training loss: 0.0217141
DEBUG:root:[ Iteration 20 ] Test loss: 0.020033
DEBUG:root:[ Iteration 21 ] Training loss: 0.0174228
DEBUG:root:[ Iteration 24 ] Training loss: 0.0177526
DEBUG:root:[ Iteration 27 ] Training loss: 0.018594
DEBUG:root:[ Iteration 30 ] Training loss: 0.0176553
DEBUG:root:[ Iteration 33 ] Training loss: 0.016935
DEBUG:root:[ Iteration 36 ] Training loss: 0.0142386
DEBUG:root:[ Iteration 39 ] Training loss: 0.011551
DEBUG:root:[ Iteration 40 ] Test loss: 0.0109572
DEBUG:root:[ Iteration 42 ] Training loss: 0.0112263
DEBUG:root:[ Iteration 45 ] Training loss: 0.0131133
DEBUG:root:[ Iteration 48 ] Training loss: 0.00975648
DEBUG:root:[ Iteration 51 ] Training loss: 0.00924224
DEBUG:root:[ Iteration 54 ] Training loss: 0.0150375
DEBUG:root:[ Iteration 57 ] Training loss: 0.00859334
DEBUG:root:[ Iteration 60 ] Training loss: 0.0108723
DEBUG:root:[ Iteration 60 ] Test loss: 0.0106947
DEBUG:root:[ Iteration 63 ] Training loss: 0.0102349
DEBUG:root:[ Iteration 66 ] Training loss: 0.00904348
DEBUG:root:[ Iteration 69 ] Training loss: 0.0102998
DEBUG:root:[ Iteration 72 ] Training loss: 0.0126704
DEBUG:root:[ Iteration 75 ] Training loss: 0.00793587
DEBUG:root:[ Iteration 78 ] Training loss: 0.0102298
DEBUG:root:[ Iteration 80 ] Test loss: 0.00913008
DEBUG:root:[ Iteration 81 ] Training loss: 0.0115272
DEBUG:root:[ Iteration 84 ] Training loss: 0.0114884
DEBUG:root:[ Iteration 87 ] Training loss: 0.00797099
DEBUG:root:[ Iteration 90 ] Training loss: 0.00765074
DEBUG:root:[ Iteration 93 ] Training loss: 0.0111341
DEBUG:root:[ Iteration 96 ] Training loss: 0.00663243
DEBUG:root:[ Iteration 99 ] Training loss: 0.0091398
DEBUG:root:[ Iteration 100 ] Test loss: 0.00786677
DEBUG:root:[ Iteration 102 ] Training loss: 0.00593233
DEBUG:root:[ Iteration 105 ] Training loss: 0.00856401
DEBUG:root:[ Iteration 108 ] Training loss: 0.00527192
DEBUG:root:[ Iteration 111 ] Training loss: 0.00717423
DEBUG:root:[ Iteration 114 ] Training loss: 0.00672574
DEBUG:root:[ Iteration 117 ] Training loss: 0.00683391
DEBUG:root:[ Iteration 120 ] Training loss: 0.00562382
DEBUG:root:[ Iteration 120 ] Test loss: 0.00583741
DEBUG:root:[ Iteration 123 ] Training loss: 0.00929651
DEBUG:root:[ Iteration 126 ] Training loss: 0.00548356
DEBUG:root:[ Iteration 129 ] Training loss: 0.00497016
DEBUG:root:[ Iteration 132 ] Training loss: 0.00813163
DEBUG:root:[ Iteration 135 ] Training loss: 0.00713876
DEBUG:root:[ Iteration 138 ] Training loss: 0.0082237
DEBUG:root:[ Iteration 140 ] Test loss: 0.00432538
DEBUG:root:[ Iteration 141 ] Training loss: 0.00494348
DEBUG:root:[ Iteration 144 ] Training loss: 0.00565788
DEBUG:root:[ Iteration 147 ] Training loss: 0.00705188
DEBUG:root:[ Iteration 150 ] Training loss: 0.0072846
DEBUG:root:[ Iteration 153 ] Training loss: 0.00482347
DEBUG:root:[ Iteration 156 ] Training loss: 0.00629151
DEBUG:root:[ Iteration 159 ] Training loss: 0.0055021
DEBUG:root:[ Iteration 160 ] Test loss: 0.00474149
DEBUG:root:[ Iteration 162 ] Training loss: 0.00405428
DEBUG:root:[ Iteration 165 ] Training loss: 0.00618627
DEBUG:root:[ Iteration 168 ] Training loss: 0.00510708
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-08-2016_11h42m45s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0411098
DEBUG:root:[ Iteration 0 ] Test loss: 0.0409544
DEBUG:root:[ Iteration 3 ] Training loss: 0.0414749
DEBUG:root:[ Iteration 6 ] Training loss: 0.0514925
DEBUG:root:[ Iteration 9 ] Training loss: 0.0386669
DEBUG:root:[ Iteration 12 ] Training loss: 0.0267182
DEBUG:root:[ Iteration 15 ] Training loss: 0.0274559
DEBUG:root:[ Iteration 18 ] Training loss: 0.0267354
DEBUG:root:[ Iteration 20 ] Test loss: 0.0278454
DEBUG:root:[ Iteration 21 ] Training loss: 0.0236709
DEBUG:root:[ Iteration 24 ] Training loss: 0.0339255
DEBUG:root:[ Iteration 27 ] Training loss: 0.0191235
DEBUG:root:[ Iteration 30 ] Training loss: 0.0207675
DEBUG:root:[ Iteration 33 ] Training loss: 0.0191983
DEBUG:root:[ Iteration 36 ] Training loss: 0.0307829
DEBUG:root:[ Iteration 39 ] Training loss: 0.0267343
DEBUG:root:[ Iteration 40 ] Test loss: 0.0200174
DEBUG:root:[ Iteration 42 ] Training loss: 0.0234519
DEBUG:root:[ Iteration 45 ] Training loss: 0.0185197
DEBUG:root:[ Iteration 48 ] Training loss: 0.018264
DEBUG:root:[ Iteration 51 ] Training loss: 0.0156544
DEBUG:root:[ Iteration 54 ] Training loss: 0.017402
DEBUG:root:[ Iteration 57 ] Training loss: 0.025122
DEBUG:root:[ Iteration 60 ] Training loss: 0.0210156
DEBUG:root:[ Iteration 60 ] Test loss: 0.0200498
DEBUG:root:[ Iteration 63 ] Training loss: 0.0155184
DEBUG:root:[ Iteration 66 ] Training loss: 0.018031
DEBUG:root:[ Iteration 69 ] Training loss: 0.020549
DEBUG:root:[ Iteration 72 ] Training loss: 0.0150264
DEBUG:root:[ Iteration 75 ] Training loss: 0.0143617
DEBUG:root:[ Iteration 78 ] Training loss: 0.0136806
DEBUG:root:[ Iteration 80 ] Test loss: 0.0180365
DEBUG:root:[ Iteration 81 ] Training loss: 0.0143071
DEBUG:root:[ Iteration 84 ] Training loss: 0.0107026
DEBUG:root:[ Iteration 87 ] Training loss: 0.0181067
DEBUG:root:[ Iteration 90 ] Training loss: 0.015001
DEBUG:root:[ Iteration 93 ] Training loss: 0.0118088
DEBUG:root:[ Iteration 96 ] Training loss: 0.0190959
DEBUG:root:[ Iteration 99 ] Training loss: 0.0154685
DEBUG:root:[ Iteration 100 ] Test loss: 0.012404
DEBUG:root:[ Iteration 102 ] Training loss: 0.00895467
DEBUG:root:[ Iteration 105 ] Training loss: 0.0121647
DEBUG:root:[ Iteration 108 ] Training loss: 0.0123702
DEBUG:root:[ Iteration 111 ] Training loss: 0.0140412
DEBUG:root:[ Iteration 114 ] Training loss: 0.0117226
DEBUG:root:[ Iteration 117 ] Training loss: 0.0112894
DEBUG:root:[ Iteration 120 ] Training loss: 0.0139847
DEBUG:root:[ Iteration 120 ] Test loss: 0.0120231
DEBUG:root:[ Iteration 123 ] Training loss: 0.0133983
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-08-2016_12h47m31s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0330494
DEBUG:root:[ Iteration 0 ] Test loss: 0.0411519
DEBUG:root:[ Iteration 3 ] Training loss: 0.0246635
DEBUG:root:[ Iteration 6 ] Training loss: 0.0262974
DEBUG:root:[ Iteration 9 ] Training loss: 0.0237811
DEBUG:root:[ Iteration 12 ] Training loss: 0.0280589
DEBUG:root:[ Iteration 15 ] Training loss: 0.0333304
DEBUG:root:[ Iteration 18 ] Training loss: 0.022479
DEBUG:root:[ Iteration 20 ] Test loss: 0.0270071
DEBUG:root:[ Iteration 21 ] Training loss: 0.0223095
DEBUG:root:[ Iteration 24 ] Training loss: 0.0181623
DEBUG:root:[ Iteration 27 ] Training loss: 0.0274415
DEBUG:root:[ Iteration 30 ] Training loss: 0.0217787
DEBUG:root:[ Iteration 33 ] Training loss: 0.0211983
DEBUG:root:[ Iteration 36 ] Training loss: 0.0192261
DEBUG:root:[ Iteration 39 ] Training loss: 0.0166362
DEBUG:root:[ Iteration 40 ] Test loss: 0.0260684
DEBUG:root:[ Iteration 42 ] Training loss: 0.022661
DEBUG:root:[ Iteration 45 ] Training loss: 0.0258653
DEBUG:root:[ Iteration 48 ] Training loss: 0.022157
DEBUG:root:[ Iteration 51 ] Training loss: 0.0210748
DEBUG:root:[ Iteration 54 ] Training loss: 0.0215722
DEBUG:root:[ Iteration 57 ] Training loss: 0.0195184
DEBUG:root:[ Iteration 60 ] Training loss: 0.0199498
DEBUG:root:[ Iteration 60 ] Test loss: 0.0249665
DEBUG:root:[ Iteration 63 ] Training loss: 0.0155984
DEBUG:root:[ Iteration 66 ] Training loss: 0.0188723
DEBUG:root:[ Iteration 69 ] Training loss: 0.0197571
DEBUG:root:[ Iteration 72 ] Training loss: 0.0254559
DEBUG:root:[ Iteration 75 ] Training loss: 0.0182461
DEBUG:root:[ Iteration 78 ] Training loss: 0.022308
DEBUG:root:[ Iteration 80 ] Test loss: 0.0206919
DEBUG:root:[ Iteration 81 ] Training loss: 0.0146174
DEBUG:root:[ Iteration 84 ] Training loss: 0.0176712
DEBUG:root:[ Iteration 87 ] Training loss: 0.0181767
DEBUG:root:[ Iteration 90 ] Training loss: 0.0151645
DEBUG:root:[ Iteration 93 ] Training loss: 0.0155201
DEBUG:root:[ Iteration 96 ] Training loss: 0.019368
DEBUG:root:[ Iteration 99 ] Training loss: 0.0203628
DEBUG:root:[ Iteration 100 ] Test loss: 0.0234531
DEBUG:root:[ Iteration 102 ] Training loss: 0.0164408
DEBUG:root:[ Iteration 105 ] Training loss: 0.016303
DEBUG:root:[ Iteration 108 ] Training loss: 0.020497
DEBUG:root:[ Iteration 111 ] Training loss: 0.0194827
DEBUG:root:[ Iteration 114 ] Training loss: 0.0181897
DEBUG:root:[ Iteration 117 ] Training loss: 0.0167446
DEBUG:root:[ Iteration 120 ] Training loss: 0.0176838
DEBUG:root:[ Iteration 120 ] Test loss: 0.0194362
DEBUG:root:[ Iteration 123 ] Training loss: 0.0137079
DEBUG:root:[ Iteration 126 ] Training loss: 0.0176901
DEBUG:root:[ Iteration 129 ] Training loss: 0.0143034
DEBUG:root:[ Iteration 132 ] Training loss: 0.0134883
DEBUG:root:[ Iteration 135 ] Training loss: 0.0200286
DEBUG:root:[ Iteration 138 ] Training loss: 0.01323
DEBUG:root:[ Iteration 140 ] Test loss: 0.011587
DEBUG:root:[ Iteration 141 ] Training loss: 0.0148659
DEBUG:root:[ Iteration 144 ] Training loss: 0.0148684
DEBUG:root:[ Iteration 147 ] Training loss: 0.0193901
DEBUG:root:[ Iteration 150 ] Training loss: 0.0165171
DEBUG:root:[ Iteration 153 ] Training loss: 0.0126088
DEBUG:root:[ Iteration 156 ] Training loss: 0.0161437
DEBUG:root:[ Iteration 159 ] Training loss: 0.0205762
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-08-2016_16h52m44s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.139305
DEBUG:root:[ Iteration 0 ] Test loss: 0.138332
DEBUG:root:[ Iteration 3 ] Training loss: 0.132352
DEBUG:root:[ Iteration 6 ] Training loss: 0.125704
DEBUG:root:[ Iteration 9 ] Training loss: 0.105507
DEBUG:root:[ Iteration 12 ] Training loss: 0.0803545
DEBUG:root:[ Iteration 15 ] Training loss: 0.044723
DEBUG:root:[ Iteration 18 ] Training loss: 0.0231479
DEBUG:root:[ Iteration 20 ] Test loss: 0.0168865
DEBUG:root:[ Iteration 21 ] Training loss: 0.0175852
DEBUG:root:[ Iteration 24 ] Training loss: 0.0159117
DEBUG:root:[ Iteration 27 ] Training loss: 0.0139043
DEBUG:root:[ Iteration 30 ] Training loss: 0.0124703
DEBUG:root:[ Iteration 33 ] Training loss: 0.0114255
DEBUG:root:[ Iteration 36 ] Training loss: 0.00978
DEBUG:root:[ Iteration 39 ] Training loss: 0.00816541
DEBUG:root:[ Iteration 40 ] Test loss: 0.00697622
DEBUG:root:[ Iteration 42 ] Training loss: 0.00714587
DEBUG:root:[ Iteration 45 ] Training loss: 0.00612359
DEBUG:root:[ Iteration 48 ] Training loss: 0.00606824
DEBUG:root:[ Iteration 51 ] Training loss: 0.00235341
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-09-2016_12h06m48s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0423855
DEBUG:root:[ Iteration 0 ] Test loss: 0.0581983
DEBUG:root:[ Iteration 3 ] Training loss: 0.0705725
DEBUG:root:[ Iteration 6 ] Training loss: 0.0706251
DEBUG:root:[ Iteration 9 ] Training loss: 0.047174
DEBUG:root:[ Iteration 12 ] Training loss: 0.0504675
DEBUG:root:[ Iteration 15 ] Training loss: 0.0575042
DEBUG:root:[ Iteration 18 ] Training loss: 0.0460247
DEBUG:root:[ Iteration 20 ] Test loss: 0.0349259
DEBUG:root:[ Iteration 21 ] Training loss: 0.0339678
DEBUG:root:[ Iteration 24 ] Training loss: 0.0372463
DEBUG:root:[ Iteration 27 ] Training loss: 0.0411541
DEBUG:root:[ Iteration 30 ] Training loss: 0.0412918
DEBUG:root:[ Iteration 33 ] Training loss: 0.0317581
DEBUG:root:[ Iteration 36 ] Training loss: 0.0405576
DEBUG:root:[ Iteration 39 ] Training loss: 0.0322733
DEBUG:root:[ Iteration 40 ] Test loss: 0.0282878
DEBUG:root:[ Iteration 42 ] Training loss: 0.0378993
DEBUG:root:[ Iteration 45 ] Training loss: 0.0303309
DEBUG:root:[ Iteration 48 ] Training loss: 0.0308204
DEBUG:root:[ Iteration 51 ] Training loss: 0.0286659
DEBUG:root:[ Iteration 54 ] Training loss: 0.0242615
DEBUG:root:[ Iteration 57 ] Training loss: 0.0379214
DEBUG:root:[ Iteration 60 ] Training loss: 0.0260975
DEBUG:root:[ Iteration 60 ] Test loss: 0.0239333
DEBUG:root:[ Iteration 63 ] Training loss: 0.03328
DEBUG:root:[ Iteration 66 ] Training loss: 0.0309583
DEBUG:root:[ Iteration 69 ] Training loss: 0.0357536
DEBUG:root:[ Iteration 72 ] Training loss: 0.0348631
DEBUG:root:[ Iteration 75 ] Training loss: 0.0312423
DEBUG:root:[ Iteration 78 ] Training loss: 0.0331871
DEBUG:root:[ Iteration 80 ] Test loss: 0.0257134
DEBUG:root:[ Iteration 81 ] Training loss: 0.027885
DEBUG:root:[ Iteration 84 ] Training loss: 0.0289499
DEBUG:root:[ Iteration 87 ] Training loss: 0.0205534
DEBUG:root:[ Iteration 90 ] Training loss: 0.0354492
DEBUG:root:[ Iteration 93 ] Training loss: 0.0255631
DEBUG:root:[ Iteration 96 ] Training loss: 0.0204631
DEBUG:root:[ Iteration 99 ] Training loss: 0.0250303
DEBUG:root:[ Iteration 100 ] Test loss: 0.0142661
DEBUG:root:[ Iteration 102 ] Training loss: 0.0308968
DEBUG:root:[ Iteration 105 ] Training loss: 0.0287289
DEBUG:root:[ Iteration 108 ] Training loss: 0.025958
DEBUG:root:[ Iteration 111 ] Training loss: 0.0322873
DEBUG:root:[ Iteration 114 ] Training loss: 0.0230767
DEBUG:root:[ Iteration 117 ] Training loss: 0.0367107
DEBUG:root:[ Iteration 120 ] Training loss: 0.0254189
DEBUG:root:[ Iteration 120 ] Test loss: 0.0182244
DEBUG:root:[ Iteration 123 ] Training loss: 0.0207517
DEBUG:root:[ Iteration 126 ] Training loss: 0.0361482
DEBUG:root:[ Iteration 129 ] Training loss: 0.0267242
DEBUG:root:[ Iteration 132 ] Training loss: 0.0304751
DEBUG:root:[ Iteration 135 ] Training loss: 0.0223776
DEBUG:root:[ Iteration 138 ] Training loss: 0.0299326
DEBUG:root:[ Iteration 140 ] Test loss: 0.0193275
DEBUG:root:[ Iteration 141 ] Training loss: 0.0401979
DEBUG:root:[ Iteration 144 ] Training loss: 0.0306912
DEBUG:root:[ Iteration 147 ] Training loss: 0.0313946
DEBUG:root:[ Iteration 150 ] Training loss: 0.0287007
DEBUG:root:[ Iteration 153 ] Training loss: 0.0172787
DEBUG:root:[ Iteration 156 ] Training loss: 0.0265036
DEBUG:root:[ Iteration 159 ] Training loss: 0.0350437
DEBUG:root:[ Iteration 160 ] Test loss: 0.0202882
DEBUG:root:[ Iteration 162 ] Training loss: 0.0167048
DEBUG:root:[ Iteration 165 ] Training loss: 0.0289263
DEBUG:root:[ Iteration 168 ] Training loss: 0.0385315
DEBUG:root:[ Iteration 171 ] Training loss: 0.0265076
DEBUG:root:[ Iteration 174 ] Training loss: 0.0285746
DEBUG:root:[ Iteration 177 ] Training loss: 0.0287869
DEBUG:root:[ Iteration 180 ] Training loss: 0.0288245
DEBUG:root:[ Iteration 180 ] Test loss: 0.0196543
DEBUG:root:[ Iteration 183 ] Training loss: 0.0281997
DEBUG:root:[ Iteration 186 ] Training loss: 0.0361404
DEBUG:root:[ Iteration 189 ] Training loss: 0.0291449
DEBUG:root:[ Iteration 192 ] Training loss: 0.0280948
DEBUG:root:[ Iteration 195 ] Training loss: 0.0240603
DEBUG:root:[ Iteration 198 ] Training loss: 0.0250883
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-09-2016_12h55m28s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.171313
DEBUG:root:[ Iteration 0 ] Test loss: 0.169669
DEBUG:root:[ Iteration 3 ] Training loss: 0.166613
DEBUG:root:[ Iteration 6 ] Training loss: 0.169491
DEBUG:root:[ Iteration 9 ] Training loss: 0.166507
DEBUG:root:[ Iteration 12 ] Training loss: 0.160471
DEBUG:root:[ Iteration 15 ] Training loss: 0.160477
DEBUG:root:[ Iteration 18 ] Training loss: 0.149847
DEBUG:root:[ Iteration 20 ] Test loss: 0.150128
DEBUG:root:[ Iteration 21 ] Training loss: 0.145891
DEBUG:root:[ Iteration 24 ] Training loss: 0.136336
DEBUG:root:[ Iteration 27 ] Training loss: 0.120997
DEBUG:root:[ Iteration 30 ] Training loss: 0.113476
DEBUG:root:[ Iteration 33 ] Training loss: 0.0896366
DEBUG:root:[ Iteration 36 ] Training loss: 0.100053
DEBUG:root:[ Iteration 39 ] Training loss: 0.073777
DEBUG:root:[ Iteration 40 ] Test loss: 0.0908278
DEBUG:root:[ Iteration 42 ] Training loss: 0.0767145
DEBUG:root:[ Iteration 45 ] Training loss: 0.0922224
DEBUG:root:[ Iteration 48 ] Training loss: 0.0771898
DEBUG:root:[ Iteration 51 ] Training loss: 0.0895296
DEBUG:root:[ Iteration 54 ] Training loss: 0.0851118
DEBUG:root:[ Iteration 57 ] Training loss: 0.0908858
DEBUG:root:[ Iteration 60 ] Training loss: 0.0773256
DEBUG:root:[ Iteration 60 ] Test loss: 0.0827904
DEBUG:root:[ Iteration 63 ] Training loss: 0.074076
DEBUG:root:[ Iteration 66 ] Training loss: 0.0769671
DEBUG:root:[ Iteration 69 ] Training loss: 0.0645833
DEBUG:root:[ Iteration 72 ] Training loss: 0.0529368
DEBUG:root:[ Iteration 75 ] Training loss: 0.0542181
DEBUG:root:[ Iteration 78 ] Training loss: 0.0537168
DEBUG:root:[ Iteration 80 ] Test loss: 0.0596784
DEBUG:root:[ Iteration 81 ] Training loss: 0.0546198
DEBUG:root:[ Iteration 84 ] Training loss: 0.0601248
DEBUG:root:[ Iteration 87 ] Training loss: 0.0549865
DEBUG:root:[ Iteration 90 ] Training loss: 0.0432896
DEBUG:root:[ Iteration 93 ] Training loss: 0.0554396
DEBUG:root:[ Iteration 96 ] Training loss: 0.0619325
DEBUG:root:[ Iteration 99 ] Training loss: 0.0387073
DEBUG:root:[ Iteration 100 ] Test loss: 0.0478387
DEBUG:root:[ Iteration 102 ] Training loss: 0.055253
DEBUG:root:[ Iteration 105 ] Training loss: 0.035993
DEBUG:root:[ Iteration 108 ] Training loss: 0.049477
DEBUG:root:[ Iteration 111 ] Training loss: 0.0442811
DEBUG:root:[ Iteration 114 ] Training loss: 0.0376842
DEBUG:root:[ Iteration 117 ] Training loss: 0.0405261
DEBUG:root:[ Iteration 120 ] Training loss: 0.0287646
DEBUG:root:[ Iteration 120 ] Test loss: 0.0412909
DEBUG:root:[ Iteration 123 ] Training loss: 0.0443684
DEBUG:root:[ Iteration 126 ] Training loss: 0.0301102
DEBUG:root:[ Iteration 129 ] Training loss: 0.0339872
DEBUG:root:[ Iteration 132 ] Training loss: 0.0371042
DEBUG:root:[ Iteration 135 ] Training loss: 0.0309691
DEBUG:root:[ Iteration 138 ] Training loss: 0.0263448
DEBUG:root:[ Iteration 140 ] Test loss: 0.0323575
DEBUG:root:[ Iteration 141 ] Training loss: 0.0239361
DEBUG:root:[ Iteration 144 ] Training loss: 0.0264504
DEBUG:root:[ Iteration 147 ] Training loss: 0.0314116
DEBUG:root:[ Iteration 150 ] Training loss: 0.025777
DEBUG:root:[ Iteration 153 ] Training loss: 0.020405
DEBUG:root:[ Iteration 156 ] Training loss: 0.0285745
DEBUG:root:[ Iteration 159 ] Training loss: 0.022509
DEBUG:root:[ Iteration 160 ] Test loss: 0.0230867
DEBUG:root:[ Iteration 162 ] Training loss: 0.022042
DEBUG:root:[ Iteration 165 ] Training loss: 0.0255618
DEBUG:root:[ Iteration 168 ] Training loss: 0.0184741
DEBUG:root:[ Iteration 171 ] Training loss: 0.0226139
DEBUG:root:[ Iteration 174 ] Training loss: 0.0301351
DEBUG:root:[ Iteration 177 ] Training loss: 0.0184666
DEBUG:root:[ Iteration 180 ] Training loss: 0.0214007
DEBUG:root:[ Iteration 180 ] Test loss: 0.0257123
DEBUG:root:[ Iteration 183 ] Training loss: 0.0270149
DEBUG:root:[ Iteration 186 ] Training loss: 0.0222225
DEBUG:root:[ Iteration 189 ] Training loss: 0.0194423
DEBUG:root:[ Iteration 192 ] Training loss: 0.0222544
DEBUG:root:[ Iteration 195 ] Training loss: 0.0240385
DEBUG:root:[ Iteration 198 ] Training loss: 0.0230227
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-09-2016_13h40m41s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0176891
DEBUG:root:[ Iteration 0 ] Test loss: 0.0150241
DEBUG:root:[ Iteration 3 ] Training loss: 0.0146632
DEBUG:root:[ Iteration 6 ] Training loss: 0.0167629
DEBUG:root:[ Iteration 9 ] Training loss: 0.0227433
DEBUG:root:[ Iteration 12 ] Training loss: 0.018087
DEBUG:root:[ Iteration 15 ] Training loss: 0.0176282
DEBUG:root:[ Iteration 18 ] Training loss: 0.0121583
DEBUG:root:[ Iteration 20 ] Test loss: 0.0209251
DEBUG:root:[ Iteration 21 ] Training loss: 0.0122915
DEBUG:root:[ Iteration 24 ] Training loss: 0.0184624
DEBUG:root:[ Iteration 27 ] Training loss: 0.0145651
DEBUG:root:[ Iteration 30 ] Training loss: 0.0118689
DEBUG:root:[ Iteration 33 ] Training loss: 0.0159928
DEBUG:root:[ Iteration 36 ] Training loss: 0.018332
DEBUG:root:[ Iteration 39 ] Training loss: 0.0156052
DEBUG:root:[ Iteration 40 ] Test loss: 0.0149954
DEBUG:root:[ Iteration 42 ] Training loss: 0.0134239
DEBUG:root:[ Iteration 45 ] Training loss: 0.016827
DEBUG:root:[ Iteration 48 ] Training loss: 0.014497
DEBUG:root:[ Iteration 51 ] Training loss: 0.0142503
DEBUG:root:[ Iteration 54 ] Training loss: 0.0160454
DEBUG:root:[ Iteration 57 ] Training loss: 0.0156692
DEBUG:root:[ Iteration 60 ] Training loss: 0.0131218
DEBUG:root:[ Iteration 60 ] Test loss: 0.0162019
DEBUG:root:[ Iteration 63 ] Training loss: 0.0143417
DEBUG:root:[ Iteration 66 ] Training loss: 0.012595
DEBUG:root:[ Iteration 69 ] Training loss: 0.00906198
DEBUG:root:[ Iteration 72 ] Training loss: 0.0141099
DEBUG:root:[ Iteration 75 ] Training loss: 0.0123398
DEBUG:root:[ Iteration 78 ] Training loss: 0.0106629
DEBUG:root:[ Iteration 80 ] Test loss: 0.0153539
DEBUG:root:[ Iteration 81 ] Training loss: 0.0134404
DEBUG:root:[ Iteration 84 ] Training loss: 0.00824943
DEBUG:root:[ Iteration 87 ] Training loss: 0.0173725
DEBUG:root:[ Iteration 90 ] Training loss: 0.0137843
DEBUG:root:[ Iteration 93 ] Training loss: 0.0116599
DEBUG:root:[ Iteration 96 ] Training loss: 0.0133563
DEBUG:root:[ Iteration 99 ] Training loss: 0.0109396
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-09-2016_13h43m30s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0353186
DEBUG:root:[ Iteration 0 ] Test loss: 0.0353888
DEBUG:root:[ Iteration 3 ] Training loss: 0.0350915
DEBUG:root:[ Iteration 6 ] Training loss: 0.0286489
DEBUG:root:[ Iteration 9 ] Training loss: 0.0223771
DEBUG:root:[ Iteration 12 ] Training loss: 0.0296393
DEBUG:root:[ Iteration 15 ] Training loss: 0.0264027
DEBUG:root:[ Iteration 18 ] Training loss: 0.0245947
DEBUG:root:[ Iteration 20 ] Test loss: 0.0155622
DEBUG:root:[ Iteration 21 ] Training loss: 0.0286846
DEBUG:root:[ Iteration 24 ] Training loss: 0.0236708
DEBUG:root:[ Iteration 27 ] Training loss: 0.0204842
DEBUG:root:[ Iteration 30 ] Training loss: 0.0208239
DEBUG:root:[ Iteration 33 ] Training loss: 0.0183465
DEBUG:root:[ Iteration 36 ] Training loss: 0.0175968
DEBUG:root:[ Iteration 39 ] Training loss: 0.0176709
DEBUG:root:[ Iteration 40 ] Test loss: 0.0186461
DEBUG:root:[ Iteration 42 ] Training loss: 0.0158966
DEBUG:root:[ Iteration 45 ] Training loss: 0.0194217
DEBUG:root:[ Iteration 48 ] Training loss: 0.0157376
DEBUG:root:[ Iteration 51 ] Training loss: 0.0134694
DEBUG:root:[ Iteration 54 ] Training loss: 0.0152796
DEBUG:root:[ Iteration 57 ] Training loss: 0.0137442
DEBUG:root:[ Iteration 60 ] Training loss: 0.0170899
DEBUG:root:[ Iteration 60 ] Test loss: 0.0177785
DEBUG:root:[ Iteration 63 ] Training loss: 0.0213226
DEBUG:root:[ Iteration 66 ] Training loss: 0.0173851
DEBUG:root:[ Iteration 69 ] Training loss: 0.0145981
DEBUG:root:[ Iteration 72 ] Training loss: 0.0137943
DEBUG:root:[ Iteration 75 ] Training loss: 0.0118151
DEBUG:root:[ Iteration 78 ] Training loss: 0.0137179
DEBUG:root:[ Iteration 80 ] Test loss: 0.0168549
DEBUG:root:[ Iteration 81 ] Training loss: 0.0157311
DEBUG:root:[ Iteration 84 ] Training loss: 0.0137885
DEBUG:root:[ Iteration 87 ] Training loss: 0.0126331
DEBUG:root:[ Iteration 90 ] Training loss: 0.0185103
DEBUG:root:[ Iteration 93 ] Training loss: 0.0117464
DEBUG:root:[ Iteration 96 ] Training loss: 0.0193579
DEBUG:root:[ Iteration 99 ] Training loss: 0.0137722
DEBUG:root:[ Iteration 100 ] Test loss: 0.0145311
DEBUG:root:[ Iteration 102 ] Training loss: 0.0173723
DEBUG:root:[ Iteration 105 ] Training loss: 0.0153672
DEBUG:root:[ Iteration 108 ] Training loss: 0.0126953
DEBUG:root:[ Iteration 111 ] Training loss: 0.0175787
DEBUG:root:[ Iteration 114 ] Training loss: 0.0115454
DEBUG:root:[ Iteration 117 ] Training loss: 0.0116804
DEBUG:root:[ Iteration 120 ] Training loss: 0.0118016
DEBUG:root:[ Iteration 120 ] Test loss: 0.0155704
DEBUG:root:[ Iteration 123 ] Training loss: 0.0136833
DEBUG:root:[ Iteration 126 ] Training loss: 0.00843501
DEBUG:root:[ Iteration 129 ] Training loss: 0.00922135
DEBUG:root:[ Iteration 132 ] Training loss: 0.0100122
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-09-2016_15h06m54s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0115731
DEBUG:root:[ Iteration 0 ] Test loss: 0.0111512
DEBUG:root:[ Iteration 3 ] Training loss: 0.010365
DEBUG:root:[ Iteration 6 ] Training loss: 0.0108555
DEBUG:root:[ Iteration 9 ] Training loss: 0.0105512
DEBUG:root:[ Iteration 12 ] Training loss: 0.0104932
DEBUG:root:[ Iteration 15 ] Training loss: 0.0107666
DEBUG:root:[ Iteration 18 ] Training loss: 0.00871232
DEBUG:root:[ Iteration 20 ] Test loss: 0.0103047
DEBUG:root:[ Iteration 21 ] Training loss: 0.00972039
DEBUG:root:[ Iteration 24 ] Training loss: 0.00841455
DEBUG:root:[ Iteration 27 ] Training loss: 0.00756665
DEBUG:root:[ Iteration 30 ] Training loss: 0.00834343
DEBUG:root:[ Iteration 33 ] Training loss: 0.00818881
DEBUG:root:[ Iteration 36 ] Training loss: 0.00749849
DEBUG:root:[ Iteration 39 ] Training loss: 0.00731987
DEBUG:root:[ Iteration 40 ] Test loss: 0.00687959
DEBUG:root:[ Iteration 42 ] Training loss: 0.00743916
DEBUG:root:[ Iteration 45 ] Training loss: 0.00665126
DEBUG:root:[ Iteration 48 ] Training loss: 0.00630335
DEBUG:root:[ Iteration 51 ] Training loss: 0.00614361
DEBUG:root:[ Iteration 54 ] Training loss: 0.00542971
DEBUG:root:[ Iteration 57 ] Training loss: 0.00464933
DEBUG:root:[ Iteration 60 ] Training loss: 0.00497443
DEBUG:root:[ Iteration 60 ] Test loss: 0.00499773
DEBUG:root:[ Iteration 63 ] Training loss: 0.0051684
DEBUG:root:[ Iteration 66 ] Training loss: 0.00503122
DEBUG:root:[ Iteration 69 ] Training loss: 0.0052073
DEBUG:root:[ Iteration 72 ] Training loss: 0.00447019
DEBUG:root:[ Iteration 75 ] Training loss: 0.00499803
DEBUG:root:[ Iteration 78 ] Training loss: 0.00497223
DEBUG:root:[ Iteration 80 ] Test loss: 0.00457248
DEBUG:root:[ Iteration 81 ] Training loss: 0.00463528
DEBUG:root:[ Iteration 84 ] Training loss: 0.00456669
DEBUG:root:[ Iteration 87 ] Training loss: 0.00478062
DEBUG:root:[ Iteration 90 ] Training loss: 0.00455168
DEBUG:root:[ Iteration 93 ] Training loss: 0.00471953
DEBUG:root:[ Iteration 96 ] Training loss: 0.00454385
DEBUG:root:[ Iteration 99 ] Training loss: 0.00433097
DEBUG:root:[ Iteration 100 ] Test loss: 0.00447575
DEBUG:root:[ Iteration 102 ] Training loss: 0.00452119
DEBUG:root:[ Iteration 105 ] Training loss: 0.00437115
DEBUG:root:[ Iteration 108 ] Training loss: 0.00462451
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-10-2016_14h40m19s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.13322
DEBUG:root:[ Iteration 0 ] Test loss: 0.131688
DEBUG:root:[ Iteration 3 ] Training loss: 0.131959
DEBUG:root:[ Iteration 6 ] Training loss: 0.127122
DEBUG:root:[ Iteration 9 ] Training loss: 0.1285
DEBUG:root:[ Iteration 12 ] Training loss: 0.13038
DEBUG:root:[ Iteration 15 ] Training loss: 0.12488
DEBUG:root:[ Iteration 18 ] Training loss: 0.124605
DEBUG:root:[ Iteration 20 ] Test loss: 0.12262
DEBUG:root:[ Iteration 21 ] Training loss: 0.121752
DEBUG:root:[ Iteration 24 ] Training loss: 0.126075
DEBUG:root:[ Iteration 27 ] Training loss: 0.113547
DEBUG:root:[ Iteration 30 ] Training loss: 0.116915
DEBUG:root:[ Iteration 33 ] Training loss: 0.114448
DEBUG:root:[ Iteration 36 ] Training loss: 0.104709
DEBUG:root:[ Iteration 39 ] Training loss: 0.103292
DEBUG:root:[ Iteration 40 ] Test loss: 0.0914217
DEBUG:root:[ Iteration 42 ] Training loss: 0.0908931
DEBUG:root:[ Iteration 45 ] Training loss: 0.0911496
DEBUG:root:[ Iteration 48 ] Training loss: 0.0972984
DEBUG:root:[ Iteration 51 ] Training loss: 0.0955548
DEBUG:root:[ Iteration 54 ] Training loss: 0.0977337
DEBUG:root:[ Iteration 57 ] Training loss: 0.0882274
DEBUG:root:[ Iteration 60 ] Training loss: 0.0933484
DEBUG:root:[ Iteration 60 ] Test loss: 0.0874965
DEBUG:root:[ Iteration 63 ] Training loss: 0.0865334
DEBUG:root:[ Iteration 66 ] Training loss: 0.0869643
DEBUG:root:[ Iteration 69 ] Training loss: 0.0939863
DEBUG:root:[ Iteration 72 ] Training loss: 0.0888106
DEBUG:root:[ Iteration 75 ] Training loss: 0.0843936
DEBUG:root:[ Iteration 78 ] Training loss: 0.0900238
DEBUG:root:[ Iteration 80 ] Test loss: 0.0820577
DEBUG:root:[ Iteration 81 ] Training loss: 0.0857687
DEBUG:root:[ Iteration 84 ] Training loss: 0.0870835
DEBUG:root:[ Iteration 87 ] Training loss: 0.0826478
DEBUG:root:[ Iteration 90 ] Training loss: 0.080874
DEBUG:root:[ Iteration 93 ] Training loss: 0.0808022
DEBUG:root:[ Iteration 96 ] Training loss: 0.0813098
DEBUG:root:[ Iteration 99 ] Training loss: 0.0833369
DEBUG:root:[ Iteration 100 ] Test loss: 0.0750528
DEBUG:root:[ Iteration 102 ] Training loss: 0.0832069
DEBUG:root:[ Iteration 105 ] Training loss: 0.0760735
DEBUG:root:[ Iteration 108 ] Training loss: 0.0860084
DEBUG:root:[ Iteration 111 ] Training loss: 0.0850125
DEBUG:root:[ Iteration 114 ] Training loss: 0.0843159
DEBUG:root:[ Iteration 117 ] Training loss: 0.0900315
DEBUG:root:[ Iteration 120 ] Training loss: 0.0833056
DEBUG:root:[ Iteration 120 ] Test loss: 0.0744673
DEBUG:root:[ Iteration 123 ] Training loss: 0.0852883
DEBUG:root:[ Iteration 126 ] Training loss: 0.0814101
DEBUG:root:[ Iteration 129 ] Training loss: 0.0724391
DEBUG:root:[ Iteration 132 ] Training loss: 0.0841307
DEBUG:root:[ Iteration 135 ] Training loss: 0.0708264
DEBUG:root:[ Iteration 138 ] Training loss: 0.0828297
DEBUG:root:[ Iteration 140 ] Test loss: 0.0749197
DEBUG:root:[ Iteration 141 ] Training loss: 0.0734186
DEBUG:root:[ Iteration 144 ] Training loss: 0.0803925
DEBUG:root:[ Iteration 147 ] Training loss: 0.0709077
DEBUG:root:[ Iteration 150 ] Training loss: 0.0748457
DEBUG:root:[ Iteration 153 ] Training loss: 0.072474
DEBUG:root:[ Iteration 156 ] Training loss: 0.0739971
DEBUG:root:[ Iteration 159 ] Training loss: 0.0773933
DEBUG:root:[ Iteration 160 ] Test loss: 0.0726732
DEBUG:root:[ Iteration 162 ] Training loss: 0.0722277
DEBUG:root:[ Iteration 165 ] Training loss: 0.0732793
DEBUG:root:[ Iteration 168 ] Training loss: 0.0728019
DEBUG:root:[ Iteration 171 ] Training loss: 0.0744841
DEBUG:root:[ Iteration 174 ] Training loss: 0.0771585
DEBUG:root:[ Iteration 177 ] Training loss: 0.0680723
DEBUG:root:[ Iteration 180 ] Training loss: 0.0621634
DEBUG:root:[ Iteration 180 ] Test loss: 0.0682435
DEBUG:root:[ Iteration 183 ] Training loss: 0.065235
DEBUG:root:[ Iteration 186 ] Training loss: 0.0708908
DEBUG:root:[ Iteration 189 ] Training loss: 0.0721569
DEBUG:root:[ Iteration 192 ] Training loss: 0.0712501
DEBUG:root:[ Iteration 195 ] Training loss: 0.0707947
DEBUG:root:[ Iteration 198 ] Training loss: 0.0618389
DEBUG:root:[ Iteration 200 ] Test loss: 0.0660021
DEBUG:root:[ Iteration 201 ] Training loss: 0.0703647
DEBUG:root:[ Iteration 204 ] Training loss: 0.0662889
DEBUG:root:[ Iteration 207 ] Training loss: 0.0697104
DEBUG:root:[ Iteration 210 ] Training loss: 0.0592236
DEBUG:root:[ Iteration 213 ] Training loss: 0.0591496
DEBUG:root:[ Iteration 216 ] Training loss: 0.0564729
DEBUG:root:[ Iteration 219 ] Training loss: 0.0615697
DEBUG:root:[ Iteration 220 ] Test loss: 0.0643039
DEBUG:root:[ Iteration 222 ] Training loss: 0.0640424
DEBUG:root:[ Iteration 225 ] Training loss: 0.0609425
DEBUG:root:[ Iteration 228 ] Training loss: 0.0659691
DEBUG:root:[ Iteration 231 ] Training loss: 0.0541631
DEBUG:root:[ Iteration 234 ] Training loss: 0.0583971
DEBUG:root:[ Iteration 237 ] Training loss: 0.0638487
DEBUG:root:[ Iteration 240 ] Training loss: 0.0636083
DEBUG:root:[ Iteration 240 ] Test loss: 0.0630863
DEBUG:root:[ Iteration 243 ] Training loss: 0.0627268
DEBUG:root:[ Iteration 246 ] Training loss: 0.0613967
DEBUG:root:[ Iteration 249 ] Training loss: 0.0638785
DEBUG:root:[ Iteration 252 ] Training loss: 0.0589793
DEBUG:root:[ Iteration 255 ] Training loss: 0.059473
DEBUG:root:[ Iteration 258 ] Training loss: 0.0558671
DEBUG:root:[ Iteration 260 ] Test loss: 0.0687311
DEBUG:root:[ Iteration 261 ] Training loss: 0.0571143
DEBUG:root:[ Iteration 264 ] Training loss: 0.0616822
DEBUG:root:[ Iteration 267 ] Training loss: 0.065395
DEBUG:root:[ Iteration 270 ] Training loss: 0.0574051
DEBUG:root:[ Iteration 273 ] Training loss: 0.0610119
DEBUG:root:[ Iteration 276 ] Training loss: 0.0527855
DEBUG:root:[ Iteration 279 ] Training loss: 0.0555951
DEBUG:root:[ Iteration 280 ] Test loss: 0.0701569
DEBUG:root:[ Iteration 282 ] Training loss: 0.0585208
DEBUG:root:[ Iteration 285 ] Training loss: 0.0521783
DEBUG:root:[ Iteration 288 ] Training loss: 0.0570138
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-10-2016_15h21m11s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.150246
DEBUG:root:[ Iteration 0 ] Test loss: 0.146832
DEBUG:root:[ Iteration 3 ] Training loss: 0.149584
DEBUG:root:[ Iteration 6 ] Training loss: 0.148739
DEBUG:root:[ Iteration 9 ] Training loss: 0.14767
DEBUG:root:[ Iteration 12 ] Training loss: 0.144284
DEBUG:root:[ Iteration 15 ] Training loss: 0.1375
DEBUG:root:[ Iteration 18 ] Training loss: 0.13437
DEBUG:root:[ Iteration 20 ] Test loss: 0.136547
DEBUG:root:[ Iteration 21 ] Training loss: 0.129819
DEBUG:root:[ Iteration 24 ] Training loss: 0.125127
DEBUG:root:[ Iteration 27 ] Training loss: 0.122751
DEBUG:root:[ Iteration 30 ] Training loss: 0.114158
DEBUG:root:[ Iteration 33 ] Training loss: 0.102513
DEBUG:root:[ Iteration 36 ] Training loss: 0.0823134
DEBUG:root:[ Iteration 39 ] Training loss: 0.0626972
DEBUG:root:[ Iteration 40 ] Test loss: 0.0635769
DEBUG:root:[ Iteration 42 ] Training loss: 0.0617072
DEBUG:root:[ Iteration 45 ] Training loss: 0.0743914
DEBUG:root:[ Iteration 48 ] Training loss: 0.0643771
DEBUG:root:[ Iteration 51 ] Training loss: 0.0496675
DEBUG:root:[ Iteration 54 ] Training loss: 0.0753914
DEBUG:root:[ Iteration 57 ] Training loss: 0.055137
DEBUG:root:[ Iteration 60 ] Training loss: 0.0653044
DEBUG:root:[ Iteration 60 ] Test loss: 0.0683643
DEBUG:root:[ Iteration 63 ] Training loss: 0.0615955
DEBUG:root:[ Iteration 66 ] Training loss: 0.0652141
DEBUG:root:[ Iteration 69 ] Training loss: 0.0593132
DEBUG:root:[ Iteration 72 ] Training loss: 0.0632649
DEBUG:root:[ Iteration 75 ] Training loss: 0.0634363
DEBUG:root:[ Iteration 78 ] Training loss: 0.0676539
DEBUG:root:[ Iteration 80 ] Test loss: 0.0537951
DEBUG:root:[ Iteration 81 ] Training loss: 0.0542828
DEBUG:root:[ Iteration 84 ] Training loss: 0.0530041
DEBUG:root:[ Iteration 87 ] Training loss: 0.0692587
DEBUG:root:[ Iteration 90 ] Training loss: 0.0585179
DEBUG:root:[ Iteration 93 ] Training loss: 0.0589772
DEBUG:root:[ Iteration 96 ] Training loss: 0.0548083
DEBUG:root:[ Iteration 99 ] Training loss: 0.0558321
DEBUG:root:[ Iteration 100 ] Test loss: 0.0537185
DEBUG:root:[ Iteration 102 ] Training loss: 0.05882
DEBUG:root:[ Iteration 105 ] Training loss: 0.0510118
DEBUG:root:[ Iteration 108 ] Training loss: 0.05831
DEBUG:root:[ Iteration 111 ] Training loss: 0.0583686
DEBUG:root:[ Iteration 114 ] Training loss: 0.0537176
DEBUG:root:[ Iteration 117 ] Training loss: 0.0513921
DEBUG:root:[ Iteration 120 ] Training loss: 0.0513384
DEBUG:root:[ Iteration 120 ] Test loss: 0.047215
DEBUG:root:[ Iteration 123 ] Training loss: 0.0505698
DEBUG:root:[ Iteration 126 ] Training loss: 0.046324
DEBUG:root:[ Iteration 129 ] Training loss: 0.0430017
DEBUG:root:[ Iteration 132 ] Training loss: 0.0466947
DEBUG:root:[ Iteration 135 ] Training loss: 0.0510892
DEBUG:root:[ Iteration 138 ] Training loss: 0.0432681
DEBUG:root:[ Iteration 140 ] Test loss: 0.0496926
DEBUG:root:[ Iteration 141 ] Training loss: 0.0490842
DEBUG:root:[ Iteration 144 ] Training loss: 0.0442238
DEBUG:root:[ Iteration 147 ] Training loss: 0.0455015
DEBUG:root:[ Iteration 150 ] Training loss: 0.0471565
DEBUG:root:[ Iteration 153 ] Training loss: 0.043528
DEBUG:root:[ Iteration 156 ] Training loss: 0.0406864
DEBUG:root:[ Iteration 159 ] Training loss: 0.0394432
DEBUG:root:[ Iteration 160 ] Test loss: 0.0409207
DEBUG:root:[ Iteration 162 ] Training loss: 0.0472269
DEBUG:root:[ Iteration 165 ] Training loss: 0.0406716
DEBUG:root:[ Iteration 168 ] Training loss: 0.0406487
DEBUG:root:[ Iteration 171 ] Training loss: 0.0422897
DEBUG:root:[ Iteration 174 ] Training loss: 0.0351922
DEBUG:root:[ Iteration 177 ] Training loss: 0.0335669
DEBUG:root:[ Iteration 180 ] Training loss: 0.0331706
DEBUG:root:[ Iteration 180 ] Test loss: 0.0395033
DEBUG:root:[ Iteration 183 ] Training loss: 0.0329622
DEBUG:root:[ Iteration 186 ] Training loss: 0.0343221
DEBUG:root:[ Iteration 189 ] Training loss: 0.0357282
DEBUG:root:[ Iteration 192 ] Training loss: 0.0371089
DEBUG:root:[ Iteration 195 ] Training loss: 0.0370843
DEBUG:root:[ Iteration 198 ] Training loss: 0.0361106
DEBUG:root:[ Iteration 200 ] Test loss: 0.0367661
DEBUG:root:[ Iteration 201 ] Training loss: 0.0316623
DEBUG:root:[ Iteration 204 ] Training loss: 0.0342653
DEBUG:root:[ Iteration 207 ] Training loss: 0.0325247
DEBUG:root:[ Iteration 210 ] Training loss: 0.033698
DEBUG:root:[ Iteration 213 ] Training loss: 0.0307165
DEBUG:root:[ Iteration 216 ] Training loss: 0.0264242
DEBUG:root:[ Iteration 219 ] Training loss: 0.0283767
DEBUG:root:[ Iteration 220 ] Test loss: 0.0346827
DEBUG:root:[ Iteration 222 ] Training loss: 0.0279931
DEBUG:root:[ Iteration 225 ] Training loss: 0.0265781
DEBUG:root:[ Iteration 228 ] Training loss: 0.0267914
DEBUG:root:[ Iteration 231 ] Training loss: 0.0262697
DEBUG:root:[ Iteration 234 ] Training loss: 0.0271596
DEBUG:root:[ Iteration 237 ] Training loss: 0.0275
DEBUG:root:[ Iteration 240 ] Training loss: 0.0262478
DEBUG:root:[ Iteration 240 ] Test loss: 0.0321923
DEBUG:root:[ Iteration 243 ] Training loss: 0.0262739
DEBUG:root:[ Iteration 246 ] Training loss: 0.027308
DEBUG:root:[ Iteration 249 ] Training loss: 0.0245518
DEBUG:root:[ Iteration 252 ] Training loss: 0.0277169
DEBUG:root:[ Iteration 255 ] Training loss: 0.0232038
DEBUG:root:[ Iteration 258 ] Training loss: 0.0265549
DEBUG:root:[ Iteration 260 ] Test loss: 0.0336683
DEBUG:root:[ Iteration 261 ] Training loss: 0.0261464
DEBUG:root:[ Iteration 264 ] Training loss: 0.0239714
DEBUG:root:[ Iteration 267 ] Training loss: 0.0258964
DEBUG:root:[ Iteration 270 ] Training loss: 0.0225151
DEBUG:root:[ Iteration 273 ] Training loss: 0.0258546
DEBUG:root:[ Iteration 276 ] Training loss: 0.0209039
DEBUG:root:[ Iteration 279 ] Training loss: 0.0225422
DEBUG:root:[ Iteration 280 ] Test loss: 0.0296168
DEBUG:root:[ Iteration 282 ] Training loss: 0.0235005
DEBUG:root:[ Iteration 285 ] Training loss: 0.0246907
DEBUG:root:[ Iteration 288 ] Training loss: 0.0214211
DEBUG:root:[ Iteration 291 ] Training loss: 0.0268978
DEBUG:root:[ Iteration 294 ] Training loss: 0.0236805
DEBUG:root:[ Iteration 297 ] Training loss: 0.0233783
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-13-2016_11h54m55s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0244247
DEBUG:root:[ Iteration 0 ] Test loss: 0.0289758
DEBUG:root:[ Iteration 3 ] Training loss: 0.0197738
DEBUG:root:[ Iteration 6 ] Training loss: 0.0214905
DEBUG:root:[ Iteration 9 ] Training loss: 0.0239662
DEBUG:root:[ Iteration 12 ] Training loss: 0.019543
DEBUG:root:[ Iteration 15 ] Training loss: 0.0197569
DEBUG:root:[ Iteration 18 ] Training loss: 0.0225379
DEBUG:root:[ Iteration 20 ] Test loss: 0.0283219
DEBUG:root:[ Iteration 21 ] Training loss: 0.019855
DEBUG:root:[ Iteration 24 ] Training loss: 0.0184005
DEBUG:root:[ Iteration 27 ] Training loss: 0.0212202
DEBUG:root:[ Iteration 30 ] Training loss: 0.0190334
DEBUG:root:[ Iteration 33 ] Training loss: 0.021383
DEBUG:root:[ Iteration 36 ] Training loss: 0.0171973
DEBUG:root:[ Iteration 39 ] Training loss: 0.0183537
DEBUG:root:[ Iteration 40 ] Test loss: 0.0263845
DEBUG:root:[ Iteration 42 ] Training loss: 0.0197126
DEBUG:root:[ Iteration 45 ] Training loss: 0.0183117
DEBUG:root:[ Iteration 48 ] Training loss: 0.0172607
DEBUG:root:[ Iteration 51 ] Training loss: 0.0150023
DEBUG:root:[ Iteration 54 ] Training loss: 0.0184251
DEBUG:root:[ Iteration 57 ] Training loss: 0.0181196
DEBUG:root:[ Iteration 60 ] Training loss: 0.0154024
DEBUG:root:[ Iteration 60 ] Test loss: 0.0249091
DEBUG:root:[ Iteration 63 ] Training loss: 0.0152875
DEBUG:root:[ Iteration 66 ] Training loss: 0.0152759
DEBUG:root:[ Iteration 69 ] Training loss: 0.0144764
DEBUG:root:[ Iteration 72 ] Training loss: 0.0172121
DEBUG:root:[ Iteration 75 ] Training loss: 0.0166283
DEBUG:root:[ Iteration 78 ] Training loss: 0.0177069
DEBUG:root:[ Iteration 80 ] Test loss: 0.0269294
DEBUG:root:[ Iteration 81 ] Training loss: 0.014626
DEBUG:root:[ Iteration 84 ] Training loss: 0.0146568
DEBUG:root:[ Iteration 87 ] Training loss: 0.0164922
DEBUG:root:[ Iteration 90 ] Training loss: 0.0169707
DEBUG:root:[ Iteration 93 ] Training loss: 0.0172233
DEBUG:root:[ Iteration 96 ] Training loss: 0.0169611
DEBUG:root:[ Iteration 99 ] Training loss: 0.0148835
DEBUG:root:[ Iteration 100 ] Test loss: 0.023647
DEBUG:root:[ Iteration 102 ] Training loss: 0.0135499
DEBUG:root:[ Iteration 105 ] Training loss: 0.0152743
DEBUG:root:[ Iteration 108 ] Training loss: 0.0166212
DEBUG:root:[ Iteration 111 ] Training loss: 0.0154826
DEBUG:root:[ Iteration 114 ] Training loss: 0.0146449
DEBUG:root:[ Iteration 117 ] Training loss: 0.0154052
DEBUG:root:[ Iteration 120 ] Training loss: 0.0133201
DEBUG:root:[ Iteration 120 ] Test loss: 0.0257768
DEBUG:root:[ Iteration 123 ] Training loss: 0.017412
DEBUG:root:[ Iteration 126 ] Training loss: 0.0147734
DEBUG:root:[ Iteration 129 ] Training loss: 0.0129811
DEBUG:root:[ Iteration 132 ] Training loss: 0.0148457
DEBUG:root:[ Iteration 135 ] Training loss: 0.0123783
DEBUG:root:[ Iteration 138 ] Training loss: 0.0159636
DEBUG:root:[ Iteration 140 ] Test loss: 0.0213074
DEBUG:root:[ Iteration 141 ] Training loss: 0.0125719
DEBUG:root:[ Iteration 144 ] Training loss: 0.0117623
DEBUG:root:[ Iteration 147 ] Training loss: 0.0135324
DEBUG:root:[ Iteration 150 ] Training loss: 0.0119433
DEBUG:root:[ Iteration 153 ] Training loss: 0.0135003
DEBUG:root:[ Iteration 156 ] Training loss: 0.0127494
DEBUG:root:[ Iteration 159 ] Training loss: 0.0121179
DEBUG:root:[ Iteration 160 ] Test loss: 0.0223221
DEBUG:root:[ Iteration 162 ] Training loss: 0.0126409
DEBUG:root:[ Iteration 165 ] Training loss: 0.0112178
DEBUG:root:[ Iteration 168 ] Training loss: 0.012773
DEBUG:root:[ Iteration 171 ] Training loss: 0.0114786
DEBUG:root:[ Iteration 174 ] Training loss: 0.0115332
DEBUG:root:[ Iteration 177 ] Training loss: 0.00905976
DEBUG:root:[ Iteration 180 ] Training loss: 0.0123469
DEBUG:root:[ Iteration 180 ] Test loss: 0.0200126
DEBUG:root:[ Iteration 183 ] Training loss: 0.0133653
DEBUG:root:[ Iteration 186 ] Training loss: 0.0139454
DEBUG:root:[ Iteration 189 ] Training loss: 0.015893
DEBUG:root:[ Iteration 192 ] Training loss: 0.012877
DEBUG:root:[ Iteration 195 ] Training loss: 0.0133833
DEBUG:root:[ Iteration 198 ] Training loss: 0.0106175
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-13-2016_12h17m23s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.152024
DEBUG:root:[ Iteration 0 ] Test loss: 0.157427
DEBUG:root:[ Iteration 3 ] Training loss: 0.147294
DEBUG:root:[ Iteration 6 ] Training loss: 0.151835
DEBUG:root:[ Iteration 9 ] Training loss: 0.148266
DEBUG:root:[ Iteration 12 ] Training loss: 0.141192
DEBUG:root:[ Iteration 15 ] Training loss: 0.136488
DEBUG:root:[ Iteration 18 ] Training loss: 0.134105
DEBUG:root:[ Iteration 20 ] Test loss: 0.131014
DEBUG:root:[ Iteration 21 ] Training loss: 0.128851
DEBUG:root:[ Iteration 24 ] Training loss: 0.119836
DEBUG:root:[ Iteration 27 ] Training loss: 0.106895
DEBUG:root:[ Iteration 30 ] Training loss: 0.0888681
DEBUG:root:[ Iteration 33 ] Training loss: 0.0759713
DEBUG:root:[ Iteration 36 ] Training loss: 0.0726468
DEBUG:root:[ Iteration 39 ] Training loss: 0.0729225
DEBUG:root:[ Iteration 40 ] Test loss: 0.0763218
DEBUG:root:[ Iteration 42 ] Training loss: 0.0705229
DEBUG:root:[ Iteration 45 ] Training loss: 0.0719149
DEBUG:root:[ Iteration 48 ] Training loss: 0.07489
DEBUG:root:[ Iteration 51 ] Training loss: 0.0663556
DEBUG:root:[ Iteration 54 ] Training loss: 0.070959
DEBUG:root:[ Iteration 57 ] Training loss: 0.0634661
DEBUG:root:[ Iteration 60 ] Training loss: 0.0625479
DEBUG:root:[ Iteration 60 ] Test loss: 0.0638761
DEBUG:root:[ Iteration 63 ] Training loss: 0.0739134
DEBUG:root:[ Iteration 66 ] Training loss: 0.0523333
DEBUG:root:[ Iteration 69 ] Training loss: 0.0676989
DEBUG:root:[ Iteration 72 ] Training loss: 0.0643174
DEBUG:root:[ Iteration 75 ] Training loss: 0.0736765
DEBUG:root:[ Iteration 78 ] Training loss: 0.0709512
DEBUG:root:[ Iteration 80 ] Test loss: 0.0608228
DEBUG:root:[ Iteration 81 ] Training loss: 0.062875
DEBUG:root:[ Iteration 84 ] Training loss: 0.0606702
DEBUG:root:[ Iteration 87 ] Training loss: 0.0577065
DEBUG:root:[ Iteration 90 ] Training loss: 0.0600778
DEBUG:root:[ Iteration 93 ] Training loss: 0.059271
DEBUG:root:[ Iteration 96 ] Training loss: 0.0670035
DEBUG:root:[ Iteration 99 ] Training loss: 0.0590589
DEBUG:root:[ Iteration 100 ] Test loss: 0.0635573
DEBUG:root:[ Iteration 102 ] Training loss: 0.0644351
DEBUG:root:[ Iteration 105 ] Training loss: 0.0663681
DEBUG:root:[ Iteration 108 ] Training loss: 0.0597803
DEBUG:root:[ Iteration 111 ] Training loss: 0.0582331
DEBUG:root:[ Iteration 114 ] Training loss: 0.0603026
DEBUG:root:[ Iteration 117 ] Training loss: 0.0551262
DEBUG:root:[ Iteration 120 ] Training loss: 0.0582744
DEBUG:root:[ Iteration 120 ] Test loss: 0.054023
DEBUG:root:[ Iteration 123 ] Training loss: 0.0591887
DEBUG:root:[ Iteration 126 ] Training loss: 0.0480904
DEBUG:root:[ Iteration 129 ] Training loss: 0.0569274
DEBUG:root:[ Iteration 132 ] Training loss: 0.0523791
DEBUG:root:[ Iteration 135 ] Training loss: 0.0560008
DEBUG:root:[ Iteration 138 ] Training loss: 0.0528963
DEBUG:root:[ Iteration 140 ] Test loss: 0.0639431
DEBUG:root:[ Iteration 141 ] Training loss: 0.0543684
DEBUG:root:[ Iteration 144 ] Training loss: 0.0541302
DEBUG:root:[ Iteration 147 ] Training loss: 0.0513394
DEBUG:root:[ Iteration 150 ] Training loss: 0.0507949
DEBUG:root:[ Iteration 153 ] Training loss: 0.0515409
DEBUG:root:[ Iteration 156 ] Training loss: 0.0495156
DEBUG:root:[ Iteration 159 ] Training loss: 0.0486452
DEBUG:root:[ Iteration 160 ] Test loss: 0.0518129
DEBUG:root:[ Iteration 162 ] Training loss: 0.0458326
DEBUG:root:[ Iteration 165 ] Training loss: 0.0467191
DEBUG:root:[ Iteration 168 ] Training loss: 0.0474928
DEBUG:root:[ Iteration 171 ] Training loss: 0.0538881
DEBUG:root:[ Iteration 174 ] Training loss: 0.0537129
DEBUG:root:[ Iteration 177 ] Training loss: 0.0453533
DEBUG:root:[ Iteration 180 ] Training loss: 0.04266
DEBUG:root:[ Iteration 180 ] Test loss: 0.0501435
DEBUG:root:[ Iteration 183 ] Training loss: 0.0498107
DEBUG:root:[ Iteration 186 ] Training loss: 0.0468944
DEBUG:root:[ Iteration 189 ] Training loss: 0.0418738
DEBUG:root:[ Iteration 192 ] Training loss: 0.045877
DEBUG:root:[ Iteration 195 ] Training loss: 0.0463794
DEBUG:root:[ Iteration 198 ] Training loss: 0.0463926
DEBUG:root:[ Iteration 200 ] Test loss: 0.048044
DEBUG:root:[ Iteration 201 ] Training loss: 0.0409481
DEBUG:root:[ Iteration 204 ] Training loss: 0.0422229
DEBUG:root:[ Iteration 207 ] Training loss: 0.0434843
DEBUG:root:[ Iteration 210 ] Training loss: 0.048906
DEBUG:root:[ Iteration 213 ] Training loss: 0.036117
DEBUG:root:[ Iteration 216 ] Training loss: 0.0392399
DEBUG:root:[ Iteration 219 ] Training loss: 0.0478551
DEBUG:root:[ Iteration 220 ] Test loss: 0.0461369
DEBUG:root:[ Iteration 222 ] Training loss: 0.0364675
DEBUG:root:[ Iteration 225 ] Training loss: 0.0448526
DEBUG:root:[ Iteration 228 ] Training loss: 0.0339073
DEBUG:root:[ Iteration 231 ] Training loss: 0.047829
DEBUG:root:[ Iteration 234 ] Training loss: 0.0385305
DEBUG:root:[ Iteration 237 ] Training loss: 0.037892
DEBUG:root:[ Iteration 240 ] Training loss: 0.0401088
DEBUG:root:[ Iteration 240 ] Test loss: 0.0401152
DEBUG:root:[ Iteration 243 ] Training loss: 0.0384429
DEBUG:root:[ Iteration 246 ] Training loss: 0.0399362
DEBUG:root:[ Iteration 249 ] Training loss: 0.0338381
DEBUG:root:[ Iteration 252 ] Training loss: 0.0412011
DEBUG:root:[ Iteration 255 ] Training loss: 0.0354152
DEBUG:root:[ Iteration 258 ] Training loss: 0.0326978
DEBUG:root:[ Iteration 260 ] Test loss: 0.0386731
DEBUG:root:[ Iteration 261 ] Training loss: 0.0386858
DEBUG:root:[ Iteration 264 ] Training loss: 0.0389786
DEBUG:root:[ Iteration 267 ] Training loss: 0.0357428
DEBUG:root:[ Iteration 270 ] Training loss: 0.0333521
DEBUG:root:[ Iteration 273 ] Training loss: 0.0367363
DEBUG:root:[ Iteration 276 ] Training loss: 0.0383132
DEBUG:root:[ Iteration 279 ] Training loss: 0.0364039
DEBUG:root:[ Iteration 280 ] Test loss: 0.0326375
DEBUG:root:[ Iteration 282 ] Training loss: 0.0419367
DEBUG:root:[ Iteration 285 ] Training loss: 0.0293844
DEBUG:root:[ Iteration 288 ] Training loss: 0.0400745
DEBUG:root:[ Iteration 291 ] Training loss: 0.0380385
DEBUG:root:[ Iteration 294 ] Training loss: 0.0358882
DEBUG:root:[ Iteration 297 ] Training loss: 0.0329716
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-13-2016_13h13m49s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0337374
DEBUG:root:[ Iteration 0 ] Test loss: 0.0404838
DEBUG:root:[ Iteration 3 ] Training loss: 0.0356163
DEBUG:root:[ Iteration 6 ] Training loss: 0.0326001
DEBUG:root:[ Iteration 9 ] Training loss: 0.0315948
DEBUG:root:[ Iteration 12 ] Training loss: 0.0327872
DEBUG:root:[ Iteration 15 ] Training loss: 0.0276583
DEBUG:root:[ Iteration 18 ] Training loss: 0.0331584
DEBUG:root:[ Iteration 20 ] Test loss: 0.0356457
DEBUG:root:[ Iteration 21 ] Training loss: 0.0326075
DEBUG:root:[ Iteration 24 ] Training loss: 0.034783
DEBUG:root:[ Iteration 27 ] Training loss: 0.0313723
DEBUG:root:[ Iteration 30 ] Training loss: 0.0307393
DEBUG:root:[ Iteration 33 ] Training loss: 0.0270936
DEBUG:root:[ Iteration 36 ] Training loss: 0.0259251
DEBUG:root:[ Iteration 39 ] Training loss: 0.0307497
DEBUG:root:[ Iteration 40 ] Test loss: 0.0358435
DEBUG:root:[ Iteration 42 ] Training loss: 0.033966
DEBUG:root:[ Iteration 45 ] Training loss: 0.0345019
DEBUG:root:[ Iteration 48 ] Training loss: 0.0309723
DEBUG:root:[ Iteration 51 ] Training loss: 0.0306335
DEBUG:root:[ Iteration 54 ] Training loss: 0.0247025
DEBUG:root:[ Iteration 57 ] Training loss: 0.0301711
DEBUG:root:[ Iteration 60 ] Training loss: 0.0283371
DEBUG:root:[ Iteration 60 ] Test loss: 0.0326897
DEBUG:root:[ Iteration 63 ] Training loss: 0.0271996
DEBUG:root:[ Iteration 66 ] Training loss: 0.0284974
DEBUG:root:[ Iteration 69 ] Training loss: 0.026617
DEBUG:root:[ Iteration 72 ] Training loss: 0.0268393
DEBUG:root:[ Iteration 75 ] Training loss: 0.0316192
DEBUG:root:[ Iteration 78 ] Training loss: 0.0306085
DEBUG:root:[ Iteration 80 ] Test loss: 0.0344333
DEBUG:root:[ Iteration 81 ] Training loss: 0.0279061
DEBUG:root:[ Iteration 84 ] Training loss: 0.0283775
DEBUG:root:[ Iteration 87 ] Training loss: 0.0285932
DEBUG:root:[ Iteration 90 ] Training loss: 0.0235423
DEBUG:root:[ Iteration 93 ] Training loss: 0.0286527
DEBUG:root:[ Iteration 96 ] Training loss: 0.028271
DEBUG:root:[ Iteration 99 ] Training loss: 0.0272723
DEBUG:root:[ Iteration 100 ] Test loss: 0.0335177
DEBUG:root:[ Iteration 102 ] Training loss: 0.0261098
DEBUG:root:[ Iteration 105 ] Training loss: 0.0229492
DEBUG:root:[ Iteration 108 ] Training loss: 0.0256262
DEBUG:root:[ Iteration 111 ] Training loss: 0.0256963
DEBUG:root:[ Iteration 114 ] Training loss: 0.0250231
DEBUG:root:[ Iteration 117 ] Training loss: 0.023487
DEBUG:root:[ Iteration 120 ] Training loss: 0.0234066
DEBUG:root:[ Iteration 120 ] Test loss: 0.0334067
DEBUG:root:[ Iteration 123 ] Training loss: 0.0231896
DEBUG:root:[ Iteration 126 ] Training loss: 0.0241922
DEBUG:root:[ Iteration 129 ] Training loss: 0.0267202
DEBUG:root:[ Iteration 132 ] Training loss: 0.0212009
DEBUG:root:[ Iteration 135 ] Training loss: 0.0237937
DEBUG:root:[ Iteration 138 ] Training loss: 0.0207024
DEBUG:root:[ Iteration 140 ] Test loss: 0.0314558
DEBUG:root:[ Iteration 141 ] Training loss: 0.023444
DEBUG:root:[ Iteration 144 ] Training loss: 0.0187002
DEBUG:root:[ Iteration 147 ] Training loss: 0.0253957
DEBUG:root:[ Iteration 150 ] Training loss: 0.0212352
DEBUG:root:[ Iteration 153 ] Training loss: 0.0216823
DEBUG:root:[ Iteration 156 ] Training loss: 0.0210881
DEBUG:root:[ Iteration 159 ] Training loss: 0.0231387
DEBUG:root:[ Iteration 160 ] Test loss: 0.0298366
DEBUG:root:[ Iteration 162 ] Training loss: 0.0222921
DEBUG:root:[ Iteration 165 ] Training loss: 0.0243877
DEBUG:root:[ Iteration 168 ] Training loss: 0.0207201
DEBUG:root:[ Iteration 171 ] Training loss: 0.0231861
DEBUG:root:[ Iteration 174 ] Training loss: 0.0216385
DEBUG:root:[ Iteration 177 ] Training loss: 0.0192316
DEBUG:root:[ Iteration 180 ] Training loss: 0.020169
DEBUG:root:[ Iteration 180 ] Test loss: 0.0307461
DEBUG:root:[ Iteration 183 ] Training loss: 0.0229133
DEBUG:root:[ Iteration 186 ] Training loss: 0.0263117
DEBUG:root:[ Iteration 189 ] Training loss: 0.020761
DEBUG:root:[ Iteration 192 ] Training loss: 0.0194017
DEBUG:root:[ Iteration 195 ] Training loss: 0.0190549
DEBUG:root:[ Iteration 198 ] Training loss: 0.0196234
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-13-2016_13h57m24s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.156795
DEBUG:root:[ Iteration 0 ] Test loss: 0.152284
DEBUG:root:[ Iteration 3 ] Training loss: 0.157083
DEBUG:root:[ Iteration 6 ] Training loss: 0.152394
DEBUG:root:[ Iteration 9 ] Training loss: 0.150416
DEBUG:root:[ Iteration 12 ] Training loss: 0.146099
DEBUG:root:[ Iteration 15 ] Training loss: 0.142089
DEBUG:root:[ Iteration 18 ] Training loss: 0.136794
DEBUG:root:[ Iteration 20 ] Test loss: 0.130844
DEBUG:root:[ Iteration 21 ] Training loss: 0.126448
DEBUG:root:[ Iteration 24 ] Training loss: 0.10809
DEBUG:root:[ Iteration 27 ] Training loss: 0.0801799
DEBUG:root:[ Iteration 30 ] Training loss: 0.0617265
DEBUG:root:[ Iteration 33 ] Training loss: 0.0623102
DEBUG:root:[ Iteration 36 ] Training loss: 0.0690378
DEBUG:root:[ Iteration 39 ] Training loss: 0.0619836
DEBUG:root:[ Iteration 40 ] Test loss: 0.0637636
DEBUG:root:[ Iteration 42 ] Training loss: 0.0689252
DEBUG:root:[ Iteration 45 ] Training loss: 0.0662412
DEBUG:root:[ Iteration 48 ] Training loss: 0.0734645
DEBUG:root:[ Iteration 51 ] Training loss: 0.0636591
DEBUG:root:[ Iteration 54 ] Training loss: 0.0647078
DEBUG:root:[ Iteration 57 ] Training loss: 0.0522557
DEBUG:root:[ Iteration 60 ] Training loss: 0.066573
DEBUG:root:[ Iteration 60 ] Test loss: 0.0632347
DEBUG:root:[ Iteration 63 ] Training loss: 0.0591208
DEBUG:root:[ Iteration 66 ] Training loss: 0.0563598
DEBUG:root:[ Iteration 69 ] Training loss: 0.0556055
DEBUG:root:[ Iteration 72 ] Training loss: 0.0638589
DEBUG:root:[ Iteration 75 ] Training loss: 0.0570182
DEBUG:root:[ Iteration 78 ] Training loss: 0.0622112
DEBUG:root:[ Iteration 80 ] Test loss: 0.0539046
DEBUG:root:[ Iteration 81 ] Training loss: 0.0633709
DEBUG:root:[ Iteration 84 ] Training loss: 0.0639789
DEBUG:root:[ Iteration 87 ] Training loss: 0.0570664
DEBUG:root:[ Iteration 90 ] Training loss: 0.051398
DEBUG:root:[ Iteration 93 ] Training loss: 0.0605482
DEBUG:root:[ Iteration 96 ] Training loss: 0.0516401
DEBUG:root:[ Iteration 99 ] Training loss: 0.0453817
DEBUG:root:[ Iteration 100 ] Test loss: 0.0616803
DEBUG:root:[ Iteration 102 ] Training loss: 0.052373
DEBUG:root:[ Iteration 105 ] Training loss: 0.0594254
DEBUG:root:[ Iteration 108 ] Training loss: 0.0575091
DEBUG:root:[ Iteration 111 ] Training loss: 0.0505169
DEBUG:root:[ Iteration 114 ] Training loss: 0.0525019
DEBUG:root:[ Iteration 117 ] Training loss: 0.0645646
DEBUG:root:[ Iteration 120 ] Training loss: 0.0565102
DEBUG:root:[ Iteration 120 ] Test loss: 0.0536764
DEBUG:root:[ Iteration 123 ] Training loss: 0.0505065
DEBUG:root:[ Iteration 126 ] Training loss: 0.0507904
DEBUG:root:[ Iteration 129 ] Training loss: 0.0481979
DEBUG:root:[ Iteration 132 ] Training loss: 0.0545015
DEBUG:root:[ Iteration 135 ] Training loss: 0.0546022
DEBUG:root:[ Iteration 138 ] Training loss: 0.0534527
DEBUG:root:[ Iteration 140 ] Test loss: 0.055281
DEBUG:root:[ Iteration 141 ] Training loss: 0.0535638
DEBUG:root:[ Iteration 144 ] Training loss: 0.0500322
DEBUG:root:[ Iteration 147 ] Training loss: 0.0498137
DEBUG:root:[ Iteration 150 ] Training loss: 0.0495987
DEBUG:root:[ Iteration 153 ] Training loss: 0.0446012
DEBUG:root:[ Iteration 156 ] Training loss: 0.0478299
DEBUG:root:[ Iteration 159 ] Training loss: 0.0483169
DEBUG:root:[ Iteration 160 ] Test loss: 0.0435463
DEBUG:root:[ Iteration 162 ] Training loss: 0.0558594
DEBUG:root:[ Iteration 165 ] Training loss: 0.0477614
DEBUG:root:[ Iteration 168 ] Training loss: 0.0454489
DEBUG:root:[ Iteration 171 ] Training loss: 0.0467724
DEBUG:root:[ Iteration 174 ] Training loss: 0.0442283
DEBUG:root:[ Iteration 177 ] Training loss: 0.0505239
DEBUG:root:[ Iteration 180 ] Training loss: 0.0373202
DEBUG:root:[ Iteration 180 ] Test loss: 0.0430083
DEBUG:root:[ Iteration 183 ] Training loss: 0.0500768
DEBUG:root:[ Iteration 186 ] Training loss: 0.0478577
DEBUG:root:[ Iteration 189 ] Training loss: 0.0525204
DEBUG:root:[ Iteration 192 ] Training loss: 0.0515159
DEBUG:root:[ Iteration 195 ] Training loss: 0.0465639
DEBUG:root:[ Iteration 198 ] Training loss: 0.0430666
DEBUG:root:[ Iteration 200 ] Test loss: 0.0455675
DEBUG:root:[ Iteration 201 ] Training loss: 0.0414481
DEBUG:root:[ Iteration 204 ] Training loss: 0.0477823
DEBUG:root:[ Iteration 207 ] Training loss: 0.0556241
DEBUG:root:[ Iteration 210 ] Training loss: 0.0430939
DEBUG:root:[ Iteration 213 ] Training loss: 0.0382428
DEBUG:root:[ Iteration 216 ] Training loss: 0.0390262
DEBUG:root:[ Iteration 219 ] Training loss: 0.0401143
DEBUG:root:[ Iteration 220 ] Test loss: 0.0482903
DEBUG:root:[ Iteration 222 ] Training loss: 0.0435117
DEBUG:root:[ Iteration 225 ] Training loss: 0.038835
DEBUG:root:[ Iteration 228 ] Training loss: 0.0403607
DEBUG:root:[ Iteration 231 ] Training loss: 0.0413543
DEBUG:root:[ Iteration 234 ] Training loss: 0.0401127
DEBUG:root:[ Iteration 237 ] Training loss: 0.0455494
DEBUG:root:[ Iteration 240 ] Training loss: 0.0340047
DEBUG:root:[ Iteration 240 ] Test loss: 0.0442959
DEBUG:root:[ Iteration 243 ] Training loss: 0.037512
DEBUG:root:[ Iteration 246 ] Training loss: 0.037484
DEBUG:root:[ Iteration 249 ] Training loss: 0.0409674
DEBUG:root:[ Iteration 252 ] Training loss: 0.0448713
DEBUG:root:[ Iteration 255 ] Training loss: 0.0428234
DEBUG:root:[ Iteration 258 ] Training loss: 0.0393372
DEBUG:root:[ Iteration 260 ] Test loss: 0.0403881
DEBUG:root:[ Iteration 261 ] Training loss: 0.0412748
DEBUG:root:[ Iteration 264 ] Training loss: 0.0315377
DEBUG:root:[ Iteration 267 ] Training loss: 0.0408246
DEBUG:root:[ Iteration 270 ] Training loss: 0.0450807
DEBUG:root:[ Iteration 273 ] Training loss: 0.0412765
DEBUG:root:[ Iteration 276 ] Training loss: 0.0390489
DEBUG:root:[ Iteration 279 ] Training loss: 0.0402611
DEBUG:root:[ Iteration 280 ] Test loss: 0.0431576
DEBUG:root:[ Iteration 282 ] Training loss: 0.043966
DEBUG:root:[ Iteration 285 ] Training loss: 0.0429395
DEBUG:root:[ Iteration 288 ] Training loss: 0.035459
DEBUG:root:[ Iteration 291 ] Training loss: 0.0389849
DEBUG:root:[ Iteration 294 ] Training loss: 0.0435161
DEBUG:root:[ Iteration 297 ] Training loss: 0.0410051
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-13-2016_14h52m06s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0395941
DEBUG:root:[ Iteration 0 ] Test loss: 0.0391377
DEBUG:root:[ Iteration 3 ] Training loss: 0.0357649
DEBUG:root:[ Iteration 6 ] Training loss: 0.0327737
DEBUG:root:[ Iteration 9 ] Training loss: 0.0443881
DEBUG:root:[ Iteration 12 ] Training loss: 0.0434513
DEBUG:root:[ Iteration 15 ] Training loss: 0.0391718
DEBUG:root:[ Iteration 18 ] Training loss: 0.0366191
DEBUG:root:[ Iteration 20 ] Test loss: 0.0387362
DEBUG:root:[ Iteration 21 ] Training loss: 0.0382265
DEBUG:root:[ Iteration 24 ] Training loss: 0.0337819
DEBUG:root:[ Iteration 27 ] Training loss: 0.0374733
DEBUG:root:[ Iteration 30 ] Training loss: 0.0342443
DEBUG:root:[ Iteration 33 ] Training loss: 0.0339374
DEBUG:root:[ Iteration 36 ] Training loss: 0.0333834
DEBUG:root:[ Iteration 39 ] Training loss: 0.0345099
DEBUG:root:[ Iteration 40 ] Test loss: 0.0367347
DEBUG:root:[ Iteration 42 ] Training loss: 0.0353516
DEBUG:root:[ Iteration 45 ] Training loss: 0.0373153
DEBUG:root:[ Iteration 48 ] Training loss: 0.03398
DEBUG:root:[ Iteration 51 ] Training loss: 0.0308123
DEBUG:root:[ Iteration 54 ] Training loss: 0.0307831
DEBUG:root:[ Iteration 57 ] Training loss: 0.0309724
DEBUG:root:[ Iteration 60 ] Training loss: 0.0349141
DEBUG:root:[ Iteration 60 ] Test loss: 0.0327572
DEBUG:root:[ Iteration 63 ] Training loss: 0.0332431
DEBUG:root:[ Iteration 66 ] Training loss: 0.0341288
DEBUG:root:[ Iteration 69 ] Training loss: 0.0322918
DEBUG:root:[ Iteration 72 ] Training loss: 0.0293817
DEBUG:root:[ Iteration 75 ] Training loss: 0.0374579
DEBUG:root:[ Iteration 78 ] Training loss: 0.0318199
DEBUG:root:[ Iteration 80 ] Test loss: 0.0413526
DEBUG:root:[ Iteration 81 ] Training loss: 0.0341914
DEBUG:root:[ Iteration 84 ] Training loss: 0.0337815
DEBUG:root:[ Iteration 87 ] Training loss: 0.0359085
DEBUG:root:[ Iteration 90 ] Training loss: 0.0277405
DEBUG:root:[ Iteration 93 ] Training loss: 0.0307775
DEBUG:root:[ Iteration 96 ] Training loss: 0.0324829
DEBUG:root:[ Iteration 99 ] Training loss: 0.0320034
DEBUG:root:[ Iteration 100 ] Test loss: 0.0284866
DEBUG:root:[ Iteration 102 ] Training loss: 0.0275133
DEBUG:root:[ Iteration 105 ] Training loss: 0.0285595
DEBUG:root:[ Iteration 108 ] Training loss: 0.0357263
DEBUG:root:[ Iteration 111 ] Training loss: 0.0297162
DEBUG:root:[ Iteration 114 ] Training loss: 0.0298534
DEBUG:root:[ Iteration 117 ] Training loss: 0.0358431
DEBUG:root:[ Iteration 120 ] Training loss: 0.0277159
DEBUG:root:[ Iteration 120 ] Test loss: 0.0353661
DEBUG:root:[ Iteration 123 ] Training loss: 0.0289064
DEBUG:root:[ Iteration 126 ] Training loss: 0.0283204
DEBUG:root:[ Iteration 129 ] Training loss: 0.0263464
DEBUG:root:[ Iteration 132 ] Training loss: 0.0335294
DEBUG:root:[ Iteration 135 ] Training loss: 0.0322683
DEBUG:root:[ Iteration 138 ] Training loss: 0.0282633
DEBUG:root:[ Iteration 140 ] Test loss: 0.0335959
DEBUG:root:[ Iteration 141 ] Training loss: 0.0307128
DEBUG:root:[ Iteration 144 ] Training loss: 0.03019
DEBUG:root:[ Iteration 147 ] Training loss: 0.0253378
DEBUG:root:[ Iteration 150 ] Training loss: 0.0314714
DEBUG:root:[ Iteration 153 ] Training loss: 0.0267459
DEBUG:root:[ Iteration 156 ] Training loss: 0.0300659
DEBUG:root:[ Iteration 159 ] Training loss: 0.0263504
DEBUG:root:[ Iteration 160 ] Test loss: 0.0338545
DEBUG:root:[ Iteration 162 ] Training loss: 0.0340182
DEBUG:root:[ Iteration 165 ] Training loss: 0.0339847
DEBUG:root:[ Iteration 168 ] Training loss: 0.0330632
DEBUG:root:[ Iteration 171 ] Training loss: 0.0303914
DEBUG:root:[ Iteration 174 ] Training loss: 0.0261827
DEBUG:root:[ Iteration 177 ] Training loss: 0.0294526
DEBUG:root:[ Iteration 180 ] Training loss: 0.0321046
DEBUG:root:[ Iteration 180 ] Test loss: 0.0357932
DEBUG:root:[ Iteration 183 ] Training loss: 0.0245227
DEBUG:root:[ Iteration 186 ] Training loss: 0.0286436
DEBUG:root:[ Iteration 189 ] Training loss: 0.0296118
DEBUG:root:[ Iteration 192 ] Training loss: 0.0278525
DEBUG:root:[ Iteration 195 ] Training loss: 0.0284946
DEBUG:root:[ Iteration 198 ] Training loss: 0.0263869
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-13-2016_15h23m31s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.164413
DEBUG:root:[ Iteration 0 ] Test loss: 0.15746
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-14-2016_15h18m09s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.162507
DEBUG:root:[ Iteration 0 ] Test loss: 0.159911
DEBUG:root:[ Iteration 3 ] Training loss: 0.160253
DEBUG:root:[ Iteration 6 ] Training loss: 0.158196
DEBUG:root:[ Iteration 9 ] Training loss: 0.150096
DEBUG:root:[ Iteration 12 ] Training loss: 0.143653
DEBUG:root:[ Iteration 15 ] Training loss: 0.14618
DEBUG:root:[ Iteration 18 ] Training loss: 0.138831
DEBUG:root:[ Iteration 20 ] Test loss: 0.126914
DEBUG:root:[ Iteration 21 ] Training loss: 0.12184
DEBUG:root:[ Iteration 24 ] Training loss: 0.100154
DEBUG:root:[ Iteration 27 ] Training loss: 0.0726057
DEBUG:root:[ Iteration 30 ] Training loss: 0.0558993
DEBUG:root:[ Iteration 33 ] Training loss: 0.0622908
DEBUG:root:[ Iteration 36 ] Training loss: 0.0581187
DEBUG:root:[ Iteration 39 ] Training loss: 0.0552161
DEBUG:root:[ Iteration 40 ] Test loss: 0.0495925
DEBUG:root:[ Iteration 42 ] Training loss: 0.0544989
DEBUG:root:[ Iteration 45 ] Training loss: 0.0540861
DEBUG:root:[ Iteration 48 ] Training loss: 0.0504547
DEBUG:root:[ Iteration 51 ] Training loss: 0.0488091
DEBUG:root:[ Iteration 54 ] Training loss: 0.0526456
DEBUG:root:[ Iteration 57 ] Training loss: 0.0535195
DEBUG:root:[ Iteration 60 ] Training loss: 0.0383004
DEBUG:root:[ Iteration 60 ] Test loss: 0.0435915
DEBUG:root:[ Iteration 63 ] Training loss: 0.0412331
DEBUG:root:[ Iteration 66 ] Training loss: 0.0431784
DEBUG:root:[ Iteration 69 ] Training loss: 0.0402798
DEBUG:root:[ Iteration 72 ] Training loss: 0.0369848
DEBUG:root:[ Iteration 75 ] Training loss: 0.0381416
DEBUG:root:[ Iteration 78 ] Training loss: 0.0377542
DEBUG:root:[ Iteration 80 ] Test loss: 0.0388136
DEBUG:root:[ Iteration 81 ] Training loss: 0.0430743
DEBUG:root:[ Iteration 84 ] Training loss: 0.0333093
DEBUG:root:[ Iteration 87 ] Training loss: 0.0398788
DEBUG:root:[ Iteration 90 ] Training loss: 0.0340361
DEBUG:root:[ Iteration 93 ] Training loss: 0.0342983
DEBUG:root:[ Iteration 96 ] Training loss: 0.039434
DEBUG:root:[ Iteration 99 ] Training loss: 0.0336712
DEBUG:root:[ Iteration 100 ] Test loss: 0.036145
DEBUG:root:[ Iteration 102 ] Training loss: 0.0322393
DEBUG:root:[ Iteration 105 ] Training loss: 0.0341157
DEBUG:root:[ Iteration 108 ] Training loss: 0.0360123
DEBUG:root:[ Iteration 111 ] Training loss: 0.0348034
DEBUG:root:[ Iteration 114 ] Training loss: 0.0336163
DEBUG:root:[ Iteration 117 ] Training loss: 0.0276306
DEBUG:root:[ Iteration 120 ] Training loss: 0.0273698
DEBUG:root:[ Iteration 120 ] Test loss: 0.0332431
DEBUG:root:[ Iteration 123 ] Training loss: 0.0292199
DEBUG:root:[ Iteration 126 ] Training loss: 0.0301996
DEBUG:root:[ Iteration 129 ] Training loss: 0.0269342
DEBUG:root:[ Iteration 132 ] Training loss: 0.0253822
DEBUG:root:[ Iteration 135 ] Training loss: 0.0299894
DEBUG:root:[ Iteration 138 ] Training loss: 0.0260677
DEBUG:root:[ Iteration 140 ] Test loss: 0.0305543
DEBUG:root:[ Iteration 141 ] Training loss: 0.0276984
DEBUG:root:[ Iteration 144 ] Training loss: 0.0254511
DEBUG:root:[ Iteration 147 ] Training loss: 0.0246232
DEBUG:root:[ Iteration 150 ] Training loss: 0.0303247
DEBUG:root:[ Iteration 153 ] Training loss: 0.0219926
DEBUG:root:[ Iteration 156 ] Training loss: 0.0266321
DEBUG:root:[ Iteration 159 ] Training loss: 0.0239324
DEBUG:root:[ Iteration 160 ] Test loss: 0.0281569
DEBUG:root:[ Iteration 162 ] Training loss: 0.0271822
DEBUG:root:[ Iteration 165 ] Training loss: 0.0253557
DEBUG:root:[ Iteration 168 ] Training loss: 0.0233489
DEBUG:root:[ Iteration 171 ] Training loss: 0.0206846
DEBUG:root:[ Iteration 174 ] Training loss: 0.0265116
DEBUG:root:[ Iteration 177 ] Training loss: 0.0248814
DEBUG:root:[ Iteration 180 ] Training loss: 0.0248726
DEBUG:root:[ Iteration 180 ] Test loss: 0.0256825
DEBUG:root:[ Iteration 183 ] Training loss: 0.024187
DEBUG:root:[ Iteration 186 ] Training loss: 0.0221562
DEBUG:root:[ Iteration 189 ] Training loss: 0.0212358
DEBUG:root:[ Iteration 192 ] Training loss: 0.0206834
DEBUG:root:[ Iteration 195 ] Training loss: 0.0191336
DEBUG:root:[ Iteration 198 ] Training loss: 0.0267195
DEBUG:root:[ Iteration 200 ] Test loss: 0.0238305
DEBUG:root:[ Iteration 201 ] Training loss: 0.0197526
DEBUG:root:[ Iteration 204 ] Training loss: 0.0219668
DEBUG:root:[ Iteration 207 ] Training loss: 0.0216634
DEBUG:root:[ Iteration 210 ] Training loss: 0.022196
DEBUG:root:[ Iteration 213 ] Training loss: 0.0193169
DEBUG:root:[ Iteration 216 ] Training loss: 0.0178873
DEBUG:root:[ Iteration 219 ] Training loss: 0.0155136
DEBUG:root:[ Iteration 220 ] Test loss: 0.0221713
DEBUG:root:[ Iteration 222 ] Training loss: 0.0206073
DEBUG:root:[ Iteration 225 ] Training loss: 0.0177335
DEBUG:root:[ Iteration 228 ] Training loss: 0.0163857
DEBUG:root:[ Iteration 231 ] Training loss: 0.0156187
DEBUG:root:[ Iteration 234 ] Training loss: 0.0177339
DEBUG:root:[ Iteration 237 ] Training loss: 0.0167959
DEBUG:root:[ Iteration 240 ] Training loss: 0.014203
DEBUG:root:[ Iteration 240 ] Test loss: 0.0207951
DEBUG:root:[ Iteration 243 ] Training loss: 0.0161804
DEBUG:root:[ Iteration 246 ] Training loss: 0.0147934
DEBUG:root:[ Iteration 249 ] Training loss: 0.0149793
DEBUG:root:[ Iteration 252 ] Training loss: 0.0163823
DEBUG:root:[ Iteration 255 ] Training loss: 0.0170928
DEBUG:root:[ Iteration 258 ] Training loss: 0.0168029
DEBUG:root:[ Iteration 260 ] Test loss: 0.0192876
DEBUG:root:[ Iteration 261 ] Training loss: 0.0154157
DEBUG:root:[ Iteration 264 ] Training loss: 0.0159365
DEBUG:root:[ Iteration 267 ] Training loss: 0.0158693
DEBUG:root:[ Iteration 270 ] Training loss: 0.014571
DEBUG:root:[ Iteration 273 ] Training loss: 0.0138439
DEBUG:root:[ Iteration 276 ] Training loss: 0.0158892
DEBUG:root:[ Iteration 279 ] Training loss: 0.0172952
DEBUG:root:[ Iteration 280 ] Test loss: 0.0180642
DEBUG:root:[ Iteration 282 ] Training loss: 0.0179298
DEBUG:root:[ Iteration 285 ] Training loss: 0.0150531
DEBUG:root:[ Iteration 288 ] Training loss: 0.0138903
DEBUG:root:[ Iteration 291 ] Training loss: 0.0110284
DEBUG:root:[ Iteration 294 ] Training loss: 0.0130374
DEBUG:root:[ Iteration 297 ] Training loss: 0.0134006
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-14-2016_15h26m23s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.173221
DEBUG:root:[ Iteration 0 ] Test loss: 0.171319
DEBUG:root:[ Iteration 3 ] Training loss: 0.172527
DEBUG:root:[ Iteration 6 ] Training loss: 0.163998
DEBUG:root:[ Iteration 9 ] Training loss: 0.161782
DEBUG:root:[ Iteration 12 ] Training loss: 0.16073
DEBUG:root:[ Iteration 15 ] Training loss: 0.159823
DEBUG:root:[ Iteration 18 ] Training loss: 0.146919
DEBUG:root:[ Iteration 20 ] Test loss: 0.146642
DEBUG:root:[ Iteration 21 ] Training loss: 0.146753
DEBUG:root:[ Iteration 24 ] Training loss: 0.134193
DEBUG:root:[ Iteration 27 ] Training loss: 0.108784
DEBUG:root:[ Iteration 30 ] Training loss: 0.0844976
DEBUG:root:[ Iteration 33 ] Training loss: 0.0568832
DEBUG:root:[ Iteration 36 ] Training loss: 0.0574706
DEBUG:root:[ Iteration 39 ] Training loss: 0.060576
DEBUG:root:[ Iteration 40 ] Test loss: 0.053832
DEBUG:root:[ Iteration 42 ] Training loss: 0.0458496
DEBUG:root:[ Iteration 45 ] Training loss: 0.0495371
DEBUG:root:[ Iteration 48 ] Training loss: 0.0561966
DEBUG:root:[ Iteration 51 ] Training loss: 0.0528204
DEBUG:root:[ Iteration 54 ] Training loss: 0.0430847
DEBUG:root:[ Iteration 57 ] Training loss: 0.049107
DEBUG:root:[ Iteration 60 ] Training loss: 0.0430062
DEBUG:root:[ Iteration 60 ] Test loss: 0.0430364
DEBUG:root:[ Iteration 63 ] Training loss: 0.0547005
DEBUG:root:[ Iteration 66 ] Training loss: 0.0343112
DEBUG:root:[ Iteration 69 ] Training loss: 0.0400614
DEBUG:root:[ Iteration 72 ] Training loss: 0.044567
DEBUG:root:[ Iteration 75 ] Training loss: 0.0369977
DEBUG:root:[ Iteration 78 ] Training loss: 0.0364581
DEBUG:root:[ Iteration 80 ] Test loss: 0.0422239
DEBUG:root:[ Iteration 81 ] Training loss: 0.0390171
DEBUG:root:[ Iteration 84 ] Training loss: 0.0448985
DEBUG:root:[ Iteration 87 ] Training loss: 0.0426719
DEBUG:root:[ Iteration 90 ] Training loss: 0.0412118
DEBUG:root:[ Iteration 93 ] Training loss: 0.0448528
DEBUG:root:[ Iteration 96 ] Training loss: 0.0426636
DEBUG:root:[ Iteration 99 ] Training loss: 0.0418501
DEBUG:root:[ Iteration 100 ] Test loss: 0.0390392
DEBUG:root:[ Iteration 102 ] Training loss: 0.0448846
DEBUG:root:[ Iteration 105 ] Training loss: 0.0396082
DEBUG:root:[ Iteration 108 ] Training loss: 0.0345701
DEBUG:root:[ Iteration 111 ] Training loss: 0.0362893
DEBUG:root:[ Iteration 114 ] Training loss: 0.0370361
DEBUG:root:[ Iteration 117 ] Training loss: 0.0353827
DEBUG:root:[ Iteration 120 ] Training loss: 0.0356593
DEBUG:root:[ Iteration 120 ] Test loss: 0.0366649
DEBUG:root:[ Iteration 123 ] Training loss: 0.0359634
DEBUG:root:[ Iteration 126 ] Training loss: 0.0381497
DEBUG:root:[ Iteration 129 ] Training loss: 0.0381219
DEBUG:root:[ Iteration 132 ] Training loss: 0.0329658
DEBUG:root:[ Iteration 135 ] Training loss: 0.0336808
DEBUG:root:[ Iteration 138 ] Training loss: 0.0333541
DEBUG:root:[ Iteration 140 ] Test loss: 0.0370372
DEBUG:root:[ Iteration 141 ] Training loss: 0.0330838
DEBUG:root:[ Iteration 144 ] Training loss: 0.0327387
DEBUG:root:[ Iteration 147 ] Training loss: 0.0376545
DEBUG:root:[ Iteration 150 ] Training loss: 0.0307828
DEBUG:root:[ Iteration 153 ] Training loss: 0.0324405
DEBUG:root:[ Iteration 156 ] Training loss: 0.0365367
DEBUG:root:[ Iteration 159 ] Training loss: 0.0313493
DEBUG:root:[ Iteration 160 ] Test loss: 0.0345864
DEBUG:root:[ Iteration 162 ] Training loss: 0.0382271
DEBUG:root:[ Iteration 165 ] Training loss: 0.033099
DEBUG:root:[ Iteration 168 ] Training loss: 0.033043
DEBUG:root:[ Iteration 171 ] Training loss: 0.0358804
DEBUG:root:[ Iteration 174 ] Training loss: 0.0266728
DEBUG:root:[ Iteration 177 ] Training loss: 0.033882
DEBUG:root:[ Iteration 180 ] Training loss: 0.0328459
DEBUG:root:[ Iteration 180 ] Test loss: 0.036091
DEBUG:root:[ Iteration 183 ] Training loss: 0.0393072
DEBUG:root:[ Iteration 186 ] Training loss: 0.0366121
DEBUG:root:[ Iteration 189 ] Training loss: 0.0332823
DEBUG:root:[ Iteration 192 ] Training loss: 0.0304675
DEBUG:root:[ Iteration 195 ] Training loss: 0.0281313
DEBUG:root:[ Iteration 198 ] Training loss: 0.034044
DEBUG:root:[ Iteration 200 ] Test loss: 0.0367556
DEBUG:root:[ Iteration 201 ] Training loss: 0.0257064
DEBUG:root:[ Iteration 204 ] Training loss: 0.0296892
DEBUG:root:[ Iteration 207 ] Training loss: 0.0336231
DEBUG:root:[ Iteration 210 ] Training loss: 0.0273774
DEBUG:root:[ Iteration 213 ] Training loss: 0.0353185
DEBUG:root:[ Iteration 216 ] Training loss: 0.031205
DEBUG:root:[ Iteration 219 ] Training loss: 0.0312383
DEBUG:root:[ Iteration 220 ] Test loss: 0.0377957
DEBUG:root:[ Iteration 222 ] Training loss: 0.0375696
DEBUG:root:[ Iteration 225 ] Training loss: 0.0341488
DEBUG:root:[ Iteration 228 ] Training loss: 0.0323303
DEBUG:root:[ Iteration 231 ] Training loss: 0.0298144
DEBUG:root:[ Iteration 234 ] Training loss: 0.0285326
DEBUG:root:[ Iteration 237 ] Training loss: 0.0281483
DEBUG:root:[ Iteration 240 ] Training loss: 0.0381255
DEBUG:root:[ Iteration 240 ] Test loss: 0.0341878
DEBUG:root:[ Iteration 243 ] Training loss: 0.0289768
DEBUG:root:[ Iteration 246 ] Training loss: 0.0293495
DEBUG:root:[ Iteration 249 ] Training loss: 0.0345948
DEBUG:root:[ Iteration 252 ] Training loss: 0.0305351
DEBUG:root:[ Iteration 255 ] Training loss: 0.0309573
DEBUG:root:[ Iteration 258 ] Training loss: 0.031732
DEBUG:root:[ Iteration 260 ] Test loss: 0.0326216
DEBUG:root:[ Iteration 261 ] Training loss: 0.0338677
DEBUG:root:[ Iteration 264 ] Training loss: 0.0240072
DEBUG:root:[ Iteration 267 ] Training loss: 0.0315103
DEBUG:root:[ Iteration 270 ] Training loss: 0.034448
DEBUG:root:[ Iteration 273 ] Training loss: 0.0310258
DEBUG:root:[ Iteration 276 ] Training loss: 0.0322369
DEBUG:root:[ Iteration 279 ] Training loss: 0.0302959
DEBUG:root:[ Iteration 280 ] Test loss: 0.0327283
DEBUG:root:[ Iteration 282 ] Training loss: 0.029874
DEBUG:root:[ Iteration 285 ] Training loss: 0.033428
DEBUG:root:[ Iteration 288 ] Training loss: 0.0342192
DEBUG:root:[ Iteration 291 ] Training loss: 0.0262969
DEBUG:root:[ Iteration 294 ] Training loss: 0.032756
DEBUG:root:[ Iteration 297 ] Training loss: 0.0284506
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-15-2016_11h59m39s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.167586
DEBUG:root:[ Iteration 0 ] Test loss: 0.170787
DEBUG:root:[ Iteration 3 ] Training loss: 0.169435
DEBUG:root:[ Iteration 6 ] Training loss: 0.163402
DEBUG:root:[ Iteration 9 ] Training loss: 0.164938
DEBUG:root:[ Iteration 12 ] Training loss: 0.153958
DEBUG:root:[ Iteration 15 ] Training loss: 0.148036
DEBUG:root:[ Iteration 18 ] Training loss: 0.141721
DEBUG:root:[ Iteration 20 ] Test loss: 0.140537
DEBUG:root:[ Iteration 21 ] Training loss: 0.137854
DEBUG:root:[ Iteration 24 ] Training loss: 0.111254
DEBUG:root:[ Iteration 27 ] Training loss: 0.0855121
DEBUG:root:[ Iteration 30 ] Training loss: 0.0715465
DEBUG:root:[ Iteration 33 ] Training loss: 0.0677057
DEBUG:root:[ Iteration 36 ] Training loss: 0.0673683
DEBUG:root:[ Iteration 39 ] Training loss: 0.0727065
DEBUG:root:[ Iteration 40 ] Test loss: 0.0647706
DEBUG:root:[ Iteration 42 ] Training loss: 0.0624719
DEBUG:root:[ Iteration 45 ] Training loss: 0.0649659
DEBUG:root:[ Iteration 48 ] Training loss: 0.0541326
DEBUG:root:[ Iteration 51 ] Training loss: 0.0584635
DEBUG:root:[ Iteration 54 ] Training loss: 0.0605418
DEBUG:root:[ Iteration 57 ] Training loss: 0.0622435
DEBUG:root:[ Iteration 60 ] Training loss: 0.0518007
DEBUG:root:[ Iteration 60 ] Test loss: 0.0609512
DEBUG:root:[ Iteration 63 ] Training loss: 0.060591
DEBUG:root:[ Iteration 66 ] Training loss: 0.0566142
DEBUG:root:[ Iteration 69 ] Training loss: 0.0559882
DEBUG:root:[ Iteration 72 ] Training loss: 0.0529839
DEBUG:root:[ Iteration 75 ] Training loss: 0.0483611
DEBUG:root:[ Iteration 78 ] Training loss: 0.0491729
DEBUG:root:[ Iteration 80 ] Test loss: 0.0554078
DEBUG:root:[ Iteration 81 ] Training loss: 0.0485973
DEBUG:root:[ Iteration 84 ] Training loss: 0.0598998
DEBUG:root:[ Iteration 87 ] Training loss: 0.0482234
DEBUG:root:[ Iteration 90 ] Training loss: 0.0517037
DEBUG:root:[ Iteration 93 ] Training loss: 0.0515613
DEBUG:root:[ Iteration 96 ] Training loss: 0.0530311
DEBUG:root:[ Iteration 99 ] Training loss: 0.0526118
DEBUG:root:[ Iteration 100 ] Test loss: 0.0541039
DEBUG:root:[ Iteration 102 ] Training loss: 0.0515319
DEBUG:root:[ Iteration 105 ] Training loss: 0.0508778
DEBUG:root:[ Iteration 108 ] Training loss: 0.0497656
DEBUG:root:[ Iteration 111 ] Training loss: 0.0511584
DEBUG:root:[ Iteration 114 ] Training loss: 0.0568287
DEBUG:root:[ Iteration 117 ] Training loss: 0.0445614
DEBUG:root:[ Iteration 120 ] Training loss: 0.0428284
DEBUG:root:[ Iteration 120 ] Test loss: 0.0547521
DEBUG:root:[ Iteration 123 ] Training loss: 0.0537774
DEBUG:root:[ Iteration 126 ] Training loss: 0.0453661
DEBUG:root:[ Iteration 129 ] Training loss: 0.0512335
DEBUG:root:[ Iteration 132 ] Training loss: 0.0464848
DEBUG:root:[ Iteration 135 ] Training loss: 0.0478949
DEBUG:root:[ Iteration 138 ] Training loss: 0.0468435
DEBUG:root:[ Iteration 140 ] Test loss: 0.0478592
DEBUG:root:[ Iteration 141 ] Training loss: 0.0456775
DEBUG:root:[ Iteration 144 ] Training loss: 0.0447145
DEBUG:root:[ Iteration 147 ] Training loss: 0.036686
DEBUG:root:[ Iteration 150 ] Training loss: 0.045362
DEBUG:root:[ Iteration 153 ] Training loss: 0.0403234
DEBUG:root:[ Iteration 156 ] Training loss: 0.0460945
DEBUG:root:[ Iteration 159 ] Training loss: 0.0412347
DEBUG:root:[ Iteration 160 ] Test loss: 0.0478081
DEBUG:root:[ Iteration 162 ] Training loss: 0.0427926
DEBUG:root:[ Iteration 165 ] Training loss: 0.0429959
DEBUG:root:[ Iteration 168 ] Training loss: 0.0456732
DEBUG:root:[ Iteration 171 ] Training loss: 0.0374247
DEBUG:root:[ Iteration 174 ] Training loss: 0.037839
DEBUG:root:[ Iteration 177 ] Training loss: 0.0377348
DEBUG:root:[ Iteration 180 ] Training loss: 0.0379381
DEBUG:root:[ Iteration 180 ] Test loss: 0.0481788
DEBUG:root:[ Iteration 183 ] Training loss: 0.0383963
DEBUG:root:[ Iteration 186 ] Training loss: 0.0419506
DEBUG:root:[ Iteration 189 ] Training loss: 0.0361184
DEBUG:root:[ Iteration 192 ] Training loss: 0.0395451
DEBUG:root:[ Iteration 195 ] Training loss: 0.0304365
DEBUG:root:[ Iteration 198 ] Training loss: 0.0428705
DEBUG:root:[ Iteration 200 ] Test loss: 0.0461187
DEBUG:root:[ Iteration 201 ] Training loss: 0.0314029
DEBUG:root:[ Iteration 204 ] Training loss: 0.0412005
DEBUG:root:[ Iteration 207 ] Training loss: 0.034232
DEBUG:root:[ Iteration 210 ] Training loss: 0.0305051
DEBUG:root:[ Iteration 213 ] Training loss: 0.033585
DEBUG:root:[ Iteration 216 ] Training loss: 0.0409324
DEBUG:root:[ Iteration 219 ] Training loss: 0.0323728
DEBUG:root:[ Iteration 220 ] Test loss: 0.0395427
DEBUG:root:[ Iteration 222 ] Training loss: 0.0361503
DEBUG:root:[ Iteration 225 ] Training loss: 0.0325453
DEBUG:root:[ Iteration 228 ] Training loss: 0.0376449
DEBUG:root:[ Iteration 231 ] Training loss: 0.0314396
DEBUG:root:[ Iteration 234 ] Training loss: 0.0343107
DEBUG:root:[ Iteration 237 ] Training loss: 0.0379342
DEBUG:root:[ Iteration 240 ] Training loss: 0.0305121
DEBUG:root:[ Iteration 240 ] Test loss: 0.0387702
DEBUG:root:[ Iteration 243 ] Training loss: 0.031172
DEBUG:root:[ Iteration 246 ] Training loss: 0.033486
DEBUG:root:[ Iteration 249 ] Training loss: 0.0333697
DEBUG:root:[ Iteration 252 ] Training loss: 0.0311425
DEBUG:root:[ Iteration 255 ] Training loss: 0.0332807
DEBUG:root:[ Iteration 258 ] Training loss: 0.031365
DEBUG:root:[ Iteration 260 ] Test loss: 0.0364836
DEBUG:root:[ Iteration 261 ] Training loss: 0.0291882
DEBUG:root:[ Iteration 264 ] Training loss: 0.0259852
DEBUG:root:[ Iteration 267 ] Training loss: 0.0271014
DEBUG:root:[ Iteration 270 ] Training loss: 0.0326457
DEBUG:root:[ Iteration 273 ] Training loss: 0.0256923
DEBUG:root:[ Iteration 276 ] Training loss: 0.0290043
DEBUG:root:[ Iteration 279 ] Training loss: 0.0339174
DEBUG:root:[ Iteration 280 ] Test loss: 0.0413469
DEBUG:root:[ Iteration 282 ] Training loss: 0.0249469
DEBUG:root:[ Iteration 285 ] Training loss: 0.0311084
DEBUG:root:[ Iteration 288 ] Training loss: 0.0304046
DEBUG:root:[ Iteration 291 ] Training loss: 0.0283959
DEBUG:root:[ Iteration 294 ] Training loss: 0.029424
DEBUG:root:[ Iteration 297 ] Training loss: 0.0281212
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-15-2016_14h40m32s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0962633
DEBUG:root:[ Iteration 0 ] Test loss: 0.0905146
DEBUG:root:[ Iteration 3 ] Training loss: 0.0708068
DEBUG:root:[ Iteration 6 ] Training loss: 0.0426848
DEBUG:root:[ Iteration 9 ] Training loss: 0.0430741
DEBUG:root:[ Iteration 12 ] Training loss: 0.0548028
DEBUG:root:[ Iteration 15 ] Training loss: 0.0471298
DEBUG:root:[ Iteration 18 ] Training loss: 0.0414777
DEBUG:root:[ Iteration 20 ] Test loss: 0.0470083
DEBUG:root:[ Iteration 21 ] Training loss: 0.0328322
DEBUG:root:[ Iteration 24 ] Training loss: 0.0397747
DEBUG:root:[ Iteration 27 ] Training loss: 0.0269862
DEBUG:root:[ Iteration 30 ] Training loss: 0.032676
DEBUG:root:[ Iteration 33 ] Training loss: 0.0329144
DEBUG:root:[ Iteration 36 ] Training loss: 0.0314314
DEBUG:root:[ Iteration 39 ] Training loss: 0.028859
DEBUG:root:[ Iteration 40 ] Test loss: 0.0322707
DEBUG:root:[ Iteration 42 ] Training loss: 0.0272802
DEBUG:root:[ Iteration 45 ] Training loss: 0.0351618
DEBUG:root:[ Iteration 48 ] Training loss: 0.029792
DEBUG:root:[ Iteration 51 ] Training loss: 0.0325385
DEBUG:root:[ Iteration 54 ] Training loss: 0.0302791
DEBUG:root:[ Iteration 57 ] Training loss: 0.027985
DEBUG:root:[ Iteration 60 ] Training loss: 0.0296445
DEBUG:root:[ Iteration 60 ] Test loss: 0.0347706
DEBUG:root:[ Iteration 63 ] Training loss: 0.0290779
DEBUG:root:[ Iteration 66 ] Training loss: 0.0244291
DEBUG:root:[ Iteration 69 ] Training loss: 0.0262939
DEBUG:root:[ Iteration 72 ] Training loss: 0.0260526
DEBUG:root:[ Iteration 75 ] Training loss: 0.0272907
DEBUG:root:[ Iteration 78 ] Training loss: 0.0319506
DEBUG:root:[ Iteration 80 ] Test loss: 0.0379765
DEBUG:root:[ Iteration 81 ] Training loss: 0.0286953
DEBUG:root:[ Iteration 84 ] Training loss: 0.0231091
DEBUG:root:[ Iteration 87 ] Training loss: 0.024346
DEBUG:root:[ Iteration 90 ] Training loss: 0.0238603
DEBUG:root:[ Iteration 93 ] Training loss: 0.0229257
DEBUG:root:[ Iteration 96 ] Training loss: 0.0217211
DEBUG:root:[ Iteration 99 ] Training loss: 0.0284187
DEBUG:root:[ Iteration 100 ] Test loss: 0.0249218
DEBUG:root:[ Iteration 102 ] Training loss: 0.0255138
DEBUG:root:[ Iteration 105 ] Training loss: 0.0190222
DEBUG:root:[ Iteration 108 ] Training loss: 0.021649
DEBUG:root:[ Iteration 111 ] Training loss: 0.0263985
DEBUG:root:[ Iteration 114 ] Training loss: 0.0266357
DEBUG:root:[ Iteration 117 ] Training loss: 0.0186802
DEBUG:root:[ Iteration 120 ] Training loss: 0.0212586
DEBUG:root:[ Iteration 120 ] Test loss: 0.0233041
DEBUG:root:[ Iteration 123 ] Training loss: 0.0183402
DEBUG:root:[ Iteration 126 ] Training loss: 0.0220665
DEBUG:root:[ Iteration 129 ] Training loss: 0.0281854
DEBUG:root:[ Iteration 132 ] Training loss: 0.0194899
DEBUG:root:[ Iteration 135 ] Training loss: 0.0196946
DEBUG:root:[ Iteration 138 ] Training loss: 0.0178489
DEBUG:root:[ Iteration 140 ] Test loss: 0.0230629
DEBUG:root:[ Iteration 141 ] Training loss: 0.0214858
DEBUG:root:[ Iteration 144 ] Training loss: 0.0182087
DEBUG:root:[ Iteration 147 ] Training loss: 0.0188004
DEBUG:root:[ Iteration 150 ] Training loss: 0.0193987
DEBUG:root:[ Iteration 153 ] Training loss: 0.0186098
DEBUG:root:[ Iteration 156 ] Training loss: 0.0183951
DEBUG:root:[ Iteration 159 ] Training loss: 0.0170714
DEBUG:root:[ Iteration 160 ] Test loss: 0.0270706
DEBUG:root:[ Iteration 162 ] Training loss: 0.0224156
DEBUG:root:[ Iteration 165 ] Training loss: 0.0192047
DEBUG:root:[ Iteration 168 ] Training loss: 0.0196189
DEBUG:root:[ Iteration 171 ] Training loss: 0.0209039
DEBUG:root:[ Iteration 174 ] Training loss: 0.0169196
DEBUG:root:[ Iteration 177 ] Training loss: 0.0234905
DEBUG:root:[ Iteration 180 ] Training loss: 0.0178128
DEBUG:root:[ Iteration 180 ] Test loss: 0.02601
DEBUG:root:[ Iteration 183 ] Training loss: 0.0190725
DEBUG:root:[ Iteration 186 ] Training loss: 0.022301
DEBUG:root:[ Iteration 189 ] Training loss: 0.0191521
DEBUG:root:[ Iteration 192 ] Training loss: 0.0189893
DEBUG:root:[ Iteration 195 ] Training loss: 0.0212881
DEBUG:root:[ Iteration 198 ] Training loss: 0.021476
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-15-2016_14h47m20s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.163339
DEBUG:root:[ Iteration 0 ] Test loss: 0.158705
DEBUG:root:[ Iteration 3 ] Training loss: 0.161421
DEBUG:root:[ Iteration 6 ] Training loss: 0.159784
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-15-2016_16h26m56s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.165856
DEBUG:root:[ Iteration 0 ] Test loss: 0.163121
DEBUG:root:[ Iteration 3 ] Training loss: 0.1609
DEBUG:root:[ Iteration 6 ] Training loss: 0.164648
DEBUG:root:[ Iteration 9 ] Training loss: 0.153878
DEBUG:root:[ Iteration 12 ] Training loss: 0.153761
DEBUG:root:[ Iteration 15 ] Training loss: 0.146727
DEBUG:root:[ Iteration 18 ] Training loss: 0.135821
DEBUG:root:[ Iteration 20 ] Test loss: 0.126541
DEBUG:root:[ Iteration 21 ] Training loss: 0.121092
DEBUG:root:[ Iteration 24 ] Training loss: 0.0949773
DEBUG:root:[ Iteration 27 ] Training loss: 0.0736107
DEBUG:root:[ Iteration 30 ] Training loss: 0.0641646
DEBUG:root:[ Iteration 33 ] Training loss: 0.0608071
DEBUG:root:[ Iteration 36 ] Training loss: 0.0633913
DEBUG:root:[ Iteration 39 ] Training loss: 0.0615904
DEBUG:root:[ Iteration 40 ] Test loss: 0.0552041
DEBUG:root:[ Iteration 42 ] Training loss: 0.0535069
DEBUG:root:[ Iteration 45 ] Training loss: 0.0570227
DEBUG:root:[ Iteration 48 ] Training loss: 0.0568284
DEBUG:root:[ Iteration 51 ] Training loss: 0.0512595
DEBUG:root:[ Iteration 54 ] Training loss: 0.0619979
DEBUG:root:[ Iteration 57 ] Training loss: 0.0538616
DEBUG:root:[ Iteration 60 ] Training loss: 0.0494617
DEBUG:root:[ Iteration 60 ] Test loss: 0.0604318
DEBUG:root:[ Iteration 63 ] Training loss: 0.0537183
DEBUG:root:[ Iteration 66 ] Training loss: 0.0516169
DEBUG:root:[ Iteration 69 ] Training loss: 0.0520945
DEBUG:root:[ Iteration 72 ] Training loss: 0.0522358
DEBUG:root:[ Iteration 75 ] Training loss: 0.0490623
DEBUG:root:[ Iteration 78 ] Training loss: 0.0510915
DEBUG:root:[ Iteration 80 ] Test loss: 0.0495543
DEBUG:root:[ Iteration 81 ] Training loss: 0.044499
DEBUG:root:[ Iteration 84 ] Training loss: 0.0527678
DEBUG:root:[ Iteration 87 ] Training loss: 0.0595138
DEBUG:root:[ Iteration 90 ] Training loss: 0.0505929
DEBUG:root:[ Iteration 93 ] Training loss: 0.0527275
DEBUG:root:[ Iteration 96 ] Training loss: 0.0476971
DEBUG:root:[ Iteration 99 ] Training loss: 0.046106
DEBUG:root:[ Iteration 100 ] Test loss: 0.0443499
DEBUG:root:[ Iteration 102 ] Training loss: 0.040743
DEBUG:root:[ Iteration 105 ] Training loss: 0.043118
DEBUG:root:[ Iteration 108 ] Training loss: 0.0452826
DEBUG:root:[ Iteration 111 ] Training loss: 0.0401215
DEBUG:root:[ Iteration 114 ] Training loss: 0.0450427
DEBUG:root:[ Iteration 117 ] Training loss: 0.0459628
DEBUG:root:[ Iteration 120 ] Training loss: 0.0457465
DEBUG:root:[ Iteration 120 ] Test loss: 0.0496332
DEBUG:root:[ Iteration 123 ] Training loss: 0.0454895
DEBUG:root:[ Iteration 126 ] Training loss: 0.0424386
DEBUG:root:[ Iteration 129 ] Training loss: 0.049924
DEBUG:root:[ Iteration 132 ] Training loss: 0.0441495
DEBUG:root:[ Iteration 135 ] Training loss: 0.0493617
DEBUG:root:[ Iteration 138 ] Training loss: 0.0398681
DEBUG:root:[ Iteration 140 ] Test loss: 0.0366685
DEBUG:root:[ Iteration 141 ] Training loss: 0.0452612
DEBUG:root:[ Iteration 144 ] Training loss: 0.0411072
DEBUG:root:[ Iteration 147 ] Training loss: 0.0395159
DEBUG:root:[ Iteration 150 ] Training loss: 0.0365631
DEBUG:root:[ Iteration 153 ] Training loss: 0.0376321
DEBUG:root:[ Iteration 156 ] Training loss: 0.0466633
DEBUG:root:[ Iteration 159 ] Training loss: 0.0404942
DEBUG:root:[ Iteration 160 ] Test loss: 0.0407205
DEBUG:root:[ Iteration 162 ] Training loss: 0.0341948
DEBUG:root:[ Iteration 165 ] Training loss: 0.0395169
DEBUG:root:[ Iteration 168 ] Training loss: 0.0438827
DEBUG:root:[ Iteration 171 ] Training loss: 0.0461348
DEBUG:root:[ Iteration 174 ] Training loss: 0.0356229
DEBUG:root:[ Iteration 177 ] Training loss: 0.0430739
DEBUG:root:[ Iteration 180 ] Training loss: 0.0331018
DEBUG:root:[ Iteration 180 ] Test loss: 0.0391071
DEBUG:root:[ Iteration 183 ] Training loss: 0.0379242
DEBUG:root:[ Iteration 186 ] Training loss: 0.0458565
DEBUG:root:[ Iteration 189 ] Training loss: 0.0374345
DEBUG:root:[ Iteration 192 ] Training loss: 0.0453614
DEBUG:root:[ Iteration 195 ] Training loss: 0.0386662
DEBUG:root:[ Iteration 198 ] Training loss: 0.0393185
DEBUG:root:[ Iteration 200 ] Test loss: 0.0382686
DEBUG:root:[ Iteration 201 ] Training loss: 0.0369004
DEBUG:root:[ Iteration 204 ] Training loss: 0.0354591
DEBUG:root:[ Iteration 207 ] Training loss: 0.0351753
DEBUG:root:[ Iteration 210 ] Training loss: 0.0372029
DEBUG:root:[ Iteration 213 ] Training loss: 0.0370666
DEBUG:root:[ Iteration 216 ] Training loss: 0.0409641
DEBUG:root:[ Iteration 219 ] Training loss: 0.0364288
DEBUG:root:[ Iteration 220 ] Test loss: 0.0367439
DEBUG:root:[ Iteration 222 ] Training loss: 0.035361
DEBUG:root:[ Iteration 225 ] Training loss: 0.0340024
DEBUG:root:[ Iteration 228 ] Training loss: 0.0355156
DEBUG:root:[ Iteration 231 ] Training loss: 0.0350953
DEBUG:root:[ Iteration 234 ] Training loss: 0.0451302
DEBUG:root:[ Iteration 237 ] Training loss: 0.0345401
DEBUG:root:[ Iteration 240 ] Training loss: 0.0352816
DEBUG:root:[ Iteration 240 ] Test loss: 0.0372632
DEBUG:root:[ Iteration 243 ] Training loss: 0.037573
DEBUG:root:[ Iteration 246 ] Training loss: 0.0392817
DEBUG:root:[ Iteration 249 ] Training loss: 0.0400867
DEBUG:root:[ Iteration 252 ] Training loss: 0.0364927
DEBUG:root:[ Iteration 255 ] Training loss: 0.0358212
DEBUG:root:[ Iteration 258 ] Training loss: 0.0424238
DEBUG:root:[ Iteration 260 ] Test loss: 0.0368266
DEBUG:root:[ Iteration 261 ] Training loss: 0.0352434
DEBUG:root:[ Iteration 264 ] Training loss: 0.0328996
DEBUG:root:[ Iteration 267 ] Training loss: 0.0320539
DEBUG:root:[ Iteration 270 ] Training loss: 0.0380057
DEBUG:root:[ Iteration 273 ] Training loss: 0.0313014
DEBUG:root:[ Iteration 276 ] Training loss: 0.0350004
DEBUG:root:[ Iteration 279 ] Training loss: 0.0327226
DEBUG:root:[ Iteration 280 ] Test loss: 0.0345361
DEBUG:root:[ Iteration 282 ] Training loss: 0.0407109
DEBUG:root:[ Iteration 285 ] Training loss: 0.0362633
DEBUG:root:[ Iteration 288 ] Training loss: 0.0382046
DEBUG:root:[ Iteration 291 ] Training loss: 0.0335525
DEBUG:root:[ Iteration 294 ] Training loss: 0.0324837
DEBUG:root:[ Iteration 297 ] Training loss: 0.0306255
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-15-2016_16h35m07s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0330514
DEBUG:root:[ Iteration 0 ] Test loss: 0.0408078
DEBUG:root:[ Iteration 3 ] Training loss: 0.03197
DEBUG:root:[ Iteration 6 ] Training loss: 0.0339949
DEBUG:root:[ Iteration 9 ] Training loss: 0.0321946
DEBUG:root:[ Iteration 12 ] Training loss: 0.034498
DEBUG:root:[ Iteration 15 ] Training loss: 0.0340958
DEBUG:root:[ Iteration 18 ] Training loss: 0.0371402
DEBUG:root:[ Iteration 20 ] Test loss: 0.0304058
DEBUG:root:[ Iteration 21 ] Training loss: 0.0343558
DEBUG:root:[ Iteration 24 ] Training loss: 0.0321585
DEBUG:root:[ Iteration 27 ] Training loss: 0.0322538
DEBUG:root:[ Iteration 30 ] Training loss: 0.0391689
DEBUG:root:[ Iteration 33 ] Training loss: 0.0433319
DEBUG:root:[ Iteration 36 ] Training loss: 0.0321691
DEBUG:root:[ Iteration 39 ] Training loss: 0.0307493
DEBUG:root:[ Iteration 40 ] Test loss: 0.0382636
DEBUG:root:[ Iteration 42 ] Training loss: 0.0406326
DEBUG:root:[ Iteration 45 ] Training loss: 0.0403631
DEBUG:root:[ Iteration 48 ] Training loss: 0.0355995
DEBUG:root:[ Iteration 51 ] Training loss: 0.0313371
DEBUG:root:[ Iteration 54 ] Training loss: 0.0363139
DEBUG:root:[ Iteration 57 ] Training loss: 0.0321692
DEBUG:root:[ Iteration 60 ] Training loss: 0.0410519
DEBUG:root:[ Iteration 60 ] Test loss: 0.0324758
DEBUG:root:[ Iteration 63 ] Training loss: 0.0306732
DEBUG:root:[ Iteration 66 ] Training loss: 0.0385446
DEBUG:root:[ Iteration 69 ] Training loss: 0.0309987
DEBUG:root:[ Iteration 72 ] Training loss: 0.0304292
DEBUG:root:[ Iteration 75 ] Training loss: 0.0314181
DEBUG:root:[ Iteration 78 ] Training loss: 0.0340042
DEBUG:root:[ Iteration 80 ] Test loss: 0.0334285
DEBUG:root:[ Iteration 81 ] Training loss: 0.0319363
DEBUG:root:[ Iteration 84 ] Training loss: 0.0312962
DEBUG:root:[ Iteration 87 ] Training loss: 0.0298057
DEBUG:root:[ Iteration 90 ] Training loss: 0.0379185
DEBUG:root:[ Iteration 93 ] Training loss: 0.0309029
DEBUG:root:[ Iteration 96 ] Training loss: 0.0291569
DEBUG:root:[ Iteration 99 ] Training loss: 0.0322632
DEBUG:root:[ Iteration 100 ] Test loss: 0.034404
DEBUG:root:[ Iteration 102 ] Training loss: 0.0328521
DEBUG:root:[ Iteration 105 ] Training loss: 0.0312731
DEBUG:root:[ Iteration 108 ] Training loss: 0.0308614
DEBUG:root:[ Iteration 111 ] Training loss: 0.0306098
DEBUG:root:[ Iteration 114 ] Training loss: 0.0300362
DEBUG:root:[ Iteration 117 ] Training loss: 0.032608
DEBUG:root:[ Iteration 120 ] Training loss: 0.0370542
DEBUG:root:[ Iteration 120 ] Test loss: 0.0326471
DEBUG:root:[ Iteration 123 ] Training loss: 0.0321071
DEBUG:root:[ Iteration 126 ] Training loss: 0.0328465
DEBUG:root:[ Iteration 129 ] Training loss: 0.0316623
DEBUG:root:[ Iteration 132 ] Training loss: 0.0323011
DEBUG:root:[ Iteration 135 ] Training loss: 0.0347259
DEBUG:root:[ Iteration 138 ] Training loss: 0.0312068
DEBUG:root:[ Iteration 140 ] Test loss: 0.0300186
DEBUG:root:[ Iteration 141 ] Training loss: 0.0305263
DEBUG:root:[ Iteration 144 ] Training loss: 0.0304364
DEBUG:root:[ Iteration 147 ] Training loss: 0.0264687
DEBUG:root:[ Iteration 150 ] Training loss: 0.0280633
DEBUG:root:[ Iteration 153 ] Training loss: 0.0343695
DEBUG:root:[ Iteration 156 ] Training loss: 0.0265874
DEBUG:root:[ Iteration 159 ] Training loss: 0.0285888
DEBUG:root:[ Iteration 160 ] Test loss: 0.0329626
DEBUG:root:[ Iteration 162 ] Training loss: 0.0329177
DEBUG:root:[ Iteration 165 ] Training loss: 0.0317337
DEBUG:root:[ Iteration 168 ] Training loss: 0.0356564
DEBUG:root:[ Iteration 171 ] Training loss: 0.0227757
DEBUG:root:[ Iteration 174 ] Training loss: 0.0331207
DEBUG:root:[ Iteration 177 ] Training loss: 0.0321248
DEBUG:root:[ Iteration 180 ] Training loss: 0.0359711
DEBUG:root:[ Iteration 180 ] Test loss: 0.0336106
DEBUG:root:[ Iteration 183 ] Training loss: 0.0313476
DEBUG:root:[ Iteration 186 ] Training loss: 0.0292977
DEBUG:root:[ Iteration 189 ] Training loss: 0.038418
DEBUG:root:[ Iteration 192 ] Training loss: 0.0280009
DEBUG:root:[ Iteration 195 ] Training loss: 0.0327146
DEBUG:root:[ Iteration 198 ] Training loss: 0.0245596
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-15-2016_16h58m56s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0315011
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-16-2016_10h27m08s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.16
DEBUG:root:[ Iteration 0 ] Test loss: 0.166435
DEBUG:root:[ Iteration 3 ] Training loss: 0.165709
DEBUG:root:[ Iteration 6 ] Training loss: 0.159269
DEBUG:root:[ Iteration 9 ] Training loss: 0.160935
DEBUG:root:[ Iteration 12 ] Training loss: 0.144918
DEBUG:root:[ Iteration 15 ] Training loss: 0.145746
DEBUG:root:[ Iteration 18 ] Training loss: 0.140332
DEBUG:root:[ Iteration 20 ] Test loss: 0.127908
DEBUG:root:[ Iteration 21 ] Training loss: 0.116711
DEBUG:root:[ Iteration 24 ] Training loss: 0.0977547
DEBUG:root:[ Iteration 27 ] Training loss: 0.082839
DEBUG:root:[ Iteration 30 ] Training loss: 0.066933
DEBUG:root:[ Iteration 33 ] Training loss: 0.0614699
DEBUG:root:[ Iteration 36 ] Training loss: 0.0675228
DEBUG:root:[ Iteration 39 ] Training loss: 0.0679162
DEBUG:root:[ Iteration 40 ] Test loss: 0.0782195
DEBUG:root:[ Iteration 42 ] Training loss: 0.0708309
DEBUG:root:[ Iteration 45 ] Training loss: 0.0645255
DEBUG:root:[ Iteration 48 ] Training loss: 0.0638281
DEBUG:root:[ Iteration 51 ] Training loss: 0.0610429
DEBUG:root:[ Iteration 54 ] Training loss: 0.0697374
DEBUG:root:[ Iteration 57 ] Training loss: 0.0674171
DEBUG:root:[ Iteration 60 ] Training loss: 0.0646507
DEBUG:root:[ Iteration 60 ] Test loss: 0.0711937
DEBUG:root:[ Iteration 63 ] Training loss: 0.0621034
DEBUG:root:[ Iteration 66 ] Training loss: 0.0602805
DEBUG:root:[ Iteration 69 ] Training loss: 0.0654181
DEBUG:root:[ Iteration 72 ] Training loss: 0.0586685
DEBUG:root:[ Iteration 75 ] Training loss: 0.0653381
DEBUG:root:[ Iteration 78 ] Training loss: 0.0599195
DEBUG:root:[ Iteration 80 ] Test loss: 0.0629512
DEBUG:root:[ Iteration 81 ] Training loss: 0.0586735
DEBUG:root:[ Iteration 84 ] Training loss: 0.0568162
DEBUG:root:[ Iteration 87 ] Training loss: 0.0592366
DEBUG:root:[ Iteration 90 ] Training loss: 0.0591319
DEBUG:root:[ Iteration 93 ] Training loss: 0.0515927
DEBUG:root:[ Iteration 96 ] Training loss: 0.0618317
DEBUG:root:[ Iteration 99 ] Training loss: 0.0625601
DEBUG:root:[ Iteration 100 ] Test loss: 0.0566049
DEBUG:root:[ Iteration 102 ] Training loss: 0.0625011
DEBUG:root:[ Iteration 105 ] Training loss: 0.0600864
DEBUG:root:[ Iteration 108 ] Training loss: 0.0537022
DEBUG:root:[ Iteration 111 ] Training loss: 0.0593034
DEBUG:root:[ Iteration 114 ] Training loss: 0.0485712
DEBUG:root:[ Iteration 117 ] Training loss: 0.0599113
DEBUG:root:[ Iteration 120 ] Training loss: 0.051608
DEBUG:root:[ Iteration 120 ] Test loss: 0.0582056
DEBUG:root:[ Iteration 123 ] Training loss: 0.0484353
DEBUG:root:[ Iteration 126 ] Training loss: 0.0470838
DEBUG:root:[ Iteration 129 ] Training loss: 0.0530848
DEBUG:root:[ Iteration 132 ] Training loss: 0.0567796
DEBUG:root:[ Iteration 135 ] Training loss: 0.0468487
DEBUG:root:[ Iteration 138 ] Training loss: 0.0529634
DEBUG:root:[ Iteration 140 ] Test loss: 0.0512338
DEBUG:root:[ Iteration 141 ] Training loss: 0.0522825
DEBUG:root:[ Iteration 144 ] Training loss: 0.0441927
DEBUG:root:[ Iteration 147 ] Training loss: 0.0506638
DEBUG:root:[ Iteration 150 ] Training loss: 0.0513441
DEBUG:root:[ Iteration 153 ] Training loss: 0.056841
DEBUG:root:[ Iteration 156 ] Training loss: 0.0473981
DEBUG:root:[ Iteration 159 ] Training loss: 0.047609
DEBUG:root:[ Iteration 160 ] Test loss: 0.053218
DEBUG:root:[ Iteration 162 ] Training loss: 0.0438853
DEBUG:root:[ Iteration 165 ] Training loss: 0.0449639
DEBUG:root:[ Iteration 168 ] Training loss: 0.0475938
DEBUG:root:[ Iteration 171 ] Training loss: 0.0493474
DEBUG:root:[ Iteration 174 ] Training loss: 0.0484988
DEBUG:root:[ Iteration 177 ] Training loss: 0.0519472
DEBUG:root:[ Iteration 180 ] Training loss: 0.0403898
DEBUG:root:[ Iteration 180 ] Test loss: 0.0517955
DEBUG:root:[ Iteration 183 ] Training loss: 0.051588
DEBUG:root:[ Iteration 186 ] Training loss: 0.0497505
DEBUG:root:[ Iteration 189 ] Training loss: 0.0448763
DEBUG:root:[ Iteration 192 ] Training loss: 0.0422295
DEBUG:root:[ Iteration 195 ] Training loss: 0.0437055
DEBUG:root:[ Iteration 198 ] Training loss: 0.0495507
DEBUG:root:[ Iteration 200 ] Test loss: 0.0461325
DEBUG:root:[ Iteration 201 ] Training loss: 0.0464447
DEBUG:root:[ Iteration 204 ] Training loss: 0.0515287
DEBUG:root:[ Iteration 207 ] Training loss: 0.0436589
DEBUG:root:[ Iteration 210 ] Training loss: 0.0463504
DEBUG:root:[ Iteration 213 ] Training loss: 0.0414641
DEBUG:root:[ Iteration 216 ] Training loss: 0.0395564
DEBUG:root:[ Iteration 219 ] Training loss: 0.0493293
DEBUG:root:[ Iteration 220 ] Test loss: 0.0494762
DEBUG:root:[ Iteration 222 ] Training loss: 0.0467239
DEBUG:root:[ Iteration 225 ] Training loss: 0.0472602
DEBUG:root:[ Iteration 228 ] Training loss: 0.0353984
DEBUG:root:[ Iteration 231 ] Training loss: 0.043381
DEBUG:root:[ Iteration 234 ] Training loss: 0.0415586
DEBUG:root:[ Iteration 237 ] Training loss: 0.04821
DEBUG:root:[ Iteration 240 ] Training loss: 0.0415328
DEBUG:root:[ Iteration 240 ] Test loss: 0.0514675
DEBUG:root:[ Iteration 243 ] Training loss: 0.0433846
DEBUG:root:[ Iteration 246 ] Training loss: 0.0428423
DEBUG:root:[ Iteration 249 ] Training loss: 0.0458739
DEBUG:root:[ Iteration 252 ] Training loss: 0.0432926
DEBUG:root:[ Iteration 255 ] Training loss: 0.0406746
DEBUG:root:[ Iteration 258 ] Training loss: 0.0414804
DEBUG:root:[ Iteration 260 ] Test loss: 0.0449732
DEBUG:root:[ Iteration 261 ] Training loss: 0.0332496
DEBUG:root:[ Iteration 264 ] Training loss: 0.036285
DEBUG:root:[ Iteration 267 ] Training loss: 0.0448952
DEBUG:root:[ Iteration 270 ] Training loss: 0.0394386
DEBUG:root:[ Iteration 273 ] Training loss: 0.0445781
DEBUG:root:[ Iteration 276 ] Training loss: 0.0383186
DEBUG:root:[ Iteration 279 ] Training loss: 0.0427733
DEBUG:root:[ Iteration 280 ] Test loss: 0.0485512
DEBUG:root:[ Iteration 282 ] Training loss: 0.0387454
DEBUG:root:[ Iteration 285 ] Training loss: 0.0446047
DEBUG:root:[ Iteration 288 ] Training loss: 0.0375697
DEBUG:root:[ Iteration 291 ] Training loss: 0.0377491
DEBUG:root:[ Iteration 294 ] Training loss: 0.0364667
DEBUG:root:[ Iteration 297 ] Training loss: 0.0377177
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-16-2016_10h35m32s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.110446
DEBUG:root:[ Iteration 0 ] Test loss: 0.134681
DEBUG:root:[ Iteration 3 ] Training loss: 0.0569589
DEBUG:root:[ Iteration 6 ] Training loss: 0.0634995
DEBUG:root:[ Iteration 9 ] Training loss: 0.0484981
DEBUG:root:[ Iteration 12 ] Training loss: 0.0666529
DEBUG:root:[ Iteration 15 ] Training loss: 0.0450108
DEBUG:root:[ Iteration 18 ] Training loss: 0.0617883
DEBUG:root:[ Iteration 20 ] Test loss: 0.0541566
DEBUG:root:[ Iteration 21 ] Training loss: 0.0517875
DEBUG:root:[ Iteration 24 ] Training loss: 0.0538074
DEBUG:root:[ Iteration 27 ] Training loss: 0.046959
DEBUG:root:[ Iteration 30 ] Training loss: 0.0641497
DEBUG:root:[ Iteration 33 ] Training loss: 0.0505014
DEBUG:root:[ Iteration 36 ] Training loss: 0.0373762
DEBUG:root:[ Iteration 39 ] Training loss: 0.0428387
DEBUG:root:[ Iteration 40 ] Test loss: 0.0427011
DEBUG:root:[ Iteration 42 ] Training loss: 0.0430692
DEBUG:root:[ Iteration 45 ] Training loss: 0.0408206
DEBUG:root:[ Iteration 48 ] Training loss: 0.04259
DEBUG:root:[ Iteration 51 ] Training loss: 0.0411713
DEBUG:root:[ Iteration 54 ] Training loss: 0.0431203
DEBUG:root:[ Iteration 57 ] Training loss: 0.0360755
DEBUG:root:[ Iteration 60 ] Training loss: 0.0408743
DEBUG:root:[ Iteration 60 ] Test loss: 0.0399841
DEBUG:root:[ Iteration 63 ] Training loss: 0.0411132
DEBUG:root:[ Iteration 66 ] Training loss: 0.0325883
DEBUG:root:[ Iteration 69 ] Training loss: 0.0334018
DEBUG:root:[ Iteration 72 ] Training loss: 0.029889
DEBUG:root:[ Iteration 75 ] Training loss: 0.0366304
DEBUG:root:[ Iteration 78 ] Training loss: 0.0332654
DEBUG:root:[ Iteration 80 ] Test loss: 0.0311558
DEBUG:root:[ Iteration 81 ] Training loss: 0.0315781
DEBUG:root:[ Iteration 84 ] Training loss: 0.0332984
DEBUG:root:[ Iteration 87 ] Training loss: 0.0286463
DEBUG:root:[ Iteration 90 ] Training loss: 0.0352238
DEBUG:root:[ Iteration 93 ] Training loss: 0.0349554
DEBUG:root:[ Iteration 96 ] Training loss: 0.0336835
DEBUG:root:[ Iteration 99 ] Training loss: 0.0309003
DEBUG:root:[ Iteration 100 ] Test loss: 0.0380028
DEBUG:root:[ Iteration 102 ] Training loss: 0.0340467
DEBUG:root:[ Iteration 105 ] Training loss: 0.0360081
DEBUG:root:[ Iteration 108 ] Training loss: 0.0334179
DEBUG:root:[ Iteration 111 ] Training loss: 0.0315377
DEBUG:root:[ Iteration 114 ] Training loss: 0.0311213
DEBUG:root:[ Iteration 117 ] Training loss: 0.0392796
DEBUG:root:[ Iteration 120 ] Training loss: 0.0360808
DEBUG:root:[ Iteration 120 ] Test loss: 0.0377617
DEBUG:root:[ Iteration 123 ] Training loss: 0.0407392
DEBUG:root:[ Iteration 126 ] Training loss: 0.032099
DEBUG:root:[ Iteration 129 ] Training loss: 0.0342072
DEBUG:root:[ Iteration 132 ] Training loss: 0.0318329
DEBUG:root:[ Iteration 135 ] Training loss: 0.0300687
DEBUG:root:[ Iteration 138 ] Training loss: 0.0316563
DEBUG:root:[ Iteration 140 ] Test loss: 0.034597
DEBUG:root:[ Iteration 141 ] Training loss: 0.0334836
DEBUG:root:[ Iteration 144 ] Training loss: 0.0274207
DEBUG:root:[ Iteration 147 ] Training loss: 0.0315204
DEBUG:root:[ Iteration 150 ] Training loss: 0.0318175
DEBUG:root:[ Iteration 153 ] Training loss: 0.0315914
DEBUG:root:[ Iteration 156 ] Training loss: 0.0276003
DEBUG:root:[ Iteration 159 ] Training loss: 0.0342713
DEBUG:root:[ Iteration 160 ] Test loss: 0.041018
DEBUG:root:[ Iteration 162 ] Training loss: 0.0330141
DEBUG:root:[ Iteration 165 ] Training loss: 0.0352629
DEBUG:root:[ Iteration 168 ] Training loss: 0.0367842
DEBUG:root:[ Iteration 171 ] Training loss: 0.0308165
DEBUG:root:[ Iteration 174 ] Training loss: 0.0260731
DEBUG:root:[ Iteration 177 ] Training loss: 0.0295648
DEBUG:root:[ Iteration 180 ] Training loss: 0.0309628
DEBUG:root:[ Iteration 180 ] Test loss: 0.0352272
DEBUG:root:[ Iteration 183 ] Training loss: 0.0301885
DEBUG:root:[ Iteration 186 ] Training loss: 0.0287977
DEBUG:root:[ Iteration 189 ] Training loss: 0.0369554
DEBUG:root:[ Iteration 192 ] Training loss: 0.0339384
DEBUG:root:[ Iteration 195 ] Training loss: 0.0356087
DEBUG:root:[ Iteration 198 ] Training loss: 0.0294027
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-16-2016_10h43m14s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.148801
DEBUG:root:[ Iteration 0 ] Test loss: 0.149194
DEBUG:root:[ Iteration 3 ] Training loss: 0.153305
DEBUG:root:[ Iteration 6 ] Training loss: 0.149031
DEBUG:root:[ Iteration 9 ] Training loss: 0.142198
DEBUG:root:[ Iteration 12 ] Training loss: 0.143006
DEBUG:root:[ Iteration 15 ] Training loss: 0.130677
DEBUG:root:[ Iteration 18 ] Training loss: 0.128255
DEBUG:root:[ Iteration 20 ] Test loss: 0.11533
DEBUG:root:[ Iteration 21 ] Training loss: 0.115639
DEBUG:root:[ Iteration 24 ] Training loss: 0.0896557
DEBUG:root:[ Iteration 27 ] Training loss: 0.0670693
DEBUG:root:[ Iteration 30 ] Training loss: 0.0658098
DEBUG:root:[ Iteration 33 ] Training loss: 0.0628335
DEBUG:root:[ Iteration 36 ] Training loss: 0.0638587
DEBUG:root:[ Iteration 39 ] Training loss: 0.0678355
DEBUG:root:[ Iteration 40 ] Test loss: 0.0618421
DEBUG:root:[ Iteration 42 ] Training loss: 0.0660021
DEBUG:root:[ Iteration 45 ] Training loss: 0.0694541
DEBUG:root:[ Iteration 48 ] Training loss: 0.0640476
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-16-2016_12h06m53s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.151755
DEBUG:root:[ Iteration 0 ] Test loss: 0.148741
DEBUG:root:[ Iteration 3 ] Training loss: 0.152259
DEBUG:root:[ Iteration 6 ] Training loss: 0.147252
DEBUG:root:[ Iteration 9 ] Training loss: 0.140304
DEBUG:root:[ Iteration 12 ] Training loss: 0.146119
DEBUG:root:[ Iteration 15 ] Training loss: 0.139197
DEBUG:root:[ Iteration 18 ] Training loss: 0.132351
DEBUG:root:[ Iteration 20 ] Test loss: 0.135747
DEBUG:root:[ Iteration 21 ] Training loss: 0.124725
DEBUG:root:[ Iteration 24 ] Training loss: 0.118536
DEBUG:root:[ Iteration 27 ] Training loss: 0.0983095
DEBUG:root:[ Iteration 30 ] Training loss: 0.0828675
DEBUG:root:[ Iteration 33 ] Training loss: 0.0709918
DEBUG:root:[ Iteration 36 ] Training loss: 0.0559135
DEBUG:root:[ Iteration 39 ] Training loss: 0.0659901
DEBUG:root:[ Iteration 40 ] Test loss: 0.0655996
DEBUG:root:[ Iteration 42 ] Training loss: 0.0615656
DEBUG:root:[ Iteration 45 ] Training loss: 0.0642656
DEBUG:root:[ Iteration 48 ] Training loss: 0.0655116
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-16-2016_12h08m13s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.154076
DEBUG:root:[ Iteration 0 ] Test loss: 0.148932
DEBUG:root:[ Iteration 3 ] Training loss: 0.15407
DEBUG:root:[ Iteration 6 ] Training loss: 0.147708
DEBUG:root:[ Iteration 9 ] Training loss: 0.151573
DEBUG:root:[ Iteration 12 ] Training loss: 0.144562
DEBUG:root:[ Iteration 15 ] Training loss: 0.141187
DEBUG:root:[ Iteration 18 ] Training loss: 0.136693
DEBUG:root:[ Iteration 20 ] Test loss: 0.132792
DEBUG:root:[ Iteration 21 ] Training loss: 0.133284
DEBUG:root:[ Iteration 24 ] Training loss: 0.12521
DEBUG:root:[ Iteration 27 ] Training loss: 0.113652
DEBUG:root:[ Iteration 30 ] Training loss: 0.0923132
DEBUG:root:[ Iteration 33 ] Training loss: 0.0702213
DEBUG:root:[ Iteration 36 ] Training loss: 0.0693307
DEBUG:root:[ Iteration 39 ] Training loss: 0.0608164
DEBUG:root:[ Iteration 40 ] Test loss: 0.0715485
DEBUG:root:[ Iteration 42 ] Training loss: 0.0721401
DEBUG:root:[ Iteration 45 ] Training loss: 0.0659627
DEBUG:root:[ Iteration 48 ] Training loss: 0.0702726
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-16-2016_12h36m57s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.150663
DEBUG:root:[ Iteration 0 ] Test loss: 0.154556
DEBUG:root:[ Iteration 3 ] Training loss: 0.152039
DEBUG:root:[ Iteration 6 ] Training loss: 0.144735
DEBUG:root:[ Iteration 9 ] Training loss: 0.141646
DEBUG:root:[ Iteration 12 ] Training loss: 0.141815
DEBUG:root:[ Iteration 15 ] Training loss: 0.129742
DEBUG:root:[ Iteration 18 ] Training loss: 0.116548
DEBUG:root:[ Iteration 20 ] Test loss: 0.102813
DEBUG:root:[ Iteration 21 ] Training loss: 0.0951643
DEBUG:root:[ Iteration 24 ] Training loss: 0.0684829
DEBUG:root:[ Iteration 27 ] Training loss: 0.0699502
DEBUG:root:[ Iteration 30 ] Training loss: 0.0654928
DEBUG:root:[ Iteration 33 ] Training loss: 0.0580622
DEBUG:root:[ Iteration 36 ] Training loss: 0.0600335
DEBUG:root:[ Iteration 39 ] Training loss: 0.0735116
DEBUG:root:[ Iteration 40 ] Test loss: 0.0712133
DEBUG:root:[ Iteration 42 ] Training loss: 0.0664928
DEBUG:root:[ Iteration 45 ] Training loss: 0.0603931
DEBUG:root:[ Iteration 48 ] Training loss: 0.04994
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-16-2016_12h38m16s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0470151
DEBUG:root:[ Iteration 0 ] Test loss: 0.0397286
DEBUG:root:[ Iteration 3 ] Training loss: 0.0453206
DEBUG:root:[ Iteration 6 ] Training loss: 0.0373779
DEBUG:root:[ Iteration 9 ] Training loss: 0.0358219
DEBUG:root:[ Iteration 12 ] Training loss: 0.036594
DEBUG:root:[ Iteration 15 ] Training loss: 0.0329928
DEBUG:root:[ Iteration 18 ] Training loss: 0.027584
DEBUG:root:[ Iteration 20 ] Test loss: 0.0287112
DEBUG:root:[ Iteration 21 ] Training loss: 0.0293423
DEBUG:root:[ Iteration 24 ] Training loss: 0.0350481
DEBUG:root:[ Iteration 27 ] Training loss: 0.026586
DEBUG:root:[ Iteration 30 ] Training loss: 0.0301993
DEBUG:root:[ Iteration 33 ] Training loss: 0.0294825
DEBUG:root:[ Iteration 36 ] Training loss: 0.0304924
DEBUG:root:[ Iteration 39 ] Training loss: 0.0287742
DEBUG:root:[ Iteration 40 ] Test loss: 0.0258611
DEBUG:root:[ Iteration 42 ] Training loss: 0.0263853
DEBUG:root:[ Iteration 45 ] Training loss: 0.0298596
DEBUG:root:[ Iteration 48 ] Training loss: 0.0291084
DEBUG:root:[ Iteration 51 ] Training loss: 0.03117
DEBUG:root:[ Iteration 54 ] Training loss: 0.0277844
DEBUG:root:[ Iteration 57 ] Training loss: 0.0245029
DEBUG:root:[ Iteration 60 ] Training loss: 0.0232648
DEBUG:root:[ Iteration 60 ] Test loss: 0.0254904
DEBUG:root:[ Iteration 63 ] Training loss: 0.024942
DEBUG:root:[ Iteration 66 ] Training loss: 0.0229711
DEBUG:root:[ Iteration 69 ] Training loss: 0.0237222
DEBUG:root:[ Iteration 72 ] Training loss: 0.0258263
DEBUG:root:[ Iteration 75 ] Training loss: 0.0245218
DEBUG:root:[ Iteration 78 ] Training loss: 0.0224979
DEBUG:root:[ Iteration 80 ] Test loss: 0.0227406
DEBUG:root:[ Iteration 81 ] Training loss: 0.0248984
DEBUG:root:[ Iteration 84 ] Training loss: 0.0199164
DEBUG:root:[ Iteration 87 ] Training loss: 0.0219499
DEBUG:root:[ Iteration 90 ] Training loss: 0.0199476
DEBUG:root:[ Iteration 93 ] Training loss: 0.0200908
DEBUG:root:[ Iteration 96 ] Training loss: 0.0219146
DEBUG:root:[ Iteration 99 ] Training loss: 0.0222378
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-16-2016_17h12m59s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0422206
DEBUG:root:[ Iteration 0 ] Test loss: 0.0386633
DEBUG:root:[ Iteration 3 ] Training loss: 0.0400055
DEBUG:root:[ Iteration 6 ] Training loss: 0.0410547
DEBUG:root:[ Iteration 9 ] Training loss: 0.0375213
DEBUG:root:[ Iteration 12 ] Training loss: 0.0347681
DEBUG:root:[ Iteration 15 ] Training loss: 0.032556
DEBUG:root:[ Iteration 18 ] Training loss: 0.0326578
DEBUG:root:[ Iteration 20 ] Test loss: 0.0318256
DEBUG:root:[ Iteration 21 ] Training loss: 0.0319803
DEBUG:root:[ Iteration 24 ] Training loss: 0.0311442
DEBUG:root:[ Iteration 27 ] Training loss: 0.0293826
DEBUG:root:[ Iteration 30 ] Training loss: 0.0301915
DEBUG:root:[ Iteration 33 ] Training loss: 0.0276998
DEBUG:root:[ Iteration 36 ] Training loss: 0.0289123
DEBUG:root:[ Iteration 39 ] Training loss: 0.0316194
DEBUG:root:[ Iteration 40 ] Test loss: 0.0289076
DEBUG:root:[ Iteration 42 ] Training loss: 0.0290394
DEBUG:root:[ Iteration 45 ] Training loss: 0.0287511
DEBUG:root:[ Iteration 48 ] Training loss: 0.0239003
DEBUG:root:[ Iteration 51 ] Training loss: 0.0281116
DEBUG:root:[ Iteration 54 ] Training loss: 0.0237696
DEBUG:root:[ Iteration 57 ] Training loss: 0.0238463
DEBUG:root:[ Iteration 60 ] Training loss: 0.0304618
DEBUG:root:[ Iteration 60 ] Test loss: 0.0272815
DEBUG:root:[ Iteration 63 ] Training loss: 0.0268438
DEBUG:root:[ Iteration 66 ] Training loss: 0.0258814
DEBUG:root:[ Iteration 69 ] Training loss: 0.0261505
DEBUG:root:[ Iteration 72 ] Training loss: 0.0242753
DEBUG:root:[ Iteration 75 ] Training loss: 0.0251186
DEBUG:root:[ Iteration 78 ] Training loss: 0.0233936
DEBUG:root:[ Iteration 80 ] Test loss: 0.0273054
DEBUG:root:[ Iteration 81 ] Training loss: 0.0243036
DEBUG:root:[ Iteration 84 ] Training loss: 0.0223659
DEBUG:root:[ Iteration 87 ] Training loss: 0.0261306
DEBUG:root:[ Iteration 90 ] Training loss: 0.0217014
DEBUG:root:[ Iteration 93 ] Training loss: 0.0238526
DEBUG:root:[ Iteration 96 ] Training loss: 0.0263953
DEBUG:root:[ Iteration 99 ] Training loss: 0.0214477
DEBUG:root:[ Iteration 100 ] Test loss: 0.0226416
DEBUG:root:[ Iteration 102 ] Training loss: 0.0212536
DEBUG:root:[ Iteration 105 ] Training loss: 0.0185152
DEBUG:root:[ Iteration 108 ] Training loss: 0.0178477
DEBUG:root:[ Iteration 111 ] Training loss: 0.0185858
DEBUG:root:[ Iteration 114 ] Training loss: 0.0187744
DEBUG:root:[ Iteration 117 ] Training loss: 0.0194837
DEBUG:root:[ Iteration 120 ] Training loss: 0.0189008
DEBUG:root:[ Iteration 120 ] Test loss: 0.022784
DEBUG:root:[ Iteration 123 ] Training loss: 0.0223113
DEBUG:root:[ Iteration 126 ] Training loss: 0.0193179
DEBUG:root:[ Iteration 129 ] Training loss: 0.016578
DEBUG:root:[ Iteration 132 ] Training loss: 0.0202977
DEBUG:root:[ Iteration 135 ] Training loss: 0.0163494
DEBUG:root:[ Iteration 138 ] Training loss: 0.019481
DEBUG:root:[ Iteration 140 ] Test loss: 0.022577
DEBUG:root:[ Iteration 141 ] Training loss: 0.0161573
DEBUG:root:[ Iteration 144 ] Training loss: 0.0175643
DEBUG:root:[ Iteration 147 ] Training loss: 0.0160835
DEBUG:root:[ Iteration 150 ] Training loss: 0.017059
DEBUG:root:[ Iteration 153 ] Training loss: 0.0159661
DEBUG:root:[ Iteration 156 ] Training loss: 0.0163833
DEBUG:root:[ Iteration 159 ] Training loss: 0.0182541
DEBUG:root:[ Iteration 160 ] Test loss: 0.019626
DEBUG:root:[ Iteration 162 ] Training loss: 0.0191297
DEBUG:root:[ Iteration 165 ] Training loss: 0.01656
DEBUG:root:[ Iteration 168 ] Training loss: 0.0168028
DEBUG:root:[ Iteration 171 ] Training loss: 0.0186537
DEBUG:root:[ Iteration 174 ] Training loss: 0.0155855
DEBUG:root:[ Iteration 177 ] Training loss: 0.018104
DEBUG:root:[ Iteration 180 ] Training loss: 0.0142333
DEBUG:root:[ Iteration 180 ] Test loss: 0.0165629
DEBUG:root:[ Iteration 183 ] Training loss: 0.0140189
DEBUG:root:[ Iteration 186 ] Training loss: 0.0148237
DEBUG:root:[ Iteration 189 ] Training loss: 0.0145811
DEBUG:root:[ Iteration 192 ] Training loss: 0.0137468
DEBUG:root:[ Iteration 195 ] Training loss: 0.0138982
DEBUG:root:[ Iteration 198 ] Training loss: 0.0106347
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-16-2016_17h21m48s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0437648
DEBUG:root:[ Iteration 0 ] Test loss: 0.0460558
DEBUG:root:[ Iteration 3 ] Training loss: 0.0509285
DEBUG:root:[ Iteration 6 ] Training loss: 0.044598
DEBUG:root:[ Iteration 9 ] Training loss: 0.0432681
DEBUG:root:[ Iteration 12 ] Training loss: 0.0481833
DEBUG:root:[ Iteration 15 ] Training loss: 0.0454418
DEBUG:root:[ Iteration 18 ] Training loss: 0.0487392
DEBUG:root:[ Iteration 20 ] Test loss: 0.0396749
DEBUG:root:[ Iteration 21 ] Training loss: 0.0429527
DEBUG:root:[ Iteration 24 ] Training loss: 0.0474847
DEBUG:root:[ Iteration 27 ] Training loss: 0.0394616
DEBUG:root:[ Iteration 30 ] Training loss: 0.0399845
DEBUG:root:[ Iteration 33 ] Training loss: 0.049357
DEBUG:root:[ Iteration 36 ] Training loss: 0.0455998
DEBUG:root:[ Iteration 39 ] Training loss: 0.0479568
DEBUG:root:[ Iteration 40 ] Test loss: 0.0446403
DEBUG:root:[ Iteration 42 ] Training loss: 0.0397667
DEBUG:root:[ Iteration 45 ] Training loss: 0.0382335
DEBUG:root:[ Iteration 48 ] Training loss: 0.0444938
DEBUG:root:[ Iteration 51 ] Training loss: 0.0418839
DEBUG:root:[ Iteration 54 ] Training loss: 0.0397272
DEBUG:root:[ Iteration 57 ] Training loss: 0.0411346
DEBUG:root:[ Iteration 60 ] Training loss: 0.0383852
DEBUG:root:[ Iteration 60 ] Test loss: 0.043528
DEBUG:root:[ Iteration 63 ] Training loss: 0.0392792
DEBUG:root:[ Iteration 66 ] Training loss: 0.0435716
DEBUG:root:[ Iteration 69 ] Training loss: 0.0391236
DEBUG:root:[ Iteration 72 ] Training loss: 0.0504751
DEBUG:root:[ Iteration 75 ] Training loss: 0.0377717
DEBUG:root:[ Iteration 78 ] Training loss: 0.0359867
DEBUG:root:[ Iteration 80 ] Test loss: 0.0395563
DEBUG:root:[ Iteration 81 ] Training loss: 0.0364323
DEBUG:root:[ Iteration 84 ] Training loss: 0.0394743
DEBUG:root:[ Iteration 87 ] Training loss: 0.0409949
DEBUG:root:[ Iteration 90 ] Training loss: 0.0324385
DEBUG:root:[ Iteration 93 ] Training loss: 0.0401911
DEBUG:root:[ Iteration 96 ] Training loss: 0.0396974
DEBUG:root:[ Iteration 99 ] Training loss: 0.0389042
DEBUG:root:[ Iteration 100 ] Test loss: 0.0392171
DEBUG:root:[ Iteration 102 ] Training loss: 0.0388887
DEBUG:root:[ Iteration 105 ] Training loss: 0.039401
DEBUG:root:[ Iteration 108 ] Training loss: 0.0417231
DEBUG:root:[ Iteration 111 ] Training loss: 0.0391842
DEBUG:root:[ Iteration 114 ] Training loss: 0.0382625
DEBUG:root:[ Iteration 117 ] Training loss: 0.0319284
DEBUG:root:[ Iteration 120 ] Training loss: 0.0410672
DEBUG:root:[ Iteration 120 ] Test loss: 0.0344076
DEBUG:root:[ Iteration 123 ] Training loss: 0.0385136
DEBUG:root:[ Iteration 126 ] Training loss: 0.0341915
DEBUG:root:[ Iteration 129 ] Training loss: 0.0345177
DEBUG:root:[ Iteration 132 ] Training loss: 0.0343971
DEBUG:root:[ Iteration 135 ] Training loss: 0.0453522
DEBUG:root:[ Iteration 138 ] Training loss: 0.0326889
DEBUG:root:[ Iteration 140 ] Test loss: 0.0308166
DEBUG:root:[ Iteration 141 ] Training loss: 0.0327101
DEBUG:root:[ Iteration 144 ] Training loss: 0.0362296
DEBUG:root:[ Iteration 147 ] Training loss: 0.0374308
DEBUG:root:[ Iteration 150 ] Training loss: 0.0367071
DEBUG:root:[ Iteration 153 ] Training loss: 0.0322919
DEBUG:root:[ Iteration 156 ] Training loss: 0.0343847
DEBUG:root:[ Iteration 159 ] Training loss: 0.0277257
DEBUG:root:[ Iteration 160 ] Test loss: 0.0358039
DEBUG:root:[ Iteration 162 ] Training loss: 0.0324046
DEBUG:root:[ Iteration 165 ] Training loss: 0.0299294
DEBUG:root:[ Iteration 168 ] Training loss: 0.0329894
DEBUG:root:[ Iteration 171 ] Training loss: 0.0354719
DEBUG:root:[ Iteration 174 ] Training loss: 0.0327858
DEBUG:root:[ Iteration 177 ] Training loss: 0.0333881
DEBUG:root:[ Iteration 180 ] Training loss: 0.0361462
DEBUG:root:[ Iteration 180 ] Test loss: 0.0312793
DEBUG:root:[ Iteration 183 ] Training loss: 0.0312081
DEBUG:root:[ Iteration 186 ] Training loss: 0.0282613
DEBUG:root:[ Iteration 189 ] Training loss: 0.03125
DEBUG:root:[ Iteration 192 ] Training loss: 0.0280465
DEBUG:root:[ Iteration 195 ] Training loss: 0.0298169
DEBUG:root:[ Iteration 198 ] Training loss: 0.0251039
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-17-2016_10h20m50s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 3.21373
DEBUG:root:[ Iteration 0 ] Test loss: 3.21394
DEBUG:root:[ Iteration 3 ] Training loss: 2.76187
DEBUG:root:[ Iteration 6 ] Training loss: 1.84994
DEBUG:root:[ Iteration 9 ] Training loss: 1.07491
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-17-2016_14h52m52s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 3.21436
DEBUG:root:[ Iteration 0 ] Test loss: 3.21415
DEBUG:root:[ Iteration 3 ] Training loss: 2.94587
DEBUG:root:[ Iteration 6 ] Training loss: 2.29017
DEBUG:root:[ Iteration 9 ] Training loss: 1.39242
DEBUG:root:[ Iteration 12 ] Training loss: 0.826398
DEBUG:root:[ Iteration 15 ] Training loss: 1.02515
DEBUG:root:[ Iteration 18 ] Training loss: 0.78142
DEBUG:root:[ Iteration 20 ] Test loss: 0.775809
DEBUG:root:[ Iteration 21 ] Training loss: 0.79959
DEBUG:root:[ Iteration 24 ] Training loss: 0.784462
DEBUG:root:[ Iteration 27 ] Training loss: 0.718682
DEBUG:root:[ Iteration 30 ] Training loss: 0.688853
DEBUG:root:[ Iteration 33 ] Training loss: 0.823724
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-17-2016_14h54m56s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.0
DEBUG:root:[ Iteration 0 ] Test loss: 0.0
DEBUG:root:[ Iteration 3 ] Training loss: 0.845
DEBUG:root:[ Iteration 6 ] Training loss: 0.8275
DEBUG:root:[ Iteration 9 ] Training loss: 0.8175
DEBUG:root:[ Iteration 12 ] Training loss: 0.8375
DEBUG:root:[ Iteration 15 ] Training loss: 0.8275
DEBUG:root:[ Iteration 18 ] Training loss: 0.815
DEBUG:root:[ Iteration 20 ] Test loss: 0.8525
DEBUG:root:[ Iteration 21 ] Training loss: 0.8325
DEBUG:root:[ Iteration 24 ] Training loss: 0.8525
DEBUG:root:[ Iteration 27 ] Training loss: 0.8175
DEBUG:root:[ Iteration 30 ] Training loss: 0.8925
DEBUG:root:[ Iteration 33 ] Training loss: 0.855
DEBUG:root:[ Iteration 36 ] Training loss: 0.84
DEBUG:root:[ Iteration 39 ] Training loss: 0.8275
DEBUG:root:[ Iteration 40 ] Test loss: 0.835
DEBUG:root:[ Iteration 42 ] Training loss: 0.86
DEBUG:root:[ Iteration 45 ] Training loss: 0.9025
DEBUG:root:[ Iteration 48 ] Training loss: 0.865
DEBUG:root:[ Iteration 51 ] Training loss: 0.8625
DEBUG:root:[ Iteration 54 ] Training loss: 0.925
DEBUG:root:[ Iteration 57 ] Training loss: 0.8725
DEBUG:root:[ Iteration 60 ] Training loss: 0.8875
DEBUG:root:[ Iteration 60 ] Test loss: 0.89
DEBUG:root:[ Iteration 63 ] Training loss: 0.8875
DEBUG:root:[ Iteration 66 ] Training loss: 0.89
DEBUG:root:[ Iteration 69 ] Training loss: 0.8825
DEBUG:root:[ Iteration 72 ] Training loss: 0.8925
DEBUG:root:[ Iteration 75 ] Training loss: 0.89
DEBUG:root:[ Iteration 78 ] Training loss: 0.885
DEBUG:root:[ Iteration 80 ] Test loss: 0.875
DEBUG:root:[ Iteration 81 ] Training loss: 0.8975
DEBUG:root:[ Iteration 84 ] Training loss: 0.9
DEBUG:root:[ Iteration 87 ] Training loss: 0.91
DEBUG:root:[ Iteration 90 ] Training loss: 0.875
DEBUG:root:[ Iteration 93 ] Training loss: 0.915
DEBUG:root:[ Iteration 96 ] Training loss: 0.93
DEBUG:root:[ Iteration 99 ] Training loss: 0.905
DEBUG:root:[ Iteration 100 ] Test loss: 0.91
DEBUG:root:[ Iteration 102 ] Training loss: 0.91
DEBUG:root:[ Iteration 105 ] Training loss: 0.89
DEBUG:root:[ Iteration 108 ] Training loss: 0.89
DEBUG:root:[ Iteration 111 ] Training loss: 0.915
DEBUG:root:[ Iteration 114 ] Training loss: 0.905
DEBUG:root:[ Iteration 117 ] Training loss: 0.89
DEBUG:root:[ Iteration 120 ] Training loss: 0.9025
DEBUG:root:[ Iteration 120 ] Test loss: 0.9
DEBUG:root:[ Iteration 123 ] Training loss: 0.925
DEBUG:root:[ Iteration 126 ] Training loss: 0.915
DEBUG:root:[ Iteration 129 ] Training loss: 0.9425
DEBUG:root:[ Iteration 132 ] Training loss: 0.905
DEBUG:root:[ Iteration 135 ] Training loss: 0.9
DEBUG:root:[ Iteration 138 ] Training loss: 0.9125
DEBUG:root:[ Iteration 140 ] Test loss: 0.905
DEBUG:root:[ Iteration 141 ] Training loss: 0.9225
DEBUG:root:[ Iteration 144 ] Training loss: 0.92
DEBUG:root:[ Iteration 147 ] Training loss: 0.9275
DEBUG:root:[ Iteration 150 ] Training loss: 0.9075
DEBUG:root:[ Iteration 153 ] Training loss: 0.9125
DEBUG:root:[ Iteration 156 ] Training loss: 0.93
DEBUG:root:[ Iteration 159 ] Training loss: 0.92
DEBUG:root:[ Iteration 160 ] Test loss: 0.9325
DEBUG:root:[ Iteration 162 ] Training loss: 0.935
DEBUG:root:[ Iteration 165 ] Training loss: 0.9175
DEBUG:root:[ Iteration 168 ] Training loss: 0.9175
DEBUG:root:[ Iteration 171 ] Training loss: 0.925
DEBUG:root:[ Iteration 174 ] Training loss: 0.9125
DEBUG:root:[ Iteration 177 ] Training loss: 0.93
DEBUG:root:[ Iteration 180 ] Training loss: 0.91
DEBUG:root:[ Iteration 180 ] Test loss: 0.915
DEBUG:root:[ Iteration 183 ] Training loss: 0.92
DEBUG:root:[ Iteration 186 ] Training loss: 0.9
DEBUG:root:[ Iteration 189 ] Training loss: 0.92
DEBUG:root:[ Iteration 192 ] Training loss: 0.9125
DEBUG:root:[ Iteration 195 ] Training loss: 0.935
DEBUG:root:[ Iteration 198 ] Training loss: 0.915
DEBUG:root:[ Iteration 200 ] Test loss: 0.915
DEBUG:root:[ Iteration 201 ] Training loss: 0.905
DEBUG:root:[ Iteration 204 ] Training loss: 0.905
DEBUG:root:[ Iteration 207 ] Training loss: 0.935
DEBUG:root:[ Iteration 210 ] Training loss: 0.945
DEBUG:root:[ Iteration 213 ] Training loss: 0.9175
DEBUG:root:[ Iteration 216 ] Training loss: 0.9375
DEBUG:root:[ Iteration 219 ] Training loss: 0.9225
DEBUG:root:[ Iteration 220 ] Test loss: 0.9275
DEBUG:root:[ Iteration 222 ] Training loss: 0.9125
DEBUG:root:[ Iteration 225 ] Training loss: 0.915
DEBUG:root:[ Iteration 228 ] Training loss: 0.9375
DEBUG:root:[ Iteration 231 ] Training loss: 0.9175
DEBUG:root:[ Iteration 234 ] Training loss: 0.8925
DEBUG:root:[ Iteration 237 ] Training loss: 0.9225
DEBUG:root:[ Iteration 240 ] Training loss: 0.92
DEBUG:root:[ Iteration 240 ] Test loss: 0.925
DEBUG:root:[ Iteration 243 ] Training loss: 0.9375
DEBUG:root:[ Iteration 246 ] Training loss: 0.9325
DEBUG:root:[ Iteration 249 ] Training loss: 0.9375
DEBUG:root:[ Iteration 252 ] Training loss: 0.9375
DEBUG:root:[ Iteration 255 ] Training loss: 0.925
DEBUG:root:[ Iteration 258 ] Training loss: 0.9375
DEBUG:root:[ Iteration 260 ] Test loss: 0.895
DEBUG:root:[ Iteration 261 ] Training loss: 0.9275
DEBUG:root:[ Iteration 264 ] Training loss: 0.935
DEBUG:root:[ Iteration 267 ] Training loss: 0.94
DEBUG:root:[ Iteration 270 ] Training loss: 0.9325
DEBUG:root:[ Iteration 273 ] Training loss: 0.925
DEBUG:root:[ Iteration 276 ] Training loss: 0.9525
DEBUG:root:[ Iteration 279 ] Training loss: 0.915
DEBUG:root:[ Iteration 280 ] Test loss: 0.93
DEBUG:root:[ Iteration 282 ] Training loss: 0.9375
DEBUG:root:[ Iteration 285 ] Training loss: 0.9475
DEBUG:root:[ Iteration 288 ] Training loss: 0.9475
DEBUG:root:[ Iteration 291 ] Training loss: 0.925
DEBUG:root:[ Iteration 294 ] Training loss: 0.95
DEBUG:root:[ Iteration 297 ] Training loss: 0.935
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-17-2016_15h03m52s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.]
DEBUG:root:[ Iteration 0 ] Test loss: [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.]
DEBUG:root:[ Iteration 3 ] Training loss: [ 0.5  1.   1.   1.   0.5  0.5  0.5  0.5  1.   0.5  1.   1.   0.5  0.5  0.
  1.   1.   0.5  0.5  1.   0.5  1.   0.5  0.5  0.5  1.   1.   1.   1.   1.
  1.   0.5  0.5  0.5  1.   1.   1.   0.5  0.5  1.   0.5  1.   0.5  1.   1.
  1.   1.   1.   1.   0.5  1.   0.5  1.   1.   1.   1.   1.   1.   1.   0.5
  0.5  0.5  0.5  0.5  0.5  0.5  0.5  1.   1.   1.   0.5  1.   1.   0.5  0.5
  0.   1.   1.   1.   1.   0.5  1.   1.   1.   1.   0.5  0.5  0.5  0.5  1.
  0.5  0.5  1.   0.5  0.5  1.   0.   1.   0.5  0.5  1.   0.5  1.   0.5  1.
  0.5  1.   1.   0.5  0.5  0.5  0.5  0.5  1.   0.5  0.5  0.5  0.5  1.   1.
  0.5  1.   0.5  1.   0.5  0.5  0.5  1.   0.5  1.   1.   1.   0.5  1.   0.
  1.   1.   0.   1.   1.   1.   1.   0.5  1.   1.   1.   0.5  0.5  0.5  0.5
  1.   1.   1.   1.   0.   1.   0.5  0.5  1.   0.5  1.   1.   0.5  0.5  1.
  1.   0.5  0.5  1.   1.   1.   0.   0.5  1.   0.5  0.5  1.   1.   1.   0.5
  1.   0.5  1.   1.   1.   0.5  0.5  0.5  0.5  0.5  1.   0.5  0.5  0.5  0.5
  1.   1.   0.5  1.   1. ]
DEBUG:root:[ Iteration 6 ] Training loss: [ 0.5  1.   1.   1.   0.5  1.   0.   0.5  1.   0.5  1.   0.5  0.5  0.5  0.5
  1.   0.5  1.   1.   0.5  1.   0.5  1.   0.5  0.5  1.   1.   1.   0.5  1.
  1.   0.5  1.   0.5  1.   1.   0.5  0.5  1.   1.   0.5  1.   1.   0.5  1.
  0.5  0.5  1.   1.   1.   1.   0.5  0.5  0.5  1.   1.   1.   1.   1.   1.
  1.   0.5  1.   1.   1.   1.   0.5  1.   1.   0.5  0.5  0.5  1.   1.   0.
  1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.5  0.5  0.5  0.5  0.5
  1.   1.   0.5  1.   1.   0.5  0.5  0.5  1.   1.   1.   0.5  0.5  0.5  0.5
  1.   1.   0.5  1.   1.   0.5  0.5  1.   1.   0.5  0.5  0.5  0.5  0.5  0.5
  1.   1.   1.   0.5  0.5  1.   0.5  1.   0.5  0.5  1.   0.   0.5  0.5  1.
  1.   0.5  0.5  0.5  1.   1.   0.5  0.5  1.   1.   1.   0.5  1.   1.   1.
  0.5  1.   1.   0.5  0.5  1.   0.5  0.5  1.   1.   0.5  0.5  1.   1.   1.
  1.   0.   0.5  1.   1.   0.5  0.5  1.   1.   0.5  1.   1.   1.   1.   1.
  0.5  1.   1.   0.5  1.   1.   1.   1.   0.5  1.   0.5  1.   1.   1.   0.5
  0.5  1.   1.   0.5  1. ]
DEBUG:root:[ Iteration 9 ] Training loss: [ 0.   1.   1.   0.5  1.   1.   1.   1.   1.   0.5  0.5  0.5  1.   1.   0.5
  1.   0.5  0.5  1.   0.5  1.   0.5  1.   1.   0.   0.5  1.   1.   1.   1.
  0.5  1.   0.5  0.5  1.   1.   1.   1.   0.5  1.   1.   0.5  1.   1.   0.5
  0.5  0.5  0.5  1.   1.   0.5  1.   0.5  1.   0.5  0.5  1.   1.   0.5  0.5
  1.   1.   1.   1.   0.5  1.   0.5  1.   0.5  1.   0.5  1.   1.   0.5  1.
  1.   0.5  0.5  1.   0.5  1.   0.5  0.5  0.5  0.5  0.5  1.   0.5  1.   1.
  1.   0.5  0.5  0.5  1.   1.   0.5  0.5  1.   1.   1.   0.5  0.5  0.5  1.
  1.   1.   0.5  1.   1.   0.5  1.   1.   1.   1.   1.   1.   1.   0.5  1.
  1.   1.   0.5  1.   1.   1.   0.5  0.5  1.   1.   1.   1.   0.5  0.5  1.
  1.   1.   0.5  0.5  1.   1.   0.5  1.   1.   1.   1.   0.5  0.5  0.5  0.5
  0.5  0.5  1.   1.   1.   1.   0.5  1.   0.5  0.5  1.   1.   0.5  1.   0.
  1.   1.   0.5  1.   1.   0.5  1.   0.   1.   0.5  1.   0.5  1.   1.   0.5
  0.5  0.5  1.   1.   1.   0.5  1.   1.   1.   0.5  1.   0.   0.   0.5  0.5
  0.5  1.   1.   0.5  1. ]
DEBUG:root:[ Iteration 12 ] Training loss: [ 0.5  0.5  0.5  1.   0.5  1.   0.5  1.   0.5  1.   1.   0.5  1.   1.   1.
  1.   1.   1.   0.5  1.   1.   0.5  1.   1.   0.5  0.5  1.   0.5  0.5  1.
  1.   0.5  0.5  0.5  1.   1.   1.   0.5  0.5  0.5  0.5  1.   0.5  1.   1.
  1.   0.5  0.5  1.   1.   1.   0.5  0.   1.   0.5  1.   1.   1.   0.5  1.
  1.   0.5  0.5  0.5  1.   0.5  0.5  1.   0.5  0.   0.5  1.   1.   0.5  0.5
  1.   1.   1.   0.5  1.   1.   1.   1.   1.   0.5  1.   1.   1.   1.   1.
  0.5  0.5  0.5  0.5  1.   1.   1.   0.5  0.5  1.   1.   1.   1.   1.   0.5
  1.   0.5  0.5  0.5  1.   1.   0.5  0.   0.5  1.   1.   1.   1.   1.   0.5
  0.5  1.   0.5  1.   1.   1.   1.   0.5  0.5  0.5  1.   1.   0.5  1.   1.
  1.   1.   1.   0.5  1.   1.   1.   0.   1.   1.   1.   0.5  0.5  1.   0.5
  1.   0.5  1.   0.   0.5  0.5  1.   0.5  1.   1.   1.   1.   1.   0.5  0.5
  1.   1.   0.5  0.5  0.5  1.   1.   1.   1.   1.   1.   1.   0.5  1.   1.
  0.5  1.   0.5  0.5  0.5  1.   1.   0.   1.   1.   1.   1.   0.5  0.5  0.5
  1.   0.5  1.   1.   0.5]
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-20-2016_11h18m46s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.545
DEBUG:root:[ Iteration 0 ] Test loss: 0.57
DEBUG:root:[ Iteration 3 ] Training loss: 0.7625
DEBUG:root:[ Iteration 6 ] Training loss: 0.75
DEBUG:root:[ Iteration 9 ] Training loss: 0.7525
DEBUG:root:[ Iteration 12 ] Training loss: 0.7425
DEBUG:root:[ Iteration 15 ] Training loss: 0.7975
DEBUG:root:[ Iteration 18 ] Training loss: 0.78
DEBUG:root:[ Iteration 20 ] Test loss: 0.8175
DEBUG:root:[ Iteration 21 ] Training loss: 0.7875
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-20-2016_11h20m41s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: [[ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]]
DEBUG:root:[ Iteration 0 ] Test loss: [[ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]]
DEBUG:root:[ Iteration 3 ] Training loss: [[ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]]
DEBUG:root:[ Iteration 6 ] Training loss: [[ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]]
DEBUG:root:[ Iteration 9 ] Training loss: [[ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]]
DEBUG:root:[ Iteration 12 ] Training loss: [[ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]]
DEBUG:root:[ Iteration 15 ] Training loss: [[ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]
 [ 0.14884758  0.14884758  0.40460968  0.14884758  0.14884758]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.14884758  0.14884758  0.14884758  0.14884758  0.40460968]
 [ 0.40460971  0.14884759  0.14884759  0.14884759  0.14884759]]
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-20-2016_11h23m11s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: [[ 0.19951375  0.20031556  0.2006235   0.2002601   0.19928704]
 [ 0.1995329   0.20026326  0.20060053  0.20024401  0.19935931]
 [ 0.19948995  0.20025255  0.20061767  0.200298    0.19934188]
 [ 0.19945477  0.20027414  0.20063433  0.20030397  0.1993328 ]
 [ 0.19953863  0.20025291  0.20054828  0.20028856  0.19937167]
 [ 0.19952321  0.20039406  0.20062013  0.20024738  0.19921522]
 [ 0.19943273  0.20024374  0.20061629  0.20030427  0.19940293]
 [ 0.19954728  0.20022862  0.20055287  0.20032358  0.1993477 ]
 [ 0.1995196   0.20030148  0.20034061  0.20041709  0.19942123]
 [ 0.19947742  0.20022343  0.20063072  0.20032082  0.1993476 ]
 [ 0.19955605  0.20027198  0.20055397  0.2003651   0.19925295]
 [ 0.19948797  0.20028579  0.20063609  0.20026219  0.19932798]
 [ 0.19951738  0.20024389  0.20056488  0.20029064  0.19938326]
 [ 0.19945535  0.2002454   0.20056859  0.20034283  0.1993878 ]
 [ 0.19957574  0.20027864  0.20050372  0.20039438  0.19924752]
 [ 0.19953385  0.20023887  0.20050286  0.20032769  0.19939668]
 [ 0.19946568  0.20025648  0.2005989   0.20031147  0.19936743]
 [ 0.19953491  0.20022713  0.20056549  0.20024498  0.19942747]
 [ 0.19956449  0.20019881  0.20047808  0.20036842  0.19939022]
 [ 0.19945918  0.2002335   0.20057847  0.20029306  0.19943574]
 [ 0.19953924  0.2003054   0.20044571  0.20033579  0.19937383]
 [ 0.19948325  0.20024706  0.20057638  0.20029466  0.19939864]
 [ 0.19954412  0.20018733  0.20041826  0.20036241  0.19948788]
 [ 0.19951621  0.20028898  0.20061308  0.20029619  0.19928554]
 [ 0.19944689  0.2003309   0.20063759  0.20031202  0.19927256]
 [ 0.19948089  0.20025299  0.20064262  0.2003172   0.19930626]
 [ 0.19951321  0.20022821  0.20062642  0.20027831  0.19935386]
 [ 0.19951339  0.20023991  0.20059456  0.20037302  0.19927913]
 [ 0.19956037  0.20024279  0.20056611  0.20038058  0.19925018]
 [ 0.19949895  0.20028105  0.20056027  0.2003032   0.19935647]
 [ 0.19946073  0.20024908  0.20056243  0.20034046  0.1993873 ]
 [ 0.19946307  0.20028225  0.20046337  0.20034237  0.1994489 ]
 [ 0.19949271  0.2003414   0.20055127  0.20041803  0.19919659]
 [ 0.19961224  0.20020184  0.20056993  0.20037699  0.199239  ]
 [ 0.19958374  0.20021281  0.20054765  0.20029159  0.1993642 ]
 [ 0.19944842  0.20046121  0.20058571  0.20028356  0.19922113]
 [ 0.19949201  0.2002591   0.20058154  0.20024018  0.19942713]
 [ 0.19960855  0.20021033  0.20055684  0.2003862   0.19923808]
 [ 0.19945505  0.20021835  0.20056558  0.2003625   0.19939852]
 [ 0.1994016   0.20044467  0.20064594  0.20021969  0.19928804]
 [ 0.19958836  0.20028394  0.20055214  0.20031561  0.19926003]
 [ 0.19959433  0.20030528  0.20041633  0.200344    0.19934011]
 [ 0.1994347   0.20026217  0.20063254  0.20033357  0.19933704]
 [ 0.19947383  0.20022677  0.20067179  0.20026352  0.19936408]
 [ 0.19956906  0.20025888  0.2005779   0.20035177  0.19924241]
 [ 0.19948387  0.20038512  0.20055482  0.20036387  0.1992123 ]
 [ 0.1995196   0.20026083  0.20057273  0.20025924  0.19938761]
 [ 0.19944055  0.20026857  0.20061648  0.20033532  0.19933911]
 [ 0.19951987  0.20030908  0.20060693  0.20025574  0.19930843]
 [ 0.19952478  0.20027295  0.20063099  0.20033094  0.1992404 ]
 [ 0.19941379  0.20034303  0.20064367  0.20034502  0.19925447]
 [ 0.1994784   0.20029092  0.2005839   0.20031179  0.19933502]
 [ 0.19953182  0.20025198  0.20054433  0.2002707   0.19940113]
 [ 0.19949298  0.20026839  0.20062408  0.20030175  0.19931281]
 [ 0.1995329   0.20021343  0.20056011  0.20029341  0.19940014]
 [ 0.19950271  0.20026834  0.20057298  0.200279    0.19937696]
 [ 0.19956852  0.20020635  0.20054294  0.20029669  0.19938549]
 [ 0.19952431  0.20026608  0.2005699   0.20030303  0.19933675]
 [ 0.19954906  0.20023635  0.20059903  0.20036438  0.19925112]
 [ 0.19951153  0.20029865  0.2006034   0.2002514   0.19933496]
 [ 0.19960363  0.20018998  0.20059474  0.20035958  0.1992521 ]
 [ 0.19954935  0.20026727  0.20048186  0.20033088  0.19937068]
 [ 0.199586    0.20019588  0.20057657  0.2004078   0.19923374]
 [ 0.19958083  0.20023426  0.2005301   0.20039301  0.19926177]
 [ 0.199549    0.20027505  0.20059735  0.20026752  0.19931109]
 [ 0.19952765  0.20027091  0.20063749  0.20023575  0.19932824]
 [ 0.19962098  0.20026352  0.20054759  0.20036462  0.19920337]
 [ 0.19949977  0.20029514  0.20058048  0.20038973  0.19923496]
 [ 0.19955753  0.2002444   0.20056428  0.20031346  0.19932036]
 [ 0.19941328  0.20042656  0.20063962  0.20027649  0.19924396]
 [ 0.19958366  0.20023178  0.20060329  0.20027153  0.19930978]
 [ 0.19952624  0.20018204  0.20054165  0.20029071  0.19945937]
 [ 0.19946478  0.20027715  0.20061992  0.20027773  0.19936039]
 [ 0.19952844  0.20021887  0.20049027  0.20031387  0.19944857]
 [ 0.19958767  0.20022538  0.20042862  0.20033415  0.19942412]
 [ 0.19951746  0.20029813  0.20048892  0.20028146  0.199414  ]
 [ 0.19957586  0.20023401  0.20054437  0.20021144  0.19943431]
 [ 0.19955118  0.20030145  0.2005123   0.20037213  0.19926292]
 [ 0.19942316  0.20037928  0.20064239  0.20030811  0.19924709]
 [ 0.19950292  0.20034274  0.20054995  0.20024195  0.19936247]
 [ 0.19955514  0.20032522  0.2004081   0.20032628  0.19938521]
 [ 0.19953318  0.2002192   0.2005529   0.20034079  0.19935395]
 [ 0.19961496  0.20020005  0.2005659   0.20040347  0.19921564]
 [ 0.19955111  0.20033138  0.20045243  0.20037945  0.19928557]
 [ 0.19951549  0.20020075  0.20057744  0.2003614   0.19934498]
 [ 0.19946614  0.20040202  0.20062719  0.20024809  0.19925657]
 [ 0.19953251  0.20024194  0.20063345  0.2002854   0.1993067 ]
 [ 0.19950387  0.20027396  0.20059332  0.2002527   0.19937611]
 [ 0.19953074  0.20027454  0.20058301  0.2002379   0.19937377]
 [ 0.19951659  0.20025665  0.20059896  0.20030656  0.19932121]
 [ 0.19955091  0.20021623  0.2005631   0.20025429  0.19941549]
 [ 0.19945776  0.20021836  0.20054948  0.20036457  0.19940978]
 [ 0.19946894  0.20028085  0.20056695  0.20026621  0.19941701]
 [ 0.19951278  0.20029117  0.20059614  0.20028935  0.19931059]
 [ 0.19948995  0.20027958  0.20060334  0.20032524  0.19930193]
 [ 0.19951703  0.20023838  0.20060074  0.2002417   0.19940212]
 [ 0.19934046  0.20040308  0.20065109  0.20030484  0.19930051]
 [ 0.19947594  0.20022506  0.20054989  0.20030156  0.19944756]
 [ 0.19954337  0.20028189  0.20060962  0.20021839  0.19934668]
 [ 0.19952363  0.2002836   0.20046659  0.20031004  0.19941621]
 [ 0.19953315  0.20024191  0.20051134  0.20030232  0.19941129]
 [ 0.19951043  0.20037726  0.2005565   0.20031394  0.19924192]
 [ 0.19949691  0.20025991  0.20057224  0.20030163  0.19936931]
 [ 0.19947274  0.20023575  0.20059493  0.20035362  0.19934294]
 [ 0.19943146  0.20044099  0.20060068  0.20021223  0.19931465]
 [ 0.19951789  0.20028527  0.2006032   0.20039308  0.19920051]
 [ 0.19955549  0.2002935   0.20056124  0.2002404   0.19934933]
 [ 0.19947629  0.20026705  0.2006484   0.20024104  0.19936723]
 [ 0.19952329  0.20027058  0.20053515  0.20024596  0.19942504]
 [ 0.19950981  0.20022805  0.20064245  0.20027047  0.19934927]
 [ 0.19955654  0.20026582  0.20050761  0.20037714  0.19929282]
 [ 0.19952226  0.20036742  0.20064308  0.2001874   0.19927987]
 [ 0.19957393  0.20023255  0.20056358  0.20040002  0.19922991]
 [ 0.19942914  0.2004209   0.20057146  0.20031968  0.19925885]
 [ 0.19952446  0.20022881  0.2006046   0.20029001  0.19935206]
 [ 0.19950442  0.20028234  0.20059574  0.2002933   0.19932428]
 [ 0.19948098  0.2003521   0.2004002   0.20033555  0.19943114]
 [ 0.19961387  0.20017985  0.20056476  0.20038502  0.19925645]
 [ 0.19943087  0.20033199  0.20059092  0.20040902  0.19923721]
 [ 0.19950916  0.20026988  0.20054424  0.20033698  0.19933979]
 [ 0.19951423  0.20021632  0.20058492  0.20031524  0.1993693 ]
 [ 0.19948196  0.20037574  0.20059161  0.20031573  0.19923492]
 [ 0.19952479  0.20028201  0.20061933  0.20028722  0.19928668]
 [ 0.19948937  0.20021255  0.20045935  0.20034818  0.19949058]
 [ 0.19952454  0.20028348  0.20054705  0.20030396  0.19934095]
 [ 0.19949976  0.20033687  0.20057517  0.20025778  0.19933039]
 [ 0.19940077  0.20044929  0.20062865  0.20025526  0.19926608]
 [ 0.19938478  0.20038967  0.20063375  0.20033725  0.19925463]
 [ 0.1995437   0.20025112  0.20056531  0.20026115  0.19937874]
 [ 0.19941427  0.20042104  0.2005804   0.20030884  0.19927549]
 [ 0.19957438  0.20029674  0.20056523  0.20034042  0.19922324]
 [ 0.19955526  0.20024209  0.20055225  0.20027804  0.1993724 ]
 [ 0.1995541   0.20026721  0.20054194  0.20033906  0.19929764]
 [ 0.19955903  0.2003158   0.20051335  0.20041247  0.1991993 ]
 [ 0.19938613  0.20035607  0.20066585  0.20029809  0.19929384]
 [ 0.19951764  0.20025633  0.20059651  0.20032626  0.19930328]
 [ 0.19951358  0.20021576  0.20053591  0.20035905  0.19937573]
 [ 0.19953224  0.20031337  0.20057844  0.20021334  0.19936262]
 [ 0.19951706  0.20022441  0.20059213  0.20028003  0.19938633]
 [ 0.19952719  0.20024359  0.2005564   0.20032188  0.19935098]
 [ 0.19952637  0.20026828  0.20046155  0.20029622  0.1994476 ]
 [ 0.1996244   0.2002053   0.20051171  0.20041303  0.19924548]
 [ 0.19958861  0.20023431  0.20059371  0.2004053   0.19917811]
 [ 0.19940734  0.20040286  0.20063911  0.20029835  0.19925232]
 [ 0.19945385  0.20023513  0.20062706  0.20031619  0.19936775]
 [ 0.19945523  0.20030472  0.20061915  0.20024405  0.19937688]
 [ 0.19946712  0.20033589  0.20059752  0.20027772  0.19932181]
 [ 0.199497    0.2002306   0.20058784  0.20031454  0.19937003]
 [ 0.19951668  0.20025915  0.20062281  0.20027035  0.19933099]
 [ 0.19957697  0.20019925  0.20060362  0.20027019  0.19934991]
 [ 0.19947051  0.20023097  0.20060356  0.20038025  0.19931467]
 [ 0.19950502  0.20034373  0.20062612  0.20022838  0.19929679]
 [ 0.19940239  0.20038299  0.2006222   0.20033783  0.19925456]
 [ 0.19949746  0.20030335  0.20055997  0.2002468   0.19939239]
 [ 0.19956808  0.20025001  0.20049983  0.20026064  0.19942153]
 [ 0.19943823  0.20044951  0.20061819  0.20025121  0.19924285]
 [ 0.19953983  0.2002771   0.2005655   0.20037255  0.19924496]
 [ 0.19950593  0.20026577  0.20067798  0.20023605  0.19931428]
 [ 0.19957186  0.20021529  0.2006184   0.20024732  0.19934714]
 [ 0.1995327   0.20032813  0.2005849   0.20027128  0.19928296]
 [ 0.19946809  0.20022929  0.20057666  0.20036986  0.19935611]
 [ 0.19950986  0.20028235  0.20065525  0.20025057  0.19930193]
 [ 0.19947055  0.20023875  0.20064871  0.2002912   0.19935082]
 [ 0.19955447  0.20027354  0.20056491  0.20029125  0.19931583]
 [ 0.19946663  0.200378    0.20056883  0.200317    0.19926961]
 [ 0.19935817  0.20037961  0.20058073  0.20038192  0.19929954]
 [ 0.1994998   0.20028344  0.20061027  0.2002238   0.19938272]
 [ 0.19952689  0.20025337  0.20049204  0.20032363  0.19940408]
 [ 0.1995631   0.20025104  0.20042855  0.20032139  0.1994359 ]
 [ 0.19948661  0.20027637  0.20057997  0.20032638  0.19933072]
 [ 0.19955099  0.20023178  0.20053248  0.20029989  0.19938488]
 [ 0.1994306   0.20034169  0.20059127  0.2004085   0.19922793]
 [ 0.19958058  0.20020589  0.2005778   0.20036714  0.19926858]
 [ 0.19953232  0.20025972  0.20052378  0.20032439  0.19935986]
 [ 0.19949774  0.20024443  0.20058605  0.20036803  0.19930378]
 [ 0.19948101  0.20027047  0.2005735   0.20017622  0.19949879]
 [ 0.19955148  0.20023809  0.20059298  0.20029877  0.19931875]
 [ 0.19956337  0.20024881  0.20050757  0.20035036  0.19932985]
 [ 0.19946502  0.20024493  0.20055616  0.20033033  0.1994036 ]
 [ 0.19948496  0.20027573  0.20058341  0.20030105  0.19935487]
 [ 0.199476    0.20026454  0.20056793  0.20041679  0.19927479]
 [ 0.19955888  0.20023885  0.20042218  0.20033638  0.19944361]
 [ 0.19956738  0.20023684  0.20046806  0.20029491  0.19943281]
 [ 0.19940491  0.20020877  0.20061126  0.20035726  0.19941783]
 [ 0.199477    0.20028184  0.20063658  0.20024791  0.19935662]
 [ 0.1995208   0.20033319  0.20052516  0.20029545  0.19932541]
 [ 0.19944502  0.20029898  0.20057899  0.20031057  0.19936642]
 [ 0.199508    0.20021622  0.20055844  0.20034105  0.19937629]
 [ 0.19958767  0.200225    0.20060687  0.20034456  0.19923587]
 [ 0.19948217  0.20029345  0.20053418  0.20030588  0.1993843 ]
 [ 0.19950415  0.20016485  0.20057771  0.20035008  0.19940317]
 [ 0.19953154  0.20028338  0.20058692  0.20029765  0.19930056]
 [ 0.19957274  0.2002632   0.20054872  0.20038299  0.19923234]
 [ 0.19956924  0.2002441   0.20054646  0.20037428  0.199266  ]
 [ 0.19943741  0.20024623  0.20057136  0.2003525   0.19939245]
 [ 0.19949609  0.20035271  0.20060274  0.20029129  0.19925712]
 [ 0.19947062  0.20028234  0.20061107  0.20031136  0.19932458]
 [ 0.1994703   0.2003727   0.20041992  0.2003516   0.19938551]
 [ 0.19948527  0.2002645   0.20056249  0.20036101  0.19932674]
 [ 0.19948202  0.20026056  0.20066078  0.20034127  0.19925526]]
DEBUG:root:[ Iteration 0 ] Test loss: [[ 0.19946352  0.20028685  0.20062771  0.20022966  0.19939229]
 [ 0.19955148  0.20018224  0.20052759  0.20031169  0.19942705]
 [ 0.19955054  0.20018835  0.20059492  0.20036151  0.19930468]
 [ 0.19946863  0.20025027  0.20060229  0.20033216  0.19934662]
 [ 0.1995005   0.20022398  0.20059501  0.20026682  0.19941369]
 [ 0.19948177  0.20025122  0.20063952  0.20030704  0.19932039]
 [ 0.19952102  0.20028749  0.20060922  0.20037028  0.19921203]
 [ 0.19951804  0.20033078  0.20063193  0.20022495  0.19929427]
 [ 0.19953932  0.20023449  0.20055687  0.20028117  0.19938813]
 [ 0.1994791   0.20033945  0.20059922  0.20027132  0.19931085]
 [ 0.19950426  0.20023434  0.2006222   0.20029607  0.19934307]
 [ 0.19946788  0.20032108  0.20040672  0.20040129  0.19940306]
 [ 0.19955625  0.2002115   0.20057438  0.20023502  0.19942282]
 [ 0.19944331  0.20026521  0.20057587  0.20033398  0.19938163]
 [ 0.19958651  0.20023875  0.20046622  0.20026048  0.19944809]
 [ 0.19958934  0.20023592  0.20040119  0.20034105  0.19943255]
 [ 0.19951661  0.20028083  0.20055158  0.20039099  0.19926001]
 [ 0.19946696  0.20032905  0.20062627  0.20038474  0.19919303]
 [ 0.1994679   0.20030622  0.20041391  0.20041189  0.19940011]
 [ 0.19952838  0.20026904  0.20061529  0.20034444  0.19924283]
 [ 0.19949235  0.20037812  0.20055938  0.20036374  0.19920646]
 [ 0.19947994  0.20044583  0.20058495  0.20028454  0.19920476]
 [ 0.19938818  0.20038886  0.20062633  0.20035456  0.19924201]
 [ 0.19945802  0.20029065  0.20054086  0.20034774  0.19936267]
 [ 0.19942591  0.20036933  0.20064017  0.20026718  0.19929746]
 [ 0.19947949  0.20031597  0.20055677  0.20032978  0.19931798]
 [ 0.19948849  0.20032324  0.20038322  0.20032428  0.19948071]
 [ 0.19947481  0.20030577  0.20063242  0.20027997  0.19930701]
 [ 0.19945656  0.20029691  0.20061912  0.20024648  0.19938095]
 [ 0.19943309  0.20044014  0.20059061  0.20023547  0.19930072]
 [ 0.19946744  0.20032768  0.20060676  0.20031278  0.1992853 ]
 [ 0.19954263  0.20034294  0.20053618  0.20032312  0.19925509]
 [ 0.19954728  0.20026855  0.20049047  0.20027147  0.19942226]
 [ 0.1996149   0.2002648   0.20054294  0.20037194  0.19920546]
 [ 0.1995877   0.2002454   0.20053779  0.20017484  0.19945423]
 [ 0.19963937  0.20019235  0.20057894  0.20035414  0.19923523]
 [ 0.19954325  0.20024744  0.20059593  0.20024811  0.19936533]
 [ 0.1994807   0.20027894  0.20064475  0.20029691  0.19929866]
 [ 0.19951937  0.2002926   0.2005717   0.2002411   0.19937521]
 [ 0.19952561  0.20028009  0.2006211   0.20033304  0.19924009]
 [ 0.19954747  0.20030405  0.20036489  0.20032692  0.19945669]
 [ 0.19954661  0.20025437  0.20052887  0.2002458   0.19942434]
 [ 0.19964248  0.20020401  0.20055681  0.20034081  0.19925597]
 [ 0.19936669  0.20042926  0.20059112  0.20035543  0.19925748]
 [ 0.19948582  0.20031488  0.20059371  0.20031931  0.19928625]
 [ 0.19951421  0.20029816  0.20057933  0.20026533  0.19934291]
 [ 0.19959405  0.20021936  0.2003745   0.20032319  0.19948889]
 [ 0.19949475  0.20028917  0.20054625  0.20030938  0.19936049]
 [ 0.19945014  0.20021014  0.20057154  0.20032483  0.19944331]
 [ 0.19955187  0.20029968  0.20054477  0.20037812  0.19922563]
 [ 0.19954319  0.20027427  0.20061646  0.20020825  0.19935788]
 [ 0.19951034  0.20029527  0.20042801  0.20036183  0.19940454]
 [ 0.1995912   0.20030026  0.20055956  0.20032439  0.19922461]
 [ 0.19958402  0.20023681  0.20056564  0.20028237  0.19933116]
 [ 0.19950643  0.2003205   0.20036454  0.2004039   0.19940466]
 [ 0.19946869  0.20024809  0.20064624  0.20027962  0.19935733]
 [ 0.19940893  0.20040125  0.20060593  0.20031236  0.19927153]
 [ 0.19950159  0.20047207  0.20056669  0.20022158  0.19923802]
 [ 0.1995292   0.20027882  0.20062813  0.20028342  0.19928044]
 [ 0.1994634   0.2002366   0.20061699  0.20028955  0.19939344]
 [ 0.19951306  0.20027271  0.20049918  0.20039767  0.1993174 ]
 [ 0.19954292  0.20031737  0.20046964  0.20029791  0.19937217]
 [ 0.19955774  0.20030636  0.20053564  0.20035486  0.19924535]
 [ 0.19961604  0.20024554  0.20042647  0.20034499  0.19936694]
 [ 0.19946401  0.20027731  0.20061707  0.20028679  0.19935486]
 [ 0.19948554  0.20030564  0.20059346  0.20031022  0.19930513]
 [ 0.19956262  0.20029482  0.20049688  0.20027091  0.19937472]
 [ 0.19955899  0.20028202  0.20054166  0.20032158  0.19929579]
 [ 0.19953069  0.20026927  0.20053978  0.20027114  0.19938916]
 [ 0.19945896  0.20034094  0.20064901  0.20032907  0.199222  ]
 [ 0.19949944  0.20028493  0.20059122  0.20023017  0.19939426]
 [ 0.19950904  0.20022681  0.20063208  0.20028961  0.19934244]
 [ 0.19956185  0.20021051  0.20055166  0.2003648   0.19931118]
 [ 0.19949742  0.20032759  0.20067705  0.20026897  0.19922893]
 [ 0.19949564  0.20029308  0.20061681  0.20027228  0.19932221]
 [ 0.19954641  0.20031723  0.20047347  0.2003637   0.1992992 ]
 [ 0.19957879  0.20024969  0.20047939  0.20025146  0.19944067]
 [ 0.19950013  0.20027412  0.20058037  0.20028977  0.19935554]
 [ 0.1995354   0.20025654  0.20057233  0.20038906  0.19924664]
 [ 0.19959892  0.20024271  0.20057847  0.2002729   0.19930695]
 [ 0.19967236  0.20024949  0.20028611  0.20030025  0.19949177]
 [ 0.19951817  0.20026527  0.20053945  0.20029587  0.19938122]
 [ 0.19955769  0.20029826  0.20057237  0.20035043  0.19922118]
 [ 0.19945143  0.20037489  0.20059894  0.20037402  0.19920078]
 [ 0.19960654  0.2002691   0.20041037  0.20027456  0.19943942]
 [ 0.19959816  0.20028105  0.20056042  0.20035307  0.19920725]
 [ 0.19954394  0.20025277  0.20057105  0.20034246  0.19928975]
 [ 0.19949213  0.20023352  0.20064366  0.20027578  0.19935487]
 [ 0.19939265  0.20046677  0.2006187   0.20032516  0.19919673]
 [ 0.19948311  0.20033275  0.20067599  0.20026092  0.19924715]
 [ 0.1995455   0.20024484  0.20050599  0.20024489  0.19945876]
 [ 0.19961134  0.2001892   0.20058885  0.2003666   0.19924393]
 [ 0.19946188  0.20030746  0.20055968  0.20032459  0.19934642]
 [ 0.19954422  0.20025696  0.20056559  0.20030737  0.19932586]
 [ 0.19948202  0.20028618  0.20057362  0.20038718  0.19927099]
 [ 0.19949913  0.20027593  0.20055027  0.20031038  0.19936429]
 [ 0.19957985  0.20019455  0.20045711  0.20036334  0.19940512]
 [ 0.1996125   0.20029506  0.20041768  0.20023103  0.19944368]
 [ 0.19951992  0.20023894  0.20058218  0.20034094  0.19931807]
 [ 0.19951956  0.20029329  0.20062441  0.20022736  0.1993354 ]
 [ 0.19953698  0.20030996  0.20057906  0.2002957   0.1992783 ]
 [ 0.19955659  0.20028488  0.20060468  0.20021051  0.1993434 ]
 [ 0.19959804  0.20023932  0.20058174  0.20033717  0.19924368]
 [ 0.19954064  0.20028728  0.20056805  0.20037985  0.19922419]
 [ 0.19951315  0.20025219  0.2005177   0.20032386  0.19939311]
 [ 0.19949338  0.20022513  0.2005574   0.20040211  0.19932202]
 [ 0.19949941  0.20026933  0.20060046  0.2002968   0.19933398]
 [ 0.19949555  0.20027944  0.20053837  0.20028666  0.19939996]
 [ 0.19950347  0.20028493  0.20059742  0.20029008  0.19932413]
 [ 0.19949743  0.20042798  0.20060393  0.20029142  0.1991792 ]
 [ 0.19947155  0.20043723  0.2006873   0.20024124  0.19916268]
 [ 0.1994614   0.20029117  0.20054096  0.20036782  0.19933867]
 [ 0.19957216  0.20025526  0.20060086  0.20022124  0.19935048]
 [ 0.19950697  0.20029658  0.20042998  0.20029818  0.1994683 ]
 [ 0.19953534  0.20030783  0.20056362  0.20026451  0.19932869]
 [ 0.19947731  0.20026958  0.20059247  0.20027438  0.19938619]
 [ 0.19955343  0.20027116  0.20053798  0.20036867  0.19926876]
 [ 0.19963597  0.20019402  0.20060702  0.20034081  0.19922218]
 [ 0.19965021  0.20022121  0.20054324  0.20037968  0.1992057 ]
 [ 0.19948244  0.20023915  0.20056777  0.20035121  0.1993594 ]
 [ 0.19960962  0.20025508  0.20053303  0.2003327   0.1992695 ]
 [ 0.199526    0.20029514  0.20052667  0.20030732  0.19934483]
 [ 0.19956091  0.20028657  0.2005412   0.20041123  0.19920009]
 [ 0.19944757  0.20029515  0.20057578  0.20034035  0.19934113]
 [ 0.19950813  0.20026067  0.20057371  0.20029905  0.19935836]
 [ 0.19952302  0.20028116  0.20064443  0.20025769  0.19929372]
 [ 0.1995281   0.20023796  0.2005741   0.20030889  0.19935091]
 [ 0.19943745  0.20035107  0.20063418  0.2003091   0.19926819]
 [ 0.19950975  0.20021215  0.20050289  0.20038022  0.19939496]
 [ 0.19952121  0.20022479  0.20059758  0.20029417  0.19936223]
 [ 0.19950435  0.20026059  0.20056982  0.20030314  0.19936208]
 [ 0.19948658  0.20028673  0.20055409  0.20024295  0.19942969]
 [ 0.19950081  0.20027511  0.20060709  0.20029014  0.19932684]
 [ 0.19949804  0.20032118  0.20055644  0.20022237  0.199402  ]
 [ 0.19950272  0.20029368  0.20057613  0.20030662  0.19932085]
 [ 0.19945307  0.20037964  0.20065913  0.20026141  0.1992467 ]
 [ 0.19947955  0.20028716  0.20042253  0.20034869  0.19946206]
 [ 0.19950211  0.20025219  0.20052299  0.20026658  0.19945616]
 [ 0.19960068  0.20022503  0.20057178  0.20035763  0.19924487]
 [ 0.19950429  0.20021656  0.20060176  0.20030119  0.1993762 ]
 [ 0.19949163  0.20025778  0.20060201  0.20027581  0.1993728 ]
 [ 0.19955792  0.2002684   0.20055577  0.20023018  0.1993877 ]
 [ 0.19950354  0.20026793  0.20065057  0.20025133  0.19932669]
 [ 0.19962415  0.20020433  0.20057073  0.20032735  0.19927339]
 [ 0.19956918  0.20026794  0.20054889  0.20040348  0.19921052]
 [ 0.19946189  0.20026048  0.20055871  0.20036024  0.19935872]
 [ 0.19949235  0.20033081  0.20041263  0.20034711  0.19941713]
 [ 0.19955936  0.20035003  0.20045824  0.20025162  0.19938076]
 [ 0.19950336  0.20042998  0.2005498   0.20026222  0.19925468]
 [ 0.19955294  0.20032763  0.20063148  0.20019734  0.19929065]
 [ 0.1995261   0.20027959  0.20050696  0.20025262  0.1994347 ]
 [ 0.19946627  0.20028691  0.20064726  0.20028788  0.1993117 ]
 [ 0.19962548  0.20022178  0.20041607  0.20031115  0.19942556]
 [ 0.19955872  0.20025441  0.20059088  0.20023744  0.19935858]
 [ 0.19949107  0.20029263  0.20060289  0.20029293  0.19932045]
 [ 0.19948192  0.20026125  0.20060721  0.20025659  0.19939302]
 [ 0.19943887  0.20029074  0.20063874  0.20025884  0.19937284]
 [ 0.19958799  0.20028131  0.20057932  0.20030457  0.19924682]
 [ 0.19960138  0.20024738  0.20052525  0.20039666  0.19922929]
 [ 0.19951195  0.20034456  0.20058608  0.20020173  0.19935574]
 [ 0.19949158  0.2002888   0.2005313   0.20032996  0.19935837]
 [ 0.19947089  0.20037647  0.20060472  0.2003033   0.19924459]
 [ 0.19944656  0.2002162   0.20060803  0.20032296  0.19940631]
 [ 0.19944772  0.2003493   0.20061614  0.20032351  0.19926329]
 [ 0.19947647  0.20035318  0.20048165  0.20029236  0.19939631]
 [ 0.19953403  0.20023619  0.20050757  0.20030379  0.1994184 ]
 [ 0.19933023  0.20042071  0.20063941  0.20034447  0.1992652 ]
 [ 0.19948284  0.2003652   0.20060413  0.20030224  0.1992456 ]
 [ 0.19953173  0.20030342  0.20055202  0.20029904  0.19931379]
 [ 0.19949915  0.200322    0.2005794   0.20027053  0.19932894]
 [ 0.19950803  0.20020093  0.20060116  0.20036857  0.19932124]
 [ 0.19956355  0.20022896  0.2005472   0.20026575  0.19939448]
 [ 0.19950281  0.20021388  0.20064071  0.20030726  0.19933532]
 [ 0.19948921  0.20030232  0.20063891  0.20024805  0.19932154]
 [ 0.19946191  0.20028685  0.20061386  0.20037727  0.19926009]
 [ 0.1994939   0.20024997  0.20058684  0.20032494  0.19934435]
 [ 0.19945244  0.20039557  0.20062953  0.20030147  0.19922093]
 [ 0.19953044  0.20029444  0.20042992  0.20034361  0.19940162]
 [ 0.19938716  0.20038731  0.20061156  0.20033732  0.19927667]
 [ 0.19945902  0.20025882  0.20060167  0.20034084  0.19933966]
 [ 0.19945145  0.20025547  0.20060849  0.20032872  0.19935586]
 [ 0.19948743  0.20028408  0.20060125  0.20028843  0.19933885]
 [ 0.19954674  0.20025247  0.20046797  0.20036429  0.19936852]
 [ 0.19940533  0.20045266  0.20061153  0.20027161  0.19925882]
 [ 0.19954923  0.20026051  0.20061187  0.20036179  0.19921665]
 [ 0.19947173  0.20033687  0.20056029  0.20036887  0.19926225]
 [ 0.19957215  0.20024642  0.20049427  0.20031136  0.19937572]
 [ 0.19951779  0.2003403   0.20053698  0.20027494  0.1993299 ]
 [ 0.19950907  0.20028085  0.20053859  0.20033775  0.19933376]
 [ 0.19947506  0.20032123  0.2006436   0.20028667  0.19927344]
 [ 0.19945858  0.20021401  0.20054929  0.20037323  0.19940481]
 [ 0.19947357  0.20029429  0.2006093   0.20027898  0.19934388]
 [ 0.1995274   0.20020175  0.20046671  0.20033565  0.19946855]
 [ 0.19962671  0.20018266  0.20057297  0.20037083  0.19924682]
 [ 0.19963841  0.20028427  0.20039566  0.20024291  0.19943871]
 [ 0.19949357  0.20027271  0.20058468  0.20030786  0.19934118]
 [ 0.19943191  0.20043831  0.20058177  0.20028794  0.19926007]
 [ 0.19944544  0.20028019  0.20062838  0.20027888  0.19936709]
 [ 0.19953145  0.20025849  0.20043552  0.20034933  0.19942522]
 [ 0.19954993  0.20026153  0.2005647   0.20029406  0.1993297 ]]
DEBUG:root:[ Iteration 3 ] Training loss: [[ 0.19957104  0.19919638  0.20243806  0.19711959  0.20167492]
 [ 0.19955912  0.19888189  0.20318165  0.19573614  0.20264128]
 [ 0.19969019  0.19935358  0.20205735  0.19778615  0.20111276]
 [ 0.19957089  0.19913401  0.20277573  0.19651376  0.20200564]
 [ 0.19974248  0.19916166  0.20242344  0.19711424  0.20155814]
 [ 0.19941239  0.19873835  0.20311964  0.19593643  0.20279321]
 [ 0.19952197  0.19860624  0.20348932  0.19536382  0.20301859]
 [ 0.19946119  0.19909528  0.20287661  0.1963561   0.20221077]
 [ 0.19959483  0.19867046  0.20311582  0.19566399  0.20295492]
 [ 0.19960395  0.19897798  0.20273033  0.1964753   0.20221247]
 [ 0.19961402  0.19913407  0.20271395  0.19655176  0.20198618]
 [ 0.19961466  0.198782    0.20338483  0.19551805  0.20270051]
 [ 0.19955572  0.19923718  0.20247968  0.19709456  0.20163283]
 [ 0.19896533  0.19849528  0.20283553  0.19562417  0.2040797 ]
 [ 0.19967188  0.19940244  0.20202351  0.19787005  0.20103213]
 [ 0.19902851  0.19858003  0.20275223  0.19592687  0.20371234]
 [ 0.1996735   0.19923028  0.20257647  0.19691019  0.20160952]
 [ 0.19956683  0.19894972  0.20277399  0.19640091  0.20230852]
 [ 0.19959876  0.19888252  0.2031941   0.19577651  0.20254807]
 [ 0.19956529  0.19902599  0.20297968  0.19605507  0.20237398]
 [ 0.19965506  0.1993715   0.20215292  0.19766624  0.20115425]
 [ 0.1995822   0.19889763  0.20293096  0.19598639  0.20260279]
 [ 0.19947791  0.1985855   0.20335847  0.19533351  0.20324463]
 [ 0.19930828  0.19864602  0.20336869  0.19536945  0.20330761]
 [ 0.19964102  0.19927758  0.20223881  0.19741583  0.20142682]
 [ 0.19947954  0.19922203  0.20233878  0.19710982  0.20184989]
 [ 0.19959216  0.19865407  0.20317149  0.19557969  0.20300259]
 [ 0.19921458  0.19867991  0.20332627  0.19540034  0.20337895]
 [ 0.19903053  0.19862574  0.20249304  0.19623613  0.20361462]
 [ 0.19967525  0.19918668  0.20247802  0.19710171  0.20155826]
 [ 0.19963565  0.19916864  0.20249014  0.1971309   0.20157465]
 [ 0.19970205  0.19929971  0.20221955  0.19749309  0.20128565]
 [ 0.19954567  0.19899659  0.20301527  0.19601995  0.20242251]
 [ 0.19954786  0.19915581  0.20259367  0.19689189  0.20181079]
 [ 0.19906214  0.19867569  0.20252976  0.1962138   0.20351863]
 [ 0.1989654   0.19847435  0.20267904  0.19572698  0.20415421]
 [ 0.19914128  0.19864179  0.20328827  0.19537184  0.20355687]
 [ 0.19949822  0.19868238  0.20318903  0.19575404  0.20287631]
 [ 0.19961615  0.19940163  0.20210886  0.19774103  0.20113234]
 [ 0.19958736  0.19916652  0.20256568  0.19681239  0.20186806]
 [ 0.19968334  0.19914779  0.20247819  0.19713345  0.20155716]
 [ 0.19893606  0.19862126  0.20267864  0.19588871  0.20387535]
 [ 0.19950724  0.19943337  0.20198527  0.19788158  0.2011926 ]
 [ 0.19953346  0.19885322  0.202802    0.19614834  0.20266291]
 [ 0.19960636  0.19891916  0.20284092  0.19621256  0.20242101]
 [ 0.19956574  0.19878209  0.2030713   0.19576459  0.20281629]
 [ 0.19941421  0.1993034   0.20264617  0.19683833  0.20179789]
 [ 0.19940875  0.19868591  0.20308475  0.19561392  0.20320657]
 [ 0.19951116  0.1989654   0.20301922  0.19606309  0.20244111]
 [ 0.19900374  0.198469    0.20302238  0.19549608  0.20400883]
 [ 0.19950703  0.19932504  0.20227119  0.19743642  0.20146026]
 [ 0.19961384  0.19938016  0.20213436  0.19771253  0.2011591 ]
 [ 0.1990065   0.19852425  0.20295742  0.1955944   0.20391741]
 [ 0.19923159  0.1985817   0.20333269  0.1954022   0.20345174]
 [ 0.19957753  0.19899055  0.20294765  0.19608194  0.20240229]
 [ 0.19954534  0.19915342  0.20261706  0.1968067   0.20187744]
 [ 0.19953933  0.19914171  0.20268208  0.19649589  0.202141  ]
 [ 0.1996149   0.19879492  0.20321858  0.19569248  0.20267911]
 [ 0.19959049  0.19899604  0.20297183  0.19605827  0.20238337]
 [ 0.19951169  0.19873758  0.2031454   0.19576848  0.20283687]
 [ 0.19900665  0.19868189  0.202466    0.19621693  0.20362854]
 [ 0.19944941  0.19913311  0.20269175  0.19648622  0.20223956]
 [ 0.19948803  0.19924276  0.20245017  0.19710058  0.20171839]
 [ 0.19964737  0.19924816  0.20226774  0.19736005  0.20147668]
 [ 0.1996167   0.19884771  0.20326605  0.19562384  0.20264563]
 [ 0.19947544  0.19919175  0.20275339  0.19650391  0.20207544]
 [ 0.19964303  0.19868964  0.20311426  0.19556691  0.20298623]
 [ 0.19960354  0.19894235  0.20324659  0.19575042  0.2024571 ]
 [ 0.19972514  0.19914553  0.2024871   0.1970506   0.20159163]
 [ 0.19925374  0.19865412  0.20334771  0.19542663  0.20331784]
 [ 0.19944715  0.19896254  0.20288317  0.19619548  0.20251167]
 [ 0.19963914  0.19919901  0.20259561  0.19671966  0.20184661]
 [ 0.19956793  0.19901207  0.20311281  0.19593033  0.20237678]
 [ 0.1995995   0.19942734  0.20207819  0.19784972  0.20104524]
 [ 0.19958469  0.19881369  0.20326722  0.19560267  0.20273171]
 [ 0.1995573   0.19908911  0.20282154  0.19630922  0.20222281]
 [ 0.1995929   0.19929586  0.20226544  0.19747029  0.20137545]
 [ 0.19904679  0.19863559  0.2024551   0.19622201  0.20364054]
 [ 0.1996564   0.19919416  0.2024968   0.19704185  0.20161074]
 [ 0.19964169  0.19906643  0.20259534  0.19674665  0.20194992]
 [ 0.19963193  0.19915596  0.20256154  0.19698165  0.20166893]
 [ 0.19963035  0.19882087  0.20337483  0.19554096  0.20263305]
 [ 0.19949317  0.1986285   0.20322767  0.19565362  0.2029971 ]
 [ 0.19963032  0.198599    0.20331758  0.19540587  0.20304722]
 [ 0.19906092  0.19885361  0.20227103  0.19664802  0.2031664 ]
 [ 0.19956093  0.19894892  0.20309751  0.19592059  0.20247212]
 [ 0.19961163  0.1993867   0.20209526  0.19777231  0.2011341 ]
 [ 0.1989513   0.19852813  0.20276727  0.19570576  0.20404758]
 [ 0.19892168  0.19853753  0.20270777  0.19584009  0.20399295]
 [ 0.19937687  0.19885102  0.2029091   0.19618337  0.20267957]
 [ 0.19889975  0.19852301  0.20265608  0.19587655  0.20404458]
 [ 0.19965447  0.19910751  0.20247626  0.19686288  0.20189886]
 [ 0.19930272  0.19865786  0.20338601  0.19528218  0.20337121]
 [ 0.19956337  0.19911483  0.20271     0.1964587   0.20215315]
 [ 0.19961597  0.19895108  0.20316064  0.19577286  0.20249946]
 [ 0.1995277   0.1991844   0.20239364  0.19705813  0.20183612]
 [ 0.19933772  0.19933785  0.20264614  0.19678153  0.20189674]
 [ 0.19958901  0.19872703  0.20321444  0.19562903  0.20284048]
 [ 0.19949725  0.19906333  0.20270266  0.19633704  0.20239979]
 [ 0.19954734  0.19902201  0.20314668  0.19588812  0.20239587]
 [ 0.19961916  0.19924225  0.20251407  0.19706105  0.20156349]
 [ 0.1994983   0.19886659  0.20298448  0.19605598  0.20259462]
 [ 0.19961937  0.1992071   0.20258963  0.19686507  0.20171876]
 [ 0.19905494  0.19868439  0.20249531  0.19629958  0.20346577]
 [ 0.19960228  0.1994592   0.20201966  0.19791248  0.20100638]
 [ 0.19904149  0.19863276  0.20251378  0.19620228  0.20360976]
 [ 0.1995246   0.19894643  0.20311877  0.19583955  0.20257066]
 [ 0.19890927  0.19860655  0.20260596  0.19597626  0.20390199]
 [ 0.19958724  0.19945137  0.20212033  0.1978837   0.20095737]
 [ 0.19900857  0.19863822  0.20255764  0.19615115  0.20364447]
 [ 0.19958256  0.19851489  0.20331146  0.19530298  0.20328806]
 [ 0.19932988  0.19928302  0.20270991  0.19666874  0.20200847]
 [ 0.19946218  0.19920494  0.20235834  0.19709648  0.20187807]
 [ 0.19930696  0.19864979  0.20336688  0.19535288  0.20332348]
 [ 0.19948629  0.19877002  0.20306784  0.19589384  0.20278208]
 [ 0.19948891  0.19870317  0.20311061  0.19587032  0.20282699]
 [ 0.19905713  0.19844475  0.20294707  0.19553506  0.20401596]
 [ 0.19967219  0.19887128  0.20289463  0.19598672  0.20257515]
 [ 0.19949231  0.19902216  0.20287278  0.19609518  0.20251755]
 [ 0.19953507  0.19916321  0.20266084  0.19672765  0.20191328]
 [ 0.19957064  0.19907002  0.20269835  0.19662499  0.20203601]
 [ 0.1989436   0.1985089   0.20280002  0.19570521  0.20404227]
 [ 0.19956443  0.19940813  0.20204395  0.19787058  0.20111291]
 [ 0.19922115  0.19859733  0.20318915  0.19543381  0.20355853]
 [ 0.19927648  0.19861098  0.20337959  0.19538309  0.20334987]
 [ 0.19889981  0.19856325  0.20259964  0.19601402  0.20392327]
 [ 0.19898143  0.19876474  0.20232938  0.19640435  0.20352001]
 [ 0.19938773  0.19862363  0.20330144  0.19539499  0.20329228]
 [ 0.19949192  0.19920051  0.2027337   0.19655176  0.20202211]
 [ 0.19905367  0.19847403  0.20281534  0.19561304  0.20404392]
 [ 0.1994561   0.19896422  0.2026923   0.19644183  0.20244555]
 [ 0.19901714  0.1986421   0.20253986  0.19615598  0.20364495]
 [ 0.19901715  0.19877174  0.2023426   0.19631578  0.20355272]
 [ 0.19959107  0.19906376  0.20279984  0.19646284  0.20208248]
 [ 0.19954148  0.19859882  0.20321713  0.19549461  0.20314795]
 [ 0.19902425  0.19849418  0.20283766  0.19563244  0.20401147]
 [ 0.19962662  0.19942935  0.20205815  0.19786499  0.20102087]
 [ 0.19954012  0.19875444  0.2034056   0.1954271   0.20287272]
 [ 0.1995834   0.19928591  0.20230034  0.19742635  0.20140398]
 [ 0.19951609  0.199296    0.2023519   0.19719759  0.20163842]
 [ 0.19953442  0.1990968   0.20278735  0.19635068  0.20223078]
 [ 0.19921714  0.19857678  0.20319885  0.19531892  0.20368826]
 [ 0.19961593  0.19895133  0.20311452  0.19587272  0.20244549]
 [ 0.19951732  0.19939378  0.20210919  0.19762322  0.20135656]
 [ 0.19953646  0.19918251  0.20265588  0.19670342  0.2019217 ]
 [ 0.19963646  0.19916193  0.20245229  0.19715782  0.20159151]
 [ 0.19971454  0.19915937  0.20257501  0.19696864  0.20158239]
 [ 0.19897856  0.19858311  0.20261839  0.19600619  0.20381373]
 [ 0.19892569  0.19857717  0.20265311  0.1959253   0.20391871]
 [ 0.19952887  0.19890764  0.20296356  0.19596708  0.20263283]
 [ 0.19893211  0.19848526  0.20276009  0.19573992  0.20408261]
 [ 0.19898926  0.19863696  0.20257406  0.1961773   0.20362239]
 [ 0.19960052  0.19886899  0.20348334  0.19546323  0.20258394]
 [ 0.19961858  0.19855386  0.20340098  0.19529088  0.20313564]
 [ 0.1995528   0.19861534  0.20322235  0.19558436  0.20302516]
 [ 0.19898906  0.19845514  0.20278567  0.19570285  0.20406729]
 [ 0.19957358  0.19904168  0.20282277  0.19646555  0.20209639]
 [ 0.19904412  0.19871077  0.2023921   0.19638625  0.20346674]
 [ 0.1996491   0.19937982  0.20215401  0.19771773  0.20109932]
 [ 0.19890794  0.19854037  0.202631    0.19579589  0.20412475]
 [ 0.19917448  0.19874544  0.20247953  0.19637063  0.20322981]
 [ 0.19960085  0.19870429  0.20303576  0.19590528  0.20275378]
 [ 0.19967696  0.19931559  0.20214997  0.19761199  0.20124553]
 [ 0.19968033  0.19918133  0.20245403  0.19712168  0.20156261]
 [ 0.19901973  0.19854563  0.20256281  0.19604649  0.20382532]
 [ 0.19944376  0.19924751  0.20264348  0.19677635  0.20188896]
 [ 0.19960871  0.19931144  0.202198    0.19760579  0.20127609]
 [ 0.19963926  0.19889319  0.20317832  0.19581549  0.20247373]
 [ 0.19899513  0.19847433  0.20303401  0.19546515  0.20403139]
 [ 0.19930297  0.19864076  0.20338477  0.19533832  0.20333318]
 [ 0.19934922  0.19889474  0.20277821  0.19628629  0.20269151]
 [ 0.19959983  0.19911635  0.20254925  0.19679919  0.20193532]
 [ 0.19945531  0.198599    0.20330679  0.19537157  0.2032674 ]
 [ 0.19956873  0.19887938  0.20305254  0.19590469  0.20259464]
 [ 0.19954062  0.19871223  0.20346043  0.19531927  0.20296742]
 [ 0.19962688  0.19921426  0.20265645  0.19679739  0.20170502]
 [ 0.19893549  0.19847098  0.20274083  0.19580182  0.20405085]
 [ 0.19898608  0.19857842  0.202516    0.19607806  0.20384143]
 [ 0.19960618  0.19872858  0.2031405   0.19580223  0.20272256]
 [ 0.19948155  0.19860524  0.20312074  0.19574869  0.20304379]
 [ 0.19961654  0.19906469  0.20266579  0.19653428  0.20211868]
 [ 0.1993161   0.19857864  0.20312218  0.19563079  0.20335229]
 [ 0.1990467   0.19895694  0.20249712  0.19674447  0.20275483]
 [ 0.1992963   0.19866131  0.20336132  0.19539049  0.20329066]
 [ 0.1996828   0.19928376  0.20229198  0.19737816  0.20136325]
 [ 0.19966793  0.19912735  0.20256881  0.19680133  0.20183459]
 [ 0.1994331   0.19912259  0.20260678  0.19673879  0.20209865]
 [ 0.1989253   0.19846573  0.2028235   0.19571754  0.20406787]
 [ 0.19969936  0.19917734  0.20246845  0.19698559  0.20166923]
 [ 0.19949701  0.19913326  0.20271713  0.19639778  0.20225483]
 [ 0.1996052   0.19938967  0.20207746  0.19786882  0.20105885]
 [ 0.19967721  0.19918896  0.20243841  0.19718726  0.20150813]
 [ 0.19954769  0.19916771  0.2026104   0.19675496  0.2019192 ]
 [ 0.19892174  0.19853324  0.2026294   0.19595438  0.20396121]
 [ 0.19937241  0.19936453  0.2026208   0.19687073  0.20177157]
 [ 0.19964916  0.1987526   0.20327741  0.19552661  0.20279424]
 [ 0.19899043  0.19846927  0.20275405  0.19568233  0.20410393]
 [ 0.19900285  0.19853956  0.20307931  0.19551781  0.20386048]
 [ 0.19958343  0.19937998  0.20209152  0.19769861  0.20124649]
 [ 0.19902825  0.19875002  0.20250233  0.19634621  0.20337313]]
DEBUG:root:[ Iteration 6 ] Training loss: [[ 0.19915293  0.1958238   0.20664842  0.1898666   0.20850821]
 [ 0.19917902  0.19595233  0.2065407   0.1901897   0.2081383 ]
 [ 0.19863251  0.19531357  0.20738403  0.18850401  0.21016584]
 [ 0.1993091   0.19735287  0.20476189  0.19313766  0.20543851]
 [ 0.19830018  0.19397147  0.20902923  0.18599471  0.21270446]
 [ 0.19580758  0.19222389  0.20554799  0.1854299   0.22099058]
 [ 0.19905698  0.19690962  0.20501474  0.19220981  0.20680888]
 [ 0.19915943  0.1969239   0.20506655  0.19243133  0.20641884]
 [ 0.1958874   0.19201916  0.20656341  0.18460943  0.22092056]
 [ 0.19938815  0.19660516  0.20542756  0.19193362  0.20664552]
 [ 0.1990017   0.19528666  0.20752074  0.18844078  0.20975016]
 [ 0.19862656  0.19465677  0.20830633  0.18713281  0.21127756]
 [ 0.19639164  0.19222343  0.20842333  0.18402876  0.21893275]
 [ 0.19836901  0.19391833  0.20877211  0.18605484  0.21288572]
 [ 0.19620271  0.19304894  0.20520736  0.1865274   0.21901356]
 [ 0.19933121  0.19715987  0.20488493  0.19292277  0.20570125]
 [ 0.19923127  0.19727756  0.20453262  0.19315138  0.20580713]
 [ 0.1956421   0.19198538  0.20602098  0.18482913  0.22152241]
 [ 0.19914015  0.1960884   0.20636049  0.19026032  0.2081507 ]
 [ 0.19913489  0.1960201   0.20649032  0.19011696  0.20823777]
 [ 0.19928165  0.19717982  0.20474841  0.19315973  0.20563039]
 [ 0.19923469  0.19607735  0.20615563  0.19059002  0.20794231]
 [ 0.19817461  0.19409795  0.20853873  0.18627632  0.21291238]
 [ 0.19618565  0.1929839   0.2054776   0.186435    0.2189178 ]
 [ 0.19560021  0.19204243  0.20565525  0.18502428  0.22167787]
 [ 0.19814998  0.19460434  0.20805115  0.18705994  0.21213464]
 [ 0.19805053  0.19466095  0.20833458  0.18724586  0.21170811]
 [ 0.1993168   0.19727215  0.20454766  0.19316198  0.20570134]
 [ 0.19827296  0.19433965  0.207734    0.18710534  0.212548  ]
 [ 0.19552819  0.19218254  0.20548259  0.18530354  0.22150315]
 [ 0.19719982  0.19257934  0.20899519  0.18435921  0.21686637]
 [ 0.19833539  0.19410904  0.20863453  0.18630236  0.21261863]
 [ 0.19931471  0.19662064  0.20548566  0.19161753  0.20696139]
 [ 0.19813891  0.19372411  0.20894051  0.18567313  0.21352343]
 [ 0.19927877  0.19729617  0.20464756  0.19326891  0.20550853]
 [ 0.19915827  0.19688639  0.20485768  0.1924734   0.20662427]
 [ 0.19736068  0.1925344   0.20995031  0.18386471  0.21628992]
 [ 0.19939788  0.19725764  0.20478176  0.19299424  0.20556851]
 [ 0.19949238  0.19601662  0.20612366  0.19062419  0.20774311]
 [ 0.19907743  0.19625193  0.2055223   0.19115263  0.20799571]
 [ 0.1984854   0.19487593  0.20750774  0.18796028  0.21117072]
 [ 0.19901067  0.19646688  0.20546895  0.19137245  0.20768104]
 [ 0.19842646  0.19398555  0.20888509  0.18615027  0.21255262]
 [ 0.19680431  0.1923037   0.20910244  0.18384433  0.21794529]
 [ 0.19910407  0.19577165  0.20673612  0.18970348  0.20868471]
 [ 0.19633792  0.19313359  0.20532909  0.18663502  0.21856444]
 [ 0.19551857  0.19212091  0.20559752  0.18516672  0.22159626]
 [ 0.19851224  0.19488649  0.20783176  0.18769279  0.21107674]
 [ 0.19678673  0.19214371  0.20893764  0.1838343   0.21829759]
 [ 0.19564961  0.19198008  0.20556052  0.18515861  0.22165117]
 [ 0.19941604  0.19673423  0.20536953  0.19197899  0.20650117]
 [ 0.19830047  0.1937764   0.20881134  0.18581235  0.21329944]
 [ 0.19927686  0.19618005  0.20606565  0.19052251  0.20795494]
 [ 0.1986915   0.1948622   0.20799872  0.18761986  0.21082772]
 [ 0.196741    0.19223762  0.2087072   0.18409848  0.21821573]
 [ 0.19900699  0.19545294  0.20712742  0.18886694  0.20954576]
 [ 0.1956372   0.19235553  0.2055617   0.18544956  0.22099598]
 [ 0.19797386  0.19374846  0.20886798  0.18572856  0.21368115]
 [ 0.19546752  0.19184785  0.20613411  0.1846461   0.22190443]
 [ 0.19923142  0.19628748  0.20602275  0.19077104  0.20768726]
 [ 0.19777597  0.19383001  0.2080678   0.18628033  0.21404588]
 [ 0.19947943  0.19670941  0.20534167  0.19204156  0.20642792]
 [ 0.19911905  0.1957812   0.20679857  0.18974267  0.20855851]
 [ 0.19701344  0.19226921  0.20918293  0.18384671  0.21768768]
 [ 0.1976144   0.19378255  0.2078169   0.18646643  0.21431969]
 [ 0.19737199  0.19320855  0.20753026  0.18590917  0.21598002]
 [ 0.19727497  0.19248629  0.20953053  0.18398021  0.21672805]
 [ 0.19925402  0.19579929  0.20651826  0.1900709   0.20835754]
 [ 0.19951764  0.19714431  0.20488359  0.1928366   0.20561786]
 [ 0.19907483  0.19719669  0.2043315   0.19325598  0.20614092]
 [ 0.19801947  0.19455364  0.20757538  0.18748911  0.21236233]
 [ 0.19837916  0.1937499   0.20919104  0.18577436  0.21290553]
 [ 0.19838689  0.19386885  0.2086827   0.18610941  0.21295214]
 [ 0.19799371  0.19343205  0.20941837  0.18493636  0.21421947]
 [ 0.19775337  0.19310744  0.20916516  0.18503262  0.21494144]
 [ 0.19801041  0.19305818  0.20964091  0.18467109  0.21461934]
 [ 0.19677477  0.19380599  0.20530285  0.18749054  0.21662584]
 [ 0.19621207  0.19216996  0.20743905  0.1842594   0.2199195 ]
 [ 0.19881408  0.1953471   0.20703919  0.18890776  0.20989197]
 [ 0.19883402  0.19553225  0.20687631  0.1891562   0.20960122]
 [ 0.1970616   0.19229747  0.20939718  0.18390073  0.21734308]
 [ 0.19884264  0.19604452  0.2066167   0.19002624  0.20846988]
 [ 0.19835171  0.19386195  0.20865984  0.1860135   0.21311301]
 [ 0.1991867   0.19658075  0.20558068  0.19156283  0.20708902]
 [ 0.1961982   0.19223996  0.20806934  0.18402888  0.2194636 ]
 [ 0.19685486  0.19227052  0.20900346  0.18392365  0.21794742]
 [ 0.19801834  0.19348636  0.2088834   0.18554497  0.21406683]
 [ 0.19738822  0.19482745  0.20537414  0.18915771  0.21325248]
 [ 0.1987467   0.1958002   0.20700163  0.18942501  0.20902641]
 [ 0.19938686  0.19602987  0.20638131  0.19035257  0.20784938]
 [ 0.19801967  0.19368172  0.20895797  0.18559353  0.2137472 ]
 [ 0.19839421  0.19409093  0.20881806  0.1862693   0.21242745]
 [ 0.1986649   0.1951315   0.20758839  0.18822142  0.21039385]
 [ 0.19900268  0.19606619  0.20638159  0.19027597  0.20827359]
 [ 0.19661219  0.19338141  0.20577927  0.1870288   0.21719833]
 [ 0.19815695  0.19552094  0.20770052  0.18864611  0.20997548]
 [ 0.19900635  0.19582859  0.2063487   0.18989374  0.20892262]
 [ 0.19939253  0.19694239  0.20506737  0.19244786  0.20614983]
 [ 0.19869229  0.19587602  0.20536479  0.19052354  0.20954342]
 [ 0.19923455  0.19599035  0.20623447  0.19018009  0.20836048]
 [ 0.19642198  0.19337581  0.2051807   0.18699747  0.21802406]
 [ 0.19546872  0.19183774  0.20575926  0.18480983  0.22212435]
 [ 0.19604561  0.19267057  0.20557238  0.18601036  0.21970101]
 [ 0.19751772  0.19299625  0.20876335  0.18516567  0.21555696]
 [ 0.19889368  0.19585785  0.20650919  0.18984161  0.20889769]
 [ 0.1992294   0.19709843  0.20468313  0.19290081  0.20608826]
 [ 0.19854356  0.19445007  0.2083866   0.18685569  0.21176413]
 [ 0.19585903  0.19253792  0.2055155   0.1858972   0.22019033]
 [ 0.19946747  0.19615141  0.20608416  0.19076517  0.20753178]
 [ 0.19925825  0.19661547  0.20550753  0.19175898  0.20685981]
 [ 0.19651149  0.19331649  0.20576917  0.1869342   0.21746868]
 [ 0.19630392  0.19225523  0.20824027  0.18409809  0.21910247]
 [ 0.19918098  0.19573718  0.20670132  0.18985473  0.20852581]
 [ 0.19612916  0.19318433  0.20525247  0.18680041  0.21863364]
 [ 0.19819184  0.19342029  0.2097065   0.18501195  0.21366948]
 [ 0.19578777  0.19198181  0.20663868  0.18454827  0.2210435 ]
 [ 0.19926837  0.19664794  0.20553136  0.19176447  0.20678782]
 [ 0.19937783  0.19664773  0.20545667  0.19173677  0.20678106]
 [ 0.19673659  0.19228356  0.20877205  0.18390416  0.21830368]
 [ 0.19598651  0.19270261  0.20583862  0.18598592  0.21948634]
 [ 0.19914351  0.19679637  0.20519249  0.19219092  0.20667671]
 [ 0.19624081  0.19229457  0.20711766  0.18478259  0.21956438]
 [ 0.19897763  0.19589759  0.20669362  0.18985493  0.2085762 ]
 [ 0.19615071  0.1928346   0.20581713  0.18612774  0.21906988]
 [ 0.19818018  0.19366482  0.2089645   0.18561237  0.2135781 ]
 [ 0.19833757  0.19374616  0.20899858  0.18567456  0.21324307]
 [ 0.19725017  0.19233392  0.20960885  0.18393697  0.21687013]
 [ 0.19647028  0.19222945  0.208413    0.18407154  0.21881574]
 [ 0.1992649   0.19702128  0.20503636  0.19246265  0.20621487]
 [ 0.19556937  0.19192798  0.20589028  0.18473616  0.22187619]
 [ 0.19683024  0.19221492  0.20895836  0.18388601  0.21811044]
 [ 0.1986199   0.19492081  0.20793401  0.18752585  0.21099944]
 [ 0.19790004  0.1941326   0.20792463  0.18681139  0.2132313 ]
 [ 0.19633339  0.19220868  0.2082637   0.18409067  0.21910353]
 [ 0.19604379  0.19267872  0.20570959  0.18602294  0.21954498]
 [ 0.19608946  0.19286121  0.20524214  0.18630216  0.219505  ]
 [ 0.19939645  0.19631654  0.20576973  0.19123991  0.20727742]
 [ 0.1992214   0.1960012   0.20645493  0.19032098  0.20800148]
 [ 0.19912393  0.195732    0.20637985  0.19015571  0.20860854]
 [ 0.19926128  0.19726875  0.20469217  0.19325037  0.2055274 ]
 [ 0.19560516  0.19190592  0.20627946  0.18448587  0.22172354]
 [ 0.19902815  0.19628188  0.2054209   0.19121     0.20805909]
 [ 0.19945383  0.19654639  0.20552844  0.1916365   0.20683485]
 [ 0.19921514  0.19603974  0.20629005  0.19050701  0.20794807]
 [ 0.19926839  0.19611639  0.20607217  0.19079354  0.20774953]
 [ 0.19614235  0.1922262   0.20786297  0.18412256  0.21964595]
 [ 0.19915684  0.19588424  0.20658629  0.18974687  0.20862579]
 [ 0.1993333   0.19594367  0.20643295  0.19041929  0.20787083]
 [ 0.19661807  0.19351108  0.20572284  0.18702453  0.21712345]
 [ 0.19930281  0.1964469   0.20595013  0.19127217  0.20702802]
 [ 0.19848816  0.19449377  0.20838374  0.1869987   0.21163565]
 [ 0.19838844  0.19420724  0.20882474  0.18637247  0.21220706]
 [ 0.19913845  0.19653551  0.20546745  0.1914985   0.20736   ]
 [ 0.19560155  0.19216076  0.20564333  0.18505621  0.2215382 ]
 [ 0.19603799  0.19284633  0.20531677  0.18628137  0.21951753]
 [ 0.19944805  0.19726242  0.20473118  0.19306378  0.20549455]
 [ 0.19694816  0.19231023  0.20894119  0.18398499  0.21781547]
 [ 0.19855958  0.19511552  0.20699714  0.18859127  0.21073651]
 [ 0.19569714  0.19213018  0.2054663   0.1853521   0.22135432]
 [ 0.19917341  0.19641949  0.20567708  0.19133154  0.20739852]
 [ 0.19833817  0.19469079  0.20808141  0.18724479  0.21164489]
 [ 0.19922434  0.19733882  0.20467041  0.19317836  0.20558812]
 [ 0.19912119  0.19699438  0.20479248  0.1926785   0.20641345]
 [ 0.19892043  0.19569241  0.2061723   0.18988155  0.20933333]
 [ 0.19840164  0.19426364  0.2082395   0.18667546  0.21241978]
 [ 0.19927707  0.19591315  0.20659658  0.19016519  0.20804797]
 [ 0.19792859  0.19285636  0.20976877  0.1843479   0.21509834]
 [ 0.19919296  0.19664127  0.20538491  0.19184901  0.20693189]
 [ 0.19574375  0.19230169  0.20556536  0.18545011  0.2209391 ]
 [ 0.19613938  0.1930393   0.20515069  0.18663952  0.21903108]
 [ 0.19865718  0.19450617  0.20823401  0.1871345   0.2114681 ]
 [ 0.1956834   0.19190282  0.20654346  0.18433554  0.22153479]
 [ 0.19824114  0.19454506  0.20810431  0.18709666  0.21201287]
 [ 0.19725031  0.19298701  0.20802091  0.18531793  0.21642391]
 [ 0.1979789   0.1933388   0.2090286   0.18538231  0.21427143]
 [ 0.19743334  0.19324058  0.20843361  0.18557343  0.21531905]
 [ 0.19754027  0.19346915  0.20804635  0.18590769  0.2150365 ]
 [ 0.19750726  0.19256741  0.20952874  0.18419228  0.21620432]
 [ 0.19924842  0.19704029  0.20492248  0.19268097  0.2061078 ]
 [ 0.19734214  0.19255252  0.20958616  0.18415239  0.2163668 ]
 [ 0.19900063  0.19633608  0.20545132  0.19116516  0.20804681]
 [ 0.19793782  0.19319105  0.2091817   0.18503584  0.21465357]
 [ 0.19800822  0.19312097  0.20962532  0.18482161  0.21442388]
 [ 0.19933134  0.19724648  0.20472106  0.19318262  0.20551851]
 [ 0.19576022  0.1919785   0.20660211  0.18435296  0.22130616]
 [ 0.19927795  0.19717754  0.20482528  0.19282849  0.20589074]
 [ 0.19882645  0.19568376  0.20652986  0.18963674  0.2093232 ]
 [ 0.19898462  0.19636887  0.20602676  0.19108672  0.20753303]
 [ 0.19933958  0.19621864  0.2059679   0.19071914  0.20775479]
 [ 0.19676028  0.19283994  0.2070981   0.1855222   0.21777946]
 [ 0.1981107   0.19512148  0.2078674   0.18791611  0.21098432]
 [ 0.19605464  0.19270225  0.20562494  0.18611839  0.21949975]
 [ 0.1985842   0.19480336  0.20766549  0.1876532   0.21129376]
 [ 0.19893628  0.19579162  0.20674051  0.18953288  0.20899871]
 [ 0.19940406  0.1965725   0.20541887  0.19172496  0.20687965]
 [ 0.19843446  0.19448683  0.20809929  0.18702641  0.21195306]
 [ 0.19943568  0.19714683  0.20483892  0.19292985  0.20564878]
 [ 0.19759689  0.19282205  0.2102405   0.18416812  0.21517247]
 [ 0.19877809  0.19592093  0.20563126  0.1904916   0.20917809]
 [ 0.19936581  0.19684772  0.20536949  0.1921889   0.20622809]]
DEBUG:root:[ Iteration 9 ] Training loss: [[ 0.19682521  0.18841326  0.2106424   0.18007247  0.22404668]
 [ 0.18922065  0.18163763  0.20617944  0.17339145  0.24957085]
 [ 0.18845293  0.17830585  0.21299867  0.1695212   0.25072131]
 [ 0.19159131  0.18161593  0.21331464  0.17230387  0.24117427]
 [ 0.18185656  0.17583628  0.19680293  0.16920353  0.27630076]
 [ 0.19660759  0.18830778  0.2100751   0.18006682  0.22494271]
 [ 0.19810848  0.19178826  0.2081275   0.184535    0.21744087]
 [ 0.18149818  0.17548461  0.19668229  0.16888431  0.27745059]
 [ 0.18517959  0.17686871  0.20679554  0.16866665  0.26248947]
 [ 0.19632514  0.18948638  0.20844691  0.18162495  0.22411664]
 [ 0.1981464   0.1922729   0.20772539  0.18526538  0.21658994]
 [ 0.18146327  0.17503819  0.19717839  0.1683937   0.27792645]
 [ 0.19630694  0.18803449  0.21013784  0.17966606  0.22585468]
 [ 0.18469661  0.17841902  0.19940929  0.17137323  0.26610184]
 [ 0.19494332  0.18656555  0.21125679  0.17742638  0.22980797]
 [ 0.18468566  0.17855564  0.19952792  0.17143418  0.26579663]
 [ 0.19630647  0.18859762  0.20949629  0.18042822  0.22517131]
 [ 0.18945879  0.17917328  0.21238279  0.17026801  0.24871713]
 [ 0.19524741  0.18807934  0.20805499  0.18001486  0.22860339]
 [ 0.19488382  0.18711677  0.20981546  0.17831509  0.2298689 ]
 [ 0.1949062   0.18690768  0.21129809  0.17785054  0.22903743]
 [ 0.19554633  0.18738958  0.21114486  0.17838092  0.2275383 ]
 [ 0.19631541  0.18910444  0.21027124  0.18079202  0.2235169 ]
 [ 0.18503365  0.17681697  0.20606279  0.16870385  0.26338276]
 [ 0.19087733  0.18079625  0.21331993  0.17158051  0.24342598]
 [ 0.18298392  0.17666157  0.19777896  0.16993123  0.27264434]
 [ 0.19180325  0.18264829  0.21219872  0.17323501  0.24011479]
 [ 0.19479598  0.18748112  0.20813912  0.17925222  0.23033156]
 [ 0.1913604   0.1814155   0.21303847  0.17215373  0.2420319 ]
 [ 0.19467196  0.1872277   0.20830336  0.17891158  0.23088543]
 [ 0.19688338  0.18882914  0.21000069  0.18047677  0.22381002]
 [ 0.18153317  0.17522211  0.19721381  0.16859043  0.27744046]
 [ 0.19063348  0.1825407   0.20895018  0.17365652  0.24421911]
 [ 0.1893589   0.17890954  0.21343848  0.16999087  0.24830218]
 [ 0.18970168  0.18133357  0.20895827  0.17252234  0.24748418]
 [ 0.18179756  0.17536522  0.19785482  0.16857246  0.27640992]
 [ 0.18938829  0.18141432  0.20719062  0.17297511  0.24903162]
 [ 0.18752971  0.18005584  0.20487864  0.17192709  0.25560871]
 [ 0.19436978  0.18579918  0.21186326  0.17638478  0.23158292]
 [ 0.19268776  0.18480366  0.21027994  0.17566213  0.23656644]
 [ 0.18878381  0.18255694  0.20307422  0.17523243  0.25035262]
 [ 0.1959291   0.18864512  0.20830001  0.18071596  0.22640979]
 [ 0.19745687  0.19038016  0.20873691  0.18269001  0.22073607]
 [ 0.19691819  0.18963611  0.20959914  0.18143168  0.22241491]
 [ 0.18753052  0.18119165  0.20219128  0.1739468   0.25513971]
 [ 0.18851636  0.1784061   0.21233077  0.16965146  0.25109527]
 [ 0.19402467  0.18516339  0.21185224  0.17571145  0.23324819]
 [ 0.19133438  0.18279164  0.21230875  0.1734117   0.24015354]
 [ 0.19063109  0.18055516  0.21317856  0.17143418  0.24420097]
 [ 0.1976535   0.19182472  0.20791489  0.1845378   0.21806912]
 [ 0.19412619  0.1856616   0.21182773  0.17631583  0.23206869]
 [ 0.19381385  0.18596935  0.20961982  0.17716721  0.23342982]
 [ 0.19147338  0.18290161  0.21224067  0.17351598  0.23986836]
 [ 0.19177079  0.183456    0.21238416  0.17408098  0.23830812]
 [ 0.18261592  0.17647122  0.19738343  0.16977318  0.27375627]
 [ 0.19567123  0.18805657  0.20944569  0.17951897  0.22730753]
 [ 0.19690314  0.18880317  0.21035706  0.18051146  0.22342516]
 [ 0.19134152  0.18191339  0.21225126  0.17260493  0.24188887]
 [ 0.18660204  0.18012248  0.20154354  0.17288432  0.25884765]
 [ 0.192045    0.18319817  0.21101178  0.17397112  0.2397739 ]
 [ 0.19794878  0.19153643  0.20813055  0.18444623  0.21793801]
 [ 0.19114225  0.18182266  0.21107383  0.17270577  0.24325551]
 [ 0.18727994  0.17805828  0.2096207   0.169517    0.25552404]
 [ 0.18351723  0.17728388  0.19873178  0.17036207  0.27010506]
 [ 0.19602397  0.18772635  0.2111453   0.17882927  0.22627518]
 [ 0.19416578  0.18595409  0.21144685  0.17652223  0.23191108]
 [ 0.19582151  0.18746942  0.21078779  0.17888449  0.22703682]
 [ 0.18331197  0.17735596  0.1976371   0.1705628   0.27113217]
 [ 0.18584789  0.17954288  0.20094597  0.17232783  0.26133543]
 [ 0.19636072  0.18756028  0.21108615  0.17904001  0.2259528 ]
 [ 0.18955463  0.17956182  0.21272279  0.17052393  0.24763687]
 [ 0.19532259  0.18669428  0.21164995  0.17764194  0.22869132]
 [ 0.19608289  0.18793418  0.21008734  0.17948104  0.22641455]
 [ 0.19458759  0.18689586  0.20944449  0.1781445   0.23092751]
 [ 0.19373892  0.18524775  0.21164201  0.17603941  0.23333192]
 [ 0.18209817  0.17587456  0.19716953  0.16924661  0.2756111 ]
 [ 0.19394098  0.18516202  0.21177317  0.17565224  0.23347165]
 [ 0.1975072   0.19150484  0.20804653  0.18406688  0.21887448]
 [ 0.19696739  0.19096279  0.2075727   0.18372172  0.22077537]
 [ 0.1956332   0.18708505  0.21150309  0.17797548  0.22780313]
 [ 0.19735423  0.19104636  0.20814504  0.18371844  0.21973586]
 [ 0.1968025   0.18808523  0.21078332  0.17979731  0.22453168]
 [ 0.19445758  0.18556572  0.21212196  0.17616539  0.23168938]
 [ 0.18187301  0.17542377  0.19815363  0.16852328  0.27602634]
 [ 0.18185343  0.17562175  0.19706134  0.16904511  0.27641839]
 [ 0.19589499  0.18826488  0.21085598  0.1794823   0.22550191]
 [ 0.18355128  0.17649736  0.20114732  0.16910261  0.26970145]
 [ 0.18170093  0.17541037  0.19754413  0.16865207  0.27669254]
 [ 0.19753861  0.19227698  0.20713972  0.18537989  0.21766479]
 [ 0.19720291  0.18895169  0.21011385  0.18083188  0.22289957]
 [ 0.18217905  0.17592543  0.1980093   0.16899979  0.2748864 ]
 [ 0.1908652   0.18083857  0.21319498  0.17153476  0.24356648]
 [ 0.19610539  0.1874529   0.21101855  0.17880566  0.22661752]
 [ 0.19778775  0.19220521  0.20743865  0.18525796  0.21731044]
 [ 0.18363602  0.17744741  0.19854376  0.17056623  0.26980656]
 [ 0.18898717  0.1785353   0.21284585  0.16974491  0.24988674]
 [ 0.19283348  0.18390429  0.21196057  0.1746324   0.23666924]
 [ 0.19594631  0.18762706  0.21113093  0.1788833   0.22641245]
 [ 0.19311501  0.18678036  0.20643134  0.17876777  0.23490553]
 [ 0.1957427   0.18769981  0.21074672  0.17885672  0.22695398]
 [ 0.18622398  0.17767429  0.20728096  0.16944543  0.25937527]
 [ 0.18646762  0.17740951  0.20935398  0.16897033  0.25779858]
 [ 0.19086871  0.18090244  0.21267462  0.17175072  0.24380355]
 [ 0.18360241  0.17645703  0.20226161  0.1687364   0.26894251]
 [ 0.19700426  0.18894668  0.21047379  0.18057059  0.22300477]
 [ 0.18230765  0.17594755  0.19717847  0.16930765  0.27525875]
 [ 0.18565732  0.17730658  0.20708385  0.16899408  0.26095814]
 [ 0.19412793  0.18552952  0.21202494  0.17602487  0.2322927 ]
 [ 0.18385287  0.17640167  0.20328967  0.16870473  0.26775107]
 [ 0.19572377  0.18734406  0.2112883   0.1783912   0.22725265]
 [ 0.19681871  0.18838078  0.21067576  0.17996965  0.2241551 ]
 [ 0.19058637  0.18059452  0.21332842  0.17132501  0.24416566]
 [ 0.19129756  0.18152486  0.21256357  0.17226984  0.24234417]
 [ 0.18643545  0.17811027  0.20692874  0.16985752  0.25866804]
 [ 0.18502556  0.17709857  0.20563158  0.1689093   0.26333502]
 [ 0.19637117  0.18772988  0.21111217  0.17920321  0.22558355]
 [ 0.19512774  0.18666379  0.21168064  0.17750944  0.22901845]
 [ 0.18220618  0.17578685  0.19880171  0.16871573  0.27448952]
 [ 0.19655105  0.18867716  0.20982206  0.18038262  0.22456709]
 [ 0.18474817  0.17838871  0.19940282  0.17137545  0.26608479]
 [ 0.19765659  0.1901399   0.2092638   0.1824306   0.22050908]
 [ 0.19455127  0.18649407  0.21001974  0.17759393  0.23134105]
 [ 0.18414217  0.17670257  0.20421106  0.16877583  0.26616839]
 [ 0.19531669  0.18684812  0.21142189  0.17786711  0.2285462 ]
 [ 0.18951064  0.17951667  0.21175334  0.17054856  0.24867071]
 [ 0.18404861  0.17821182  0.19879472  0.17114921  0.26779562]
 [ 0.18153277  0.17527616  0.19653498  0.16876478  0.27789131]
 [ 0.19471481  0.18612754  0.21154577  0.17692074  0.23069119]
 [ 0.19576189  0.18736157  0.21106812  0.17835848  0.22744989]
 [ 0.19697164  0.18861881  0.21046691  0.18028598  0.22365671]
 [ 0.18724409  0.17770669  0.21059345  0.16910273  0.25535303]
 [ 0.18719503  0.1778103   0.20964248  0.16929066  0.25606155]
 [ 0.18203877  0.17563392  0.19863647  0.16858065  0.27511021]
 [ 0.18515076  0.17896698  0.19957319  0.17188542  0.2644237 ]
 [ 0.19570231  0.18826696  0.21080674  0.17971647  0.22550759]
 [ 0.19802023  0.19244355  0.2075408   0.18545263  0.21654275]
 [ 0.18960617  0.17963326  0.21283938  0.17061122  0.24730998]
 [ 0.18513079  0.1769274   0.20636952  0.16875722  0.26281512]
 [ 0.19658008  0.19011162  0.20788039  0.18251094  0.22291692]
 [ 0.19030206  0.18037757  0.21316963  0.17119475  0.24495596]
 [ 0.19303864  0.18434161  0.21203321  0.17488253  0.23570399]
 [ 0.18155318  0.17556337  0.19654942  0.16901723  0.27731684]
 [ 0.192408    0.18363993  0.2110125   0.17445651  0.23848313]
 [ 0.18309186  0.17667955  0.19840129  0.16979955  0.27202773]
 [ 0.19556062  0.18723476  0.21024746  0.17872915  0.22822799]
 [ 0.18643856  0.17725569  0.20982236  0.16879319  0.25769016]
 [ 0.19698806  0.18868344  0.21039873  0.18047507  0.22345476]
 [ 0.19658534  0.18805727  0.21096347  0.17953542  0.22485852]
 [ 0.18227986  0.17568615  0.19991234  0.16839741  0.27372426]
 [ 0.18232156  0.17610645  0.19739261  0.16938187  0.27479753]
 [ 0.18421748  0.17833364  0.19889268  0.17126153  0.26729468]
 [ 0.19379456  0.18525223  0.21136102  0.17606276  0.2335294 ]
 [ 0.18719855  0.17741923  0.21141143  0.168842    0.25512883]
 [ 0.18336646  0.17714819  0.19816387  0.17028666  0.27103478]
 [ 0.18204831  0.17535311  0.19891334  0.16841987  0.27526543]
 [ 0.19593205  0.18752605  0.21127005  0.17868112  0.22659071]
 [ 0.19344722  0.18425429  0.21249379  0.17480822  0.23499642]
 [ 0.19622679  0.18771067  0.21119648  0.17883871  0.22602734]
 [ 0.19003028  0.18019727  0.2118223   0.17112449  0.2468257 ]
 [ 0.19076899  0.18072575  0.21353336  0.17140345  0.24356847]
 [ 0.18375446  0.17736925  0.19894576  0.17042224  0.26950827]
 [ 0.19475062  0.18673895  0.2103464   0.17785487  0.23030914]
 [ 0.19054639  0.18029661  0.21340428  0.17110549  0.24464725]
 [ 0.19645555  0.18883134  0.21016192  0.18037105  0.22418016]
 [ 0.19293956  0.18440174  0.21096806  0.17519295  0.23649766]
 [ 0.18774249  0.18139847  0.20224184  0.17413101  0.25448626]
 [ 0.18223765  0.17623135  0.19710021  0.16955984  0.27487096]
 [ 0.1826686   0.17660561  0.19735627  0.16987224  0.27349731]
 [ 0.19761102  0.19089532  0.20861045  0.18336175  0.21952145]
 [ 0.19072711  0.18113041  0.21102688  0.17211281  0.24500278]
 [ 0.19620255  0.18800209  0.21020047  0.17950475  0.22609009]
 [ 0.1924805   0.18425806  0.21016352  0.17517424  0.23792367]
 [ 0.19495918  0.18687624  0.21152775  0.17770046  0.22893636]
 [ 0.19003658  0.17994922  0.21198936  0.17093445  0.24709038]
 [ 0.1886456   0.17850728  0.21173932  0.1697482   0.25135964]
 [ 0.19583802  0.18749601  0.21117471  0.17868029  0.22681101]
 [ 0.19555447  0.18741174  0.21030232  0.1785626   0.22816882]
 [ 0.19585091  0.187277    0.21117772  0.17853384  0.22716048]
 [ 0.18156935  0.17513181  0.19804704  0.1682917   0.2769601 ]
 [ 0.19495121  0.18618742  0.21179792  0.17696677  0.23009671]
 [ 0.18710503  0.17754777  0.20992678  0.16911259  0.25630781]
 [ 0.19047596  0.18032703  0.21280761  0.17115027  0.24523906]
 [ 0.19796197  0.19234425  0.2077232   0.18550754  0.216463  ]
 [ 0.19598067  0.18740749  0.21124479  0.17852853  0.22683862]
 [ 0.19767436  0.19080102  0.20871127  0.18328619  0.21952717]
 [ 0.19191511  0.18271217  0.21278645  0.17316817  0.23941813]
 [ 0.18681607  0.17748022  0.20964089  0.16905078  0.25701204]
 [ 0.19122681  0.18174952  0.21193275  0.1724765   0.24261445]
 [ 0.19204019  0.18317492  0.21173523  0.17380697  0.23924273]
 [ 0.19013025  0.17998567  0.21362078  0.1707588   0.24550453]
 [ 0.19130415  0.18120798  0.21331322  0.17195415  0.24222049]
 [ 0.19401771  0.18538499  0.21175818  0.17619605  0.23264311]
 [ 0.19218443  0.18368468  0.21038696  0.17460418  0.2391398 ]
 [ 0.18462777  0.17834657  0.19938572  0.17131971  0.2663202 ]
 [ 0.19244041  0.18335775  0.21212825  0.17387861  0.23819493]
 [ 0.19341759  0.18449797  0.21191643  0.17500034  0.23516771]
 [ 0.18228358  0.17581891  0.19894855  0.16872494  0.27422401]
 [ 0.18968122  0.18083435  0.2098598   0.17199947  0.24762513]
 [ 0.1967352   0.18892629  0.2094126   0.18078795  0.22413792]
 [ 0.18227629  0.17603937  0.19752426  0.16927138  0.27488863]]
DEBUG:root:[ Iteration 12 ] Training loss: [[ 0.17525473  0.16498795  0.19535732  0.16093642  0.30346361]
 [ 0.16075546  0.15719655  0.16694282  0.15536955  0.35973558]
 [ 0.18490918  0.17316572  0.20114702  0.16690159  0.27387649]
 [ 0.16364868  0.15874644  0.17179167  0.15657887  0.34923434]
 [ 0.18171115  0.17041841  0.20091173  0.16467734  0.28228134]
 [ 0.1945072   0.18381925  0.20663211  0.17635514  0.23868629]
 [ 0.18452626  0.17227906  0.20390135  0.16600801  0.27328527]
 [ 0.19282112  0.18264739  0.20474052  0.17559007  0.24420089]
 [ 0.17118466  0.16282149  0.18697013  0.15946051  0.31956321]
 [ 0.19286446  0.18072954  0.20701773  0.1732693   0.24611907]
 [ 0.16941102  0.16158903  0.18488254  0.15862028  0.32549712]
 [ 0.19208434  0.17991708  0.20624898  0.17260417  0.24914543]
 [ 0.16992873  0.16174348  0.18644023  0.1587418   0.32314581]
 [ 0.1732523   0.16360842  0.19301866  0.16004226  0.31007841]
 [ 0.18769988  0.17597304  0.20206755  0.16938499  0.26487452]
 [ 0.16260801  0.15860236  0.16921656  0.15643479  0.35313824]
 [ 0.17692731  0.16622446  0.19702597  0.16175734  0.29806489]
 [ 0.18469591  0.17301875  0.20070955  0.16678815  0.2747876 ]
 [ 0.17765179  0.16926588  0.18846069  0.16466323  0.29995841]
 [ 0.15810856  0.15516022  0.16336395  0.15381667  0.36955056]
 [ 0.15996777  0.15659274  0.1657466   0.15491113  0.36278176]
 [ 0.15928115  0.15609135  0.16494234  0.15453021  0.36515501]
 [ 0.18447836  0.17335802  0.1992898   0.16728579  0.27558804]
 [ 0.15826969  0.15535392  0.16361892  0.15395917  0.36879826]
 [ 0.18784915  0.17534736  0.20417733  0.16863345  0.2639927 ]
 [ 0.16849867  0.16164227  0.18002452  0.15865998  0.33117455]
 [ 0.16124293  0.15683104  0.17011173  0.15511572  0.3566986 ]
 [ 0.16299745  0.15882482  0.16983679  0.15663181  0.3517091 ]
 [ 0.1944361   0.18389362  0.20645343  0.17645702  0.2387598 ]
 [ 0.19096792  0.17929871  0.20581482  0.17194971  0.25196883]
 [ 0.17766392  0.16703366  0.19775034  0.16223395  0.29531813]
 [ 0.18128003  0.16989976  0.20117924  0.1642009   0.28344002]
 [ 0.19406453  0.18285877  0.20648849  0.17552491  0.24106325]
 [ 0.18634957  0.17353667  0.20482789  0.16708018  0.2682057 ]
 [ 0.17317756  0.16352822  0.19304998  0.15998235  0.31026191]
 [ 0.16339953  0.15905143  0.17033722  0.15681826  0.35039356]
 [ 0.17654504  0.16713497  0.19287506  0.16242439  0.30102053]
 [ 0.1764503   0.16626297  0.19612414  0.16175705  0.29940552]
 [ 0.18815577  0.17481604  0.20562698  0.16810651  0.26329473]
 [ 0.19297844  0.18256061  0.20557266  0.17517668  0.24371161]
 [ 0.15876739  0.15551226  0.16482267  0.1540889   0.36680874]
 [ 0.17563625  0.1660213   0.19239569  0.16164713  0.30429965]
 [ 0.16071972  0.15711169  0.16697823  0.15531319  0.35987717]
 [ 0.17479242  0.16582367  0.19435222  0.16141658  0.30361512]
 [ 0.1644275   0.15853643  0.17757487  0.15645629  0.34300488]
 [ 0.15964058  0.15641671  0.16530961  0.15476716  0.36386597]
 [ 0.16218463  0.15817088  0.16899644  0.15613787  0.35451016]
 [ 0.1882989   0.17595614  0.20513524  0.16897209  0.26163769]
 [ 0.18896334  0.17585966  0.20524801  0.1690117   0.26091731]
 [ 0.16862571  0.16076693  0.18523552  0.15811089  0.32726094]
 [ 0.16262572  0.15766123  0.17227094  0.15574798  0.35169417]
 [ 0.16724271  0.1599147   0.18362921  0.15754241  0.331671  ]
 [ 0.19408865  0.18296516  0.20665349  0.17555413  0.24073866]
 [ 0.17439538  0.16447157  0.19397299  0.16058105  0.30657896]
 [ 0.19242562  0.1801167   0.2065779   0.17278263  0.24809711]
 [ 0.15835156  0.1552683   0.16398826  0.15391077  0.3684811 ]
 [ 0.16282074  0.15857936  0.16968662  0.15644285  0.3524704 ]
 [ 0.18824163  0.17570262  0.20438975  0.16892473  0.26274127]
 [ 0.19430748  0.18368271  0.20643413  0.1762477   0.239328  ]
 [ 0.19268402  0.18204668  0.20558222  0.17465554  0.24503152]
 [ 0.19362767  0.18298203  0.20596009  0.17556188  0.24186833]
 [ 0.18666515  0.17447381  0.2045033   0.16775335  0.26660439]
 [ 0.17890246  0.1690731   0.19509326  0.1638404   0.29309073]
 [ 0.17631306  0.16831735  0.18707627  0.16395231  0.30434102]
 [ 0.16466622  0.15861486  0.17792563  0.1565309   0.34226239]
 [ 0.1620798   0.15743707  0.17048852  0.15557769  0.35441691]
 [ 0.18449594  0.17261857  0.20363918  0.16621909  0.27302724]
 [ 0.16740246  0.16020745  0.1827185   0.15767629  0.33199528]
 [ 0.16923645  0.1610622   0.18715887  0.15834753  0.32419497]
 [ 0.1711542   0.16237566  0.18962164  0.15920539  0.31764311]
 [ 0.16149592  0.15787785  0.16758679  0.1558806   0.35715887]
 [ 0.17269515  0.16338418  0.19155335  0.15985896  0.3125084 ]
 [ 0.16101992  0.15743151  0.1671332   0.15554176  0.35887361]
 [ 0.18862376  0.17537546  0.20556705  0.1685479   0.26188579]
 [ 0.15946198  0.15624233  0.1650573   0.1546372   0.36460119]
 [ 0.16836958  0.16063973  0.18533039  0.15802704  0.32763332]
 [ 0.18331411  0.17176081  0.20260432  0.16562201  0.27669874]
 [ 0.16287251  0.15860753  0.16967352  0.15646707  0.35237938]
 [ 0.17437601  0.16457155  0.19345808  0.16065085  0.30694351]
 [ 0.16808327  0.16088     0.18189591  0.15811856  0.33102229]
 [ 0.19029553  0.17770363  0.20629066  0.17056903  0.25514111]
 [ 0.15963696  0.15637201  0.16535477  0.1547424   0.36389393]
 [ 0.18639244  0.17400335  0.20477995  0.16732541  0.26749885]
 [ 0.15944323  0.15585554  0.16621411  0.15436427  0.36412281]
 [ 0.18619086  0.17402691  0.2029826   0.16755466  0.26924494]
 [ 0.17548148  0.16620286  0.19541365  0.16169471  0.3012073 ]
 [ 0.19270442  0.1826199   0.20437877  0.17562902  0.24466786]
 [ 0.18778563  0.17547305  0.20321946  0.16893056  0.26459128]
 [ 0.16999391  0.16160856  0.1876896   0.1586899   0.32201797]
 [ 0.16476674  0.15872782  0.17835343  0.156602    0.34155002]
 [ 0.19244875  0.18039557  0.20719542  0.17294246  0.24701777]
 [ 0.18760867  0.17445982  0.20500255  0.16787694  0.26505205]
 [ 0.17264193  0.16325456  0.19163553  0.15980235  0.31266564]
 [ 0.17280728  0.16367559  0.19082813  0.16003464  0.31265438]
 [ 0.17380917  0.16407488  0.19360238  0.16032696  0.30818656]
 [ 0.18844132  0.1754165   0.20511822  0.16865724  0.26236671]
 [ 0.18991402  0.17727143  0.20541982  0.17019938  0.25719535]
 [ 0.18640822  0.17424354  0.20447241  0.16755238  0.26732346]
 [ 0.16790481  0.16042189  0.18500631  0.15786189  0.32880512]
 [ 0.15853885  0.15541546  0.16408709  0.15403211  0.36792651]
 [ 0.18648006  0.17375509  0.20504723  0.16714907  0.26756856]
 [ 0.17503966  0.16539279  0.19419636  0.16116194  0.30420926]
 [ 0.15837027  0.15525043  0.16392832  0.15390304  0.36854795]
 [ 0.15910606  0.15561967  0.16552217  0.15419622  0.36555588]
 [ 0.1930096   0.18177488  0.20606577  0.17442812  0.24472159]
 [ 0.17405827  0.1641541   0.19396597  0.16039293  0.30742875]
 [ 0.16101079  0.15742856  0.16706853  0.15553132  0.35896081]
 [ 0.18404378  0.17269509  0.20099638  0.16646525  0.27579951]
 [ 0.17326918  0.16364343  0.19268021  0.16005272  0.31035444]
 [ 0.16229303  0.15750003  0.17125103  0.15562209  0.35333386]
 [ 0.17352134  0.16470681  0.18938184  0.1607457   0.31164426]
 [ 0.19364406  0.18328464  0.20602793  0.17599079  0.24105261]
 [ 0.15847379  0.15534087  0.16407207  0.15396996  0.36814332]
 [ 0.17385752  0.16416496  0.19289505  0.16038974  0.30869275]
 [ 0.18401706  0.17241791  0.20121837  0.16624564  0.27610102]
 [ 0.17505772  0.1656189   0.194114    0.1613111   0.30389825]
 [ 0.17476697  0.1649127   0.19386528  0.16087645  0.30557856]
 [ 0.18485044  0.17254281  0.20350371  0.16621445  0.27288854]
 [ 0.18841481  0.17533246  0.20625941  0.16844533  0.26154798]
 [ 0.18806024  0.17502853  0.20457797  0.16839124  0.26394203]
 [ 0.16329528  0.1579086   0.17468727  0.15595585  0.348153  ]
 [ 0.16382818  0.15820742  0.17601651  0.15620145  0.3457464 ]
 [ 0.16581118  0.15923181  0.18036616  0.1569909   0.33759999]
 [ 0.18788041  0.17476267  0.20537071  0.16811703  0.2638692 ]
 [ 0.1762729   0.16631871  0.1954349   0.16178505  0.30018845]
 [ 0.15945008  0.15583637  0.16588184  0.15437247  0.36445916]
 [ 0.16184577  0.15720403  0.17159006  0.15540016  0.35396001]
 [ 0.16037063  0.15637635  0.16841635  0.15475303  0.36008361]
 [ 0.16998604  0.16143246  0.18889119  0.15861352  0.32107681]
 [ 0.16787528  0.16033946  0.18544106  0.15783569  0.32850847]
 [ 0.18498735  0.1743094   0.19767258  0.1682881   0.27474254]
 [ 0.18699168  0.17473313  0.20315783  0.16827773  0.26683956]
 [ 0.17283168  0.16332331  0.19265276  0.15985893  0.31133327]
 [ 0.16209076  0.15767148  0.1695565   0.15575501  0.35492626]
 [ 0.17107415  0.16216631  0.18956941  0.15908489  0.31810519]
 [ 0.18477409  0.17297149  0.20246446  0.16658196  0.27320808]
 [ 0.18163846  0.17036611  0.20120001  0.16455226  0.28224316]
 [ 0.1907791   0.17964971  0.20373748  0.17272149  0.2531122 ]
 [ 0.16288242  0.15770204  0.1735758   0.15579811  0.35004166]
 [ 0.15984152  0.15654179  0.16552693  0.15485603  0.36323372]
 [ 0.16085817  0.15667665  0.16895857  0.1549806   0.35852605]
 [ 0.18801154  0.17496485  0.20607989  0.16815244  0.26279131]
 [ 0.18704216  0.1760848   0.20003462  0.16970704  0.26713142]
 [ 0.18495111  0.173491    0.20405988  0.16694923  0.27054876]
 [ 0.19072238  0.17868613  0.20503017  0.17155641  0.25400493]
 [ 0.17552423  0.16536509  0.19473241  0.16117427  0.30320397]
 [ 0.17751801  0.1671655   0.19760202  0.16233806  0.29537639]
 [ 0.16361491  0.15955675  0.1704811   0.15716474  0.34918249]
 [ 0.18895893  0.17687924  0.20315805  0.17005388  0.26094988]
 [ 0.16632302  0.16017291  0.17655806  0.15761818  0.33932781]
 [ 0.16458049  0.15867475  0.17803729  0.1565468   0.34216073]
 [ 0.18745743  0.17501371  0.20374964  0.16832609  0.26545313]
 [ 0.16213307  0.158228    0.16875325  0.15614261  0.35474303]
 [ 0.1827272   0.17214014  0.20236297  0.16585761  0.27691209]
 [ 0.19123805  0.17976613  0.2048026   0.17264144  0.25155184]
 [ 0.16428098  0.15853752  0.17573595  0.15642476  0.34502083]
 [ 0.16147651  0.15780945  0.16770291  0.15582596  0.35718518]
 [ 0.15858909  0.15558092  0.16403933  0.15412961  0.367661  ]
 [ 0.16083804  0.15725887  0.16701049  0.15540418  0.35948843]
 [ 0.17269966  0.16411799  0.18830548  0.16033214  0.31454471]
 [ 0.18629713  0.17353612  0.20473278  0.16699496  0.26843902]
 [ 0.189981    0.17770761  0.2061002   0.17049387  0.25571731]
 [ 0.17972431  0.16967313  0.19573033  0.16429557  0.29057664]
 [ 0.19397675  0.18303412  0.20654958  0.17557727  0.24086225]
 [ 0.18541268  0.17364965  0.20188341  0.16722013  0.27183411]
 [ 0.1864275   0.17490892  0.20066072  0.16853027  0.26947263]
 [ 0.16324462  0.15855384  0.17076579  0.15643658  0.35099924]
 [ 0.16072646  0.15713917  0.16685489  0.15533492  0.35994455]
 [ 0.19314431  0.18251306  0.2062718   0.17509121  0.24297968]
 [ 0.17289796  0.16340603  0.19256724  0.15989722  0.31123152]
 [ 0.16177475  0.15717168  0.1713881   0.15537645  0.35428903]
 [ 0.18641545  0.17375119  0.20437297  0.16716468  0.26829576]
 [ 0.16396458  0.15934025  0.17127366  0.15706047  0.34836099]
 [ 0.18847439  0.17509444  0.20566961  0.16833293  0.26242867]
 [ 0.17653283  0.16714151  0.19639756  0.16231772  0.29761034]
 [ 0.19293343  0.18206066  0.20557864  0.17476498  0.24466227]
 [ 0.16840601  0.16076304  0.18445539  0.15807188  0.32830366]
 [ 0.16238551  0.15745243  0.17255408  0.15559888  0.35200906]
 [ 0.1608949   0.15736334  0.16705979  0.15548979  0.35919219]
 [ 0.1924248   0.18139681  0.20536034  0.17408222  0.24673584]
 [ 0.15881048  0.15550087  0.16477473  0.15409388  0.36681998]
 [ 0.19028576  0.17705013  0.20610784  0.1699893   0.256567  ]
 [ 0.15960811  0.15629524  0.16525409  0.15468311  0.36415946]
 [ 0.17564833  0.16541365  0.19550319  0.16121328  0.30222154]
 [ 0.1917278   0.17914428  0.20641091  0.17181446  0.25090259]
 [ 0.17172129  0.16396765  0.18437544  0.16035312  0.31958249]
 [ 0.19345208  0.18311988  0.20630793  0.17567769  0.2414424 ]
 [ 0.16231313  0.15855958  0.16882102  0.1564061   0.35390016]
 [ 0.16133898  0.15762527  0.16751184  0.15570129  0.35782266]
 [ 0.18684831  0.17396627  0.20503603  0.16730937  0.26683998]
 [ 0.19334392  0.18183334  0.20667073  0.17436005  0.24379189]
 [ 0.16064872  0.15705436  0.16683438  0.1552761   0.36018643]
 [ 0.15887977  0.15572941  0.16455735  0.15425484  0.36657864]
 [ 0.1671562   0.16063181  0.17854099  0.15793791  0.33573309]
 [ 0.17911562  0.1694129   0.19950898  0.1638443   0.28811821]
 [ 0.16574016  0.1592717   0.18023963  0.15700044  0.33774805]
 [ 0.18312395  0.17196156  0.20285672  0.16571851  0.27633929]
 [ 0.16698474  0.15995157  0.18351008  0.1575142   0.33203942]
 [ 0.18158308  0.16973558  0.20149852  0.16415356  0.28302929]
 [ 0.18650047  0.17368235  0.20458576  0.16720235  0.26802912]]
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-20-2016_11h24m18s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: [[ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]]
DEBUG:root:[ Iteration 0 ] Test loss: [[ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]]
DEBUG:root:[ Iteration 3 ] Training loss: [[ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]]
DEBUG:root:[ Iteration 6 ] Training loss: [[ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]]
DEBUG:root:[ Iteration 9 ] Training loss: [[ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 1.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.]]
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-20-2016_11h25m40s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.43
DEBUG:root:[ Iteration 0 ] Test loss: 0.4375
DEBUG:root:[ Iteration 3 ] Training loss: 0.5375
DEBUG:root:[ Iteration 6 ] Training loss: 0.5625
DEBUG:root:[ Iteration 9 ] Training loss: 0.5625
DEBUG:root:[ Iteration 12 ] Training loss: 0.54
DEBUG:root:[ Iteration 15 ] Training loss: 0.5475
DEBUG:root:[ Iteration 18 ] Training loss: 0.595
DEBUG:root:[ Iteration 20 ] Test loss: 0.565
DEBUG:root:[ Iteration 21 ] Training loss: 0.6025
DEBUG:root:[ Iteration 24 ] Training loss: 0.6025
DEBUG:root:[ Iteration 27 ] Training loss: 0.555
DEBUG:root:[ Iteration 30 ] Training loss: 0.5675
DEBUG:root:[ Iteration 33 ] Training loss: 0.6125
DEBUG:root:[ Iteration 36 ] Training loss: 0.645
DEBUG:root:[ Iteration 39 ] Training loss: 0.6375
DEBUG:root:[ Iteration 40 ] Test loss: 0.6175
DEBUG:root:[ Iteration 42 ] Training loss: 0.62
DEBUG:root:[ Iteration 45 ] Training loss: 0.615
DEBUG:root:[ Iteration 48 ] Training loss: 0.6575
DEBUG:root:[ Iteration 51 ] Training loss: 0.65
DEBUG:root:[ Iteration 54 ] Training loss: 0.655
DEBUG:root:[ Iteration 57 ] Training loss: 0.6225
DEBUG:root:[ Iteration 60 ] Training loss: 0.65
DEBUG:root:[ Iteration 60 ] Test loss: 0.645
DEBUG:root:[ Iteration 63 ] Training loss: 0.685
DEBUG:root:[ Iteration 66 ] Training loss: 0.6825
DEBUG:root:[ Iteration 69 ] Training loss: 0.665
DEBUG:root:[ Iteration 72 ] Training loss: 0.6525
DEBUG:root:[ Iteration 75 ] Training loss: 0.66
DEBUG:root:[ Iteration 78 ] Training loss: 0.695
DEBUG:root:[ Iteration 80 ] Test loss: 0.6575
DEBUG:root:[ Iteration 81 ] Training loss: 0.6775
DEBUG:root:[ Iteration 84 ] Training loss: 0.68
DEBUG:root:[ Iteration 87 ] Training loss: 0.7075
DEBUG:root:[ Iteration 90 ] Training loss: 0.705
DEBUG:root:[ Iteration 93 ] Training loss: 0.695
DEBUG:root:[ Iteration 96 ] Training loss: 0.695
DEBUG:root:[ Iteration 99 ] Training loss: 0.645
DEBUG:root:[ Iteration 100 ] Test loss: 0.715
DEBUG:root:[ Iteration 102 ] Training loss: 0.715
DEBUG:root:[ Iteration 105 ] Training loss: 0.715
DEBUG:root:[ Iteration 108 ] Training loss: 0.71
DEBUG:root:[ Iteration 111 ] Training loss: 0.73
DEBUG:root:[ Iteration 114 ] Training loss: 0.7
DEBUG:root:[ Iteration 117 ] Training loss: 0.685
DEBUG:root:[ Iteration 120 ] Training loss: 0.7325
DEBUG:root:[ Iteration 120 ] Test loss: 0.72
DEBUG:root:[ Iteration 123 ] Training loss: 0.7175
DEBUG:root:[ Iteration 126 ] Training loss: 0.7675
DEBUG:root:[ Iteration 129 ] Training loss: 0.71
DEBUG:root:[ Iteration 132 ] Training loss: 0.7525
DEBUG:root:[ Iteration 135 ] Training loss: 0.7175
DEBUG:root:[ Iteration 138 ] Training loss: 0.72
DEBUG:root:[ Iteration 140 ] Test loss: 0.7
DEBUG:root:[ Iteration 141 ] Training loss: 0.7525
DEBUG:root:[ Iteration 144 ] Training loss: 0.7425
DEBUG:root:[ Iteration 147 ] Training loss: 0.7525
DEBUG:root:[ Iteration 150 ] Training loss: 0.7325
DEBUG:root:[ Iteration 153 ] Training loss: 0.7375
DEBUG:root:[ Iteration 156 ] Training loss: 0.7325
DEBUG:root:[ Iteration 159 ] Training loss: 0.7725
DEBUG:root:[ Iteration 160 ] Test loss: 0.7175
DEBUG:root:[ Iteration 162 ] Training loss: 0.7725
DEBUG:root:[ Iteration 165 ] Training loss: 0.8025
DEBUG:root:[ Iteration 168 ] Training loss: 0.775
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-20-2016_11h39m28s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.2175
DEBUG:root:[ Iteration 0 ] Test loss: 0.2375
DEBUG:root:[ Iteration 3 ] Training loss: 0.5275
DEBUG:root:[ Iteration 6 ] Training loss: 0.565
DEBUG:root:[ Iteration 9 ] Training loss: 0.5225
DEBUG:root:[ Iteration 12 ] Training loss: 0.55
DEBUG:root:[ Iteration 15 ] Training loss: 0.5525
DEBUG:root:[ Iteration 18 ] Training loss: 0.5975
DEBUG:root:[ Iteration 20 ] Test loss: 0.5925
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-20-2016_11h43m17s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.7775
DEBUG:root:[ Iteration 0 ] Test loss: 0.7575
DEBUG:root:[ Iteration 3 ] Training loss: 0.775
DEBUG:root:[ Iteration 6 ] Training loss: 0.79
DEBUG:root:[ Iteration 9 ] Training loss: 0.7825
DEBUG:root:[ Iteration 12 ] Training loss: 0.7675
DEBUG:root:[ Iteration 15 ] Training loss: 0.7875
DEBUG:root:[ Iteration 18 ] Training loss: 0.7825
DEBUG:root:[ Iteration 20 ] Test loss: 0.7025
DEBUG:root:[ Iteration 21 ] Training loss: 0.77
DEBUG:root:[ Iteration 24 ] Training loss: 0.805
DEBUG:root:[ Iteration 27 ] Training loss: 0.7675
DEBUG:root:[ Iteration 30 ] Training loss: 0.7725
DEBUG:root:[ Iteration 33 ] Training loss: 0.8125
DEBUG:root:[ Iteration 36 ] Training loss: 0.8125
DEBUG:root:[ Iteration 39 ] Training loss: 0.8525
DEBUG:root:[ Iteration 40 ] Test loss: 0.7575
DEBUG:root:[ Iteration 42 ] Training loss: 0.8
DEBUG:root:[ Iteration 45 ] Training loss: 0.815
DEBUG:root:[ Iteration 48 ] Training loss: 0.7825
DEBUG:root:[ Iteration 51 ] Training loss: 0.84
DEBUG:root:[ Iteration 54 ] Training loss: 0.8125
DEBUG:root:[ Iteration 57 ] Training loss: 0.8075
DEBUG:root:[ Iteration 60 ] Training loss: 0.8225
DEBUG:root:[ Iteration 60 ] Test loss: 0.76
DEBUG:root:[ Iteration 63 ] Training loss: 0.81
DEBUG:root:[ Iteration 66 ] Training loss: 0.875
DEBUG:root:[ Iteration 69 ] Training loss: 0.86
DEBUG:root:[ Iteration 72 ] Training loss: 0.87
DEBUG:root:[ Iteration 75 ] Training loss: 0.85
DEBUG:root:[ Iteration 78 ] Training loss: 0.8275
DEBUG:root:[ Iteration 80 ] Test loss: 0.7725
DEBUG:root:[ Iteration 81 ] Training loss: 0.865
DEBUG:root:[ Iteration 84 ] Training loss: 0.85
DEBUG:root:[ Iteration 87 ] Training loss: 0.895
DEBUG:root:[ Iteration 90 ] Training loss: 0.845
DEBUG:root:[ Iteration 93 ] Training loss: 0.8325
DEBUG:root:[ Iteration 96 ] Training loss: 0.8775
DEBUG:root:[ Iteration 99 ] Training loss: 0.8375
DEBUG:root:[ Iteration 100 ] Test loss: 0.815
DEBUG:root:[ Iteration 102 ] Training loss: 0.8575
DEBUG:root:[ Iteration 105 ] Training loss: 0.8925
DEBUG:root:[ Iteration 108 ] Training loss: 0.8625
DEBUG:root:[ Iteration 111 ] Training loss: 0.8575
DEBUG:root:[ Iteration 114 ] Training loss: 0.865
DEBUG:root:[ Iteration 117 ] Training loss: 0.835
DEBUG:root:[ Iteration 120 ] Training loss: 0.865
DEBUG:root:[ Iteration 120 ] Test loss: 0.835
DEBUG:root:[ Iteration 123 ] Training loss: 0.8725
DEBUG:root:[ Iteration 126 ] Training loss: 0.865
DEBUG:root:[ Iteration 129 ] Training loss: 0.8475
DEBUG:root:[ Iteration 132 ] Training loss: 0.8875
DEBUG:root:[ Iteration 135 ] Training loss: 0.885
DEBUG:root:[ Iteration 138 ] Training loss: 0.8775
DEBUG:root:[ Iteration 140 ] Test loss: 0.8125
DEBUG:root:[ Iteration 141 ] Training loss: 0.885
DEBUG:root:[ Iteration 144 ] Training loss: 0.8625
DEBUG:root:[ Iteration 147 ] Training loss: 0.8825
DEBUG:root:[ Iteration 150 ] Training loss: 0.8875
DEBUG:root:[ Iteration 153 ] Training loss: 0.9225
DEBUG:root:[ Iteration 156 ] Training loss: 0.8825
DEBUG:root:[ Iteration 159 ] Training loss: 0.875
DEBUG:root:[ Iteration 160 ] Test loss: 0.81
DEBUG:root:[ Iteration 162 ] Training loss: 0.88
DEBUG:root:[ Iteration 165 ] Training loss: 0.8675
DEBUG:root:[ Iteration 168 ] Training loss: 0.8525
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-20-2016_11h50m08s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.89
DEBUG:root:[ Iteration 0 ] Test loss: 0.865
DEBUG:root:[ Iteration 3 ] Training loss: 0.8775
DEBUG:root:[ Iteration 6 ] Training loss: 0.9075
DEBUG:root:[ Iteration 9 ] Training loss: 0.905
DEBUG:root:[ Iteration 12 ] Training loss: 0.9175
DEBUG:root:[ Iteration 15 ] Training loss: 0.8925
DEBUG:root:[ Iteration 18 ] Training loss: 0.9125
DEBUG:root:[ Iteration 20 ] Test loss: 0.825
DEBUG:root:[ Iteration 21 ] Training loss: 0.9175
DEBUG:root:[ Iteration 24 ] Training loss: 0.895
DEBUG:root:[ Iteration 27 ] Training loss: 0.895
DEBUG:root:[ Iteration 30 ] Training loss: 0.915
DEBUG:root:[ Iteration 33 ] Training loss: 0.9025
DEBUG:root:[ Iteration 36 ] Training loss: 0.9025
DEBUG:root:[ Iteration 39 ] Training loss: 0.9
DEBUG:root:[ Iteration 40 ] Test loss: 0.8175
DEBUG:root:[ Iteration 42 ] Training loss: 0.905
DEBUG:root:[ Iteration 45 ] Training loss: 0.9
DEBUG:root:[ Iteration 48 ] Training loss: 0.925
DEBUG:root:[ Iteration 51 ] Training loss: 0.935
DEBUG:root:[ Iteration 54 ] Training loss: 0.8975
DEBUG:root:[ Iteration 57 ] Training loss: 0.885
DEBUG:root:[ Iteration 60 ] Training loss: 0.895
DEBUG:root:[ Iteration 60 ] Test loss: 0.8275
DEBUG:root:[ Iteration 63 ] Training loss: 0.9275
DEBUG:root:[ Iteration 66 ] Training loss: 0.91
DEBUG:root:[ Iteration 69 ] Training loss: 0.91
DEBUG:root:[ Iteration 72 ] Training loss: 0.8925
DEBUG:root:[ Iteration 75 ] Training loss: 0.885
DEBUG:root:[ Iteration 78 ] Training loss: 0.935
DEBUG:root:[ Iteration 80 ] Test loss: 0.8475
DEBUG:root:[ Iteration 81 ] Training loss: 0.9225
DEBUG:root:[ Iteration 84 ] Training loss: 0.9
DEBUG:root:[ Iteration 87 ] Training loss: 0.9225
DEBUG:root:[ Iteration 90 ] Training loss: 0.9225
DEBUG:root:[ Iteration 93 ] Training loss: 0.9325
DEBUG:root:[ Iteration 96 ] Training loss: 0.9175
DEBUG:root:[ Iteration 99 ] Training loss: 0.9225
DEBUG:root:[ Iteration 100 ] Test loss: 0.84
DEBUG:root:[ Iteration 102 ] Training loss: 0.9175
DEBUG:root:[ Iteration 105 ] Training loss: 0.92
DEBUG:root:[ Iteration 108 ] Training loss: 0.915
DEBUG:root:[ Iteration 111 ] Training loss: 0.9275
DEBUG:root:[ Iteration 114 ] Training loss: 0.9025
DEBUG:root:[ Iteration 117 ] Training loss: 0.9175
DEBUG:root:[ Iteration 120 ] Training loss: 0.9225
DEBUG:root:[ Iteration 120 ] Test loss: 0.855
DEBUG:root:[ Iteration 123 ] Training loss: 0.9
DEBUG:root:[ Iteration 126 ] Training loss: 0.9425
DEBUG:root:[ Iteration 129 ] Training loss: 0.9425
DEBUG:root:[ Iteration 132 ] Training loss: 0.9325
DEBUG:root:[ Iteration 135 ] Training loss: 0.925
DEBUG:root:[ Iteration 138 ] Training loss: 0.9225
DEBUG:root:[ Iteration 140 ] Test loss: 0.87
DEBUG:root:[ Iteration 141 ] Training loss: 0.935
DEBUG:root:[ Iteration 144 ] Training loss: 0.9325
DEBUG:root:[ Iteration 147 ] Training loss: 0.92
DEBUG:root:[ Iteration 150 ] Training loss: 0.9325
DEBUG:root:[ Iteration 153 ] Training loss: 0.9525
DEBUG:root:[ Iteration 156 ] Training loss: 0.92
DEBUG:root:[ Iteration 159 ] Training loss: 0.92
DEBUG:root:[ Iteration 160 ] Test loss: 0.8675
DEBUG:root:[ Iteration 162 ] Training loss: 0.95
DEBUG:root:[ Iteration 165 ] Training loss: 0.94
DEBUG:root:[ Iteration 168 ] Training loss: 0.9225
DEBUG:root:[ Iteration 171 ] Training loss: 0.9425
DEBUG:root:[ Iteration 174 ] Training loss: 0.92
DEBUG:root:[ Iteration 177 ] Training loss: 0.9425
DEBUG:root:[ Iteration 180 ] Training loss: 0.9425
DEBUG:root:[ Iteration 180 ] Test loss: 0.8475
DEBUG:root:[ Iteration 183 ] Training loss: 0.9275
DEBUG:root:[ Iteration 186 ] Training loss: 0.94
DEBUG:root:[ Iteration 189 ] Training loss: 0.9425
DEBUG:root:[ Iteration 192 ] Training loss: 0.94
DEBUG:root:[ Iteration 195 ] Training loss: 0.9325
DEBUG:root:[ Iteration 198 ] Training loss: 0.9375
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-20-2016_12h57m16s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.175
DEBUG:root:[ Iteration 0 ] Test loss: 0.16
DEBUG:root:[ Iteration 3 ] Training loss: 0.42
DEBUG:root:[ Iteration 6 ] Training loss: 0.52
DEBUG:root:[ Iteration 9 ] Training loss: 0.5525
DEBUG:root:[ Iteration 12 ] Training loss: 0.5725
DEBUG:root:[ Iteration 15 ] Training loss: 0.5375
DEBUG:root:[ Iteration 18 ] Training loss: 0.545
DEBUG:root:[ Iteration 20 ] Test loss: 0.5325
DEBUG:root:[ Iteration 21 ] Training loss: 0.495
DEBUG:root:[ Iteration 24 ] Training loss: 0.5475
DEBUG:root:[ Iteration 27 ] Training loss: 0.5925
DEBUG:root:[ Iteration 30 ] Training loss: 0.565
DEBUG:root:[ Iteration 33 ] Training loss: 0.6125
DEBUG:root:[ Iteration 36 ] Training loss: 0.62
DEBUG:root:[ Iteration 39 ] Training loss: 0.6225
DEBUG:root:[ Iteration 40 ] Test loss: 0.6025
DEBUG:root:[ Iteration 42 ] Training loss: 0.6075
DEBUG:root:[ Iteration 45 ] Training loss: 0.615
DEBUG:root:[ Iteration 48 ] Training loss: 0.64
DEBUG:root:[ Iteration 51 ] Training loss: 0.6475
DEBUG:root:[ Iteration 54 ] Training loss: 0.66
DEBUG:root:[ Iteration 57 ] Training loss: 0.6625
DEBUG:root:[ Iteration 60 ] Training loss: 0.7
DEBUG:root:[ Iteration 60 ] Test loss: 0.6725
DEBUG:root:[ Iteration 63 ] Training loss: 0.6825
DEBUG:root:[ Iteration 66 ] Training loss: 0.6625
DEBUG:root:[ Iteration 69 ] Training loss: 0.7125
DEBUG:root:[ Iteration 72 ] Training loss: 0.725
DEBUG:root:[ Iteration 75 ] Training loss: 0.76
DEBUG:root:[ Iteration 78 ] Training loss: 0.73
DEBUG:root:[ Iteration 80 ] Test loss: 0.6825
DEBUG:root:[ Iteration 81 ] Training loss: 0.6975
DEBUG:root:[ Iteration 84 ] Training loss: 0.665
DEBUG:root:[ Iteration 87 ] Training loss: 0.8
DEBUG:root:[ Iteration 90 ] Training loss: 0.74
DEBUG:root:[ Iteration 93 ] Training loss: 0.7475
DEBUG:root:[ Iteration 96 ] Training loss: 0.7475
DEBUG:root:[ Iteration 99 ] Training loss: 0.7325
DEBUG:root:[ Iteration 100 ] Test loss: 0.78
DEBUG:root:[ Iteration 102 ] Training loss: 0.7575
DEBUG:root:[ Iteration 105 ] Training loss: 0.7675
DEBUG:root:[ Iteration 108 ] Training loss: 0.7775
DEBUG:root:[ Iteration 111 ] Training loss: 0.8125
DEBUG:root:[ Iteration 114 ] Training loss: 0.8175
DEBUG:root:[ Iteration 117 ] Training loss: 0.7825
DEBUG:root:[ Iteration 120 ] Training loss: 0.8225
DEBUG:root:[ Iteration 120 ] Test loss: 0.76
DEBUG:root:[ Iteration 123 ] Training loss: 0.8
DEBUG:root:[ Iteration 126 ] Training loss: 0.8525
DEBUG:root:[ Iteration 129 ] Training loss: 0.815
DEBUG:root:[ Iteration 132 ] Training loss: 0.82
DEBUG:root:[ Iteration 135 ] Training loss: 0.845
DEBUG:root:[ Iteration 138 ] Training loss: 0.87
DEBUG:root:[ Iteration 140 ] Test loss: 0.8225
DEBUG:root:[ Iteration 141 ] Training loss: 0.8525
DEBUG:root:[ Iteration 144 ] Training loss: 0.875
DEBUG:root:[ Iteration 147 ] Training loss: 0.8225
DEBUG:root:[ Iteration 150 ] Training loss: 0.8975
DEBUG:root:[ Iteration 153 ] Training loss: 0.8925
DEBUG:root:[ Iteration 156 ] Training loss: 0.8725
DEBUG:root:[ Iteration 159 ] Training loss: 0.8725
DEBUG:root:[ Iteration 160 ] Test loss: 0.8425
DEBUG:root:[ Iteration 162 ] Training loss: 0.845
DEBUG:root:[ Iteration 165 ] Training loss: 0.9
DEBUG:root:[ Iteration 168 ] Training loss: 0.855
DEBUG:root:[ Iteration 171 ] Training loss: 0.85
DEBUG:root:[ Iteration 174 ] Training loss: 0.875
DEBUG:root:[ Iteration 177 ] Training loss: 0.8675
DEBUG:root:[ Iteration 180 ] Training loss: 0.87
DEBUG:root:[ Iteration 180 ] Test loss: 0.8025
DEBUG:root:[ Iteration 183 ] Training loss: 0.865
DEBUG:root:[ Iteration 186 ] Training loss: 0.85
DEBUG:root:[ Iteration 189 ] Training loss: 0.8725
DEBUG:root:[ Iteration 192 ] Training loss: 0.8675
DEBUG:root:[ Iteration 195 ] Training loss: 0.915
DEBUG:root:[ Iteration 198 ] Training loss: 0.9075
DEBUG:root:[ Iteration 200 ] Test loss: 0.84
DEBUG:root:[ Iteration 201 ] Training loss: 0.8875
DEBUG:root:[ Iteration 204 ] Training loss: 0.9
DEBUG:root:[ Iteration 207 ] Training loss: 0.865
DEBUG:root:[ Iteration 210 ] Training loss: 0.915
DEBUG:root:[ Iteration 213 ] Training loss: 0.8875
DEBUG:root:[ Iteration 216 ] Training loss: 0.905
DEBUG:root:[ Iteration 219 ] Training loss: 0.8625
DEBUG:root:[ Iteration 220 ] Test loss: 0.8325
DEBUG:root:[ Iteration 222 ] Training loss: 0.9175
DEBUG:root:[ Iteration 225 ] Training loss: 0.925
DEBUG:root:[ Iteration 228 ] Training loss: 0.8825
DEBUG:root:[ Iteration 231 ] Training loss: 0.8675
DEBUG:root:[ Iteration 234 ] Training loss: 0.9275
DEBUG:root:[ Iteration 237 ] Training loss: 0.9175
DEBUG:root:[ Iteration 240 ] Training loss: 0.9225
DEBUG:root:[ Iteration 240 ] Test loss: 0.8575
DEBUG:root:[ Iteration 243 ] Training loss: 0.8975
DEBUG:root:[ Iteration 246 ] Training loss: 0.9125
DEBUG:root:[ Iteration 249 ] Training loss: 0.93
DEBUG:root:[ Iteration 252 ] Training loss: 0.9225
DEBUG:root:[ Iteration 255 ] Training loss: 0.9175
DEBUG:root:[ Iteration 258 ] Training loss: 0.92
DEBUG:root:[ Iteration 260 ] Test loss: 0.85
DEBUG:root:[ Iteration 261 ] Training loss: 0.9175
DEBUG:root:[ Iteration 264 ] Training loss: 0.93
DEBUG:root:[ Iteration 267 ] Training loss: 0.9325
DEBUG:root:[ Iteration 270 ] Training loss: 0.9
DEBUG:root:[ Iteration 273 ] Training loss: 0.9375
DEBUG:root:[ Iteration 276 ] Training loss: 0.9075
DEBUG:root:[ Iteration 279 ] Training loss: 0.905
DEBUG:root:[ Iteration 280 ] Test loss: 0.8725
DEBUG:root:[ Iteration 282 ] Training loss: 0.9025
DEBUG:root:[ Iteration 285 ] Training loss: 0.905
DEBUG:root:[ Iteration 288 ] Training loss: 0.945
DEBUG:root:[ Iteration 291 ] Training loss: 0.9325
DEBUG:root:[ Iteration 294 ] Training loss: 0.9275
DEBUG:root:[ Iteration 297 ] Training loss: 0.9075
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-21-2016_11h58m28s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.9325
DEBUG:root:[ Iteration 0 ] Test loss: 0.8875
DEBUG:root:[ Iteration 3 ] Training loss: 0.94
DEBUG:root:[ Iteration 6 ] Training loss: 0.9125
DEBUG:root:[ Iteration 9 ] Training loss: 0.9375
DEBUG:root:[ Iteration 12 ] Training loss: 0.9325
DEBUG:root:[ Iteration 15 ] Training loss: 0.9325
DEBUG:root:[ Iteration 18 ] Training loss: 0.955
DEBUG:root:[ Iteration 20 ] Test loss: 0.8625
DEBUG:root:[ Iteration 21 ] Training loss: 0.9325
DEBUG:root:[ Iteration 24 ] Training loss: 0.935
DEBUG:root:[ Iteration 27 ] Training loss: 0.955
DEBUG:root:[ Iteration 30 ] Training loss: 0.9425
DEBUG:root:[ Iteration 33 ] Training loss: 0.9275
DEBUG:root:[ Iteration 36 ] Training loss: 0.945
DEBUG:root:[ Iteration 39 ] Training loss: 0.9225
DEBUG:root:[ Iteration 40 ] Test loss: 0.895
DEBUG:root:[ Iteration 42 ] Training loss: 0.92
DEBUG:root:[ Iteration 45 ] Training loss: 0.94
DEBUG:root:[ Iteration 48 ] Training loss: 0.9275
DEBUG:root:[ Iteration 51 ] Training loss: 0.94
DEBUG:root:[ Iteration 54 ] Training loss: 0.935
DEBUG:root:[ Iteration 57 ] Training loss: 0.945
DEBUG:root:[ Iteration 60 ] Training loss: 0.955
DEBUG:root:[ Iteration 60 ] Test loss: 0.8575
DEBUG:root:[ Iteration 63 ] Training loss: 0.9275
DEBUG:root:[ Iteration 66 ] Training loss: 0.9275
DEBUG:root:[ Iteration 69 ] Training loss: 0.94
DEBUG:root:[ Iteration 72 ] Training loss: 0.945
DEBUG:root:[ Iteration 75 ] Training loss: 0.945
DEBUG:root:[ Iteration 78 ] Training loss: 0.93
DEBUG:root:[ Iteration 80 ] Test loss: 0.8525
DEBUG:root:[ Iteration 81 ] Training loss: 0.945
DEBUG:root:[ Iteration 84 ] Training loss: 0.9275
DEBUG:root:[ Iteration 87 ] Training loss: 0.9475
DEBUG:root:[ Iteration 90 ] Training loss: 0.95
DEBUG:root:[ Iteration 93 ] Training loss: 0.9575
DEBUG:root:[ Iteration 96 ] Training loss: 0.92
DEBUG:root:[ Iteration 99 ] Training loss: 0.92
DEBUG:root:[ Iteration 100 ] Test loss: 0.895
DEBUG:root:[ Iteration 102 ] Training loss: 0.9325
DEBUG:root:[ Iteration 105 ] Training loss: 0.9375
DEBUG:root:[ Iteration 108 ] Training loss: 0.9625
DEBUG:root:[ Iteration 111 ] Training loss: 0.9425
DEBUG:root:[ Iteration 114 ] Training loss: 0.94
DEBUG:root:[ Iteration 117 ] Training loss: 0.945
DEBUG:root:[ Iteration 120 ] Training loss: 0.945
DEBUG:root:[ Iteration 120 ] Test loss: 0.91
DEBUG:root:[ Iteration 123 ] Training loss: 0.9625
DEBUG:root:[ Iteration 126 ] Training loss: 0.945
DEBUG:root:[ Iteration 129 ] Training loss: 0.965
DEBUG:root:[ Iteration 132 ] Training loss: 0.9675
DEBUG:root:[ Iteration 135 ] Training loss: 0.95
DEBUG:root:[ Iteration 138 ] Training loss: 0.9575
DEBUG:root:[ Iteration 140 ] Test loss: 0.8775
DEBUG:root:[ Iteration 141 ] Training loss: 0.9675
DEBUG:root:[ Iteration 144 ] Training loss: 0.95
DEBUG:root:[ Iteration 147 ] Training loss: 0.955
DEBUG:root:[ Iteration 150 ] Training loss: 0.9525
DEBUG:root:[ Iteration 153 ] Training loss: 0.9775
DEBUG:root:[ Iteration 156 ] Training loss: 0.955
DEBUG:root:[ Iteration 159 ] Training loss: 0.9675
DEBUG:root:[ Iteration 160 ] Test loss: 0.8725
DEBUG:root:[ Iteration 162 ] Training loss: 0.9425
DEBUG:root:[ Iteration 165 ] Training loss: 0.9675
DEBUG:root:[ Iteration 168 ] Training loss: 0.95
DEBUG:root:[ Iteration 171 ] Training loss: 0.9575
DEBUG:root:[ Iteration 174 ] Training loss: 0.97
DEBUG:root:[ Iteration 177 ] Training loss: 0.96
DEBUG:root:[ Iteration 180 ] Training loss: 0.9575
DEBUG:root:[ Iteration 180 ] Test loss: 0.8975
DEBUG:root:[ Iteration 183 ] Training loss: 0.9575
DEBUG:root:[ Iteration 186 ] Training loss: 0.955
DEBUG:root:[ Iteration 189 ] Training loss: 0.98
DEBUG:root:[ Iteration 192 ] Training loss: 0.97
DEBUG:root:[ Iteration 195 ] Training loss: 0.965
DEBUG:root:[ Iteration 198 ] Training loss: 0.9675
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-21-2016_12h05m34s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.9775
DEBUG:root:[ Iteration 0 ] Test loss: 0.88
DEBUG:root:[ Iteration 3 ] Training loss: 0.98
DEBUG:root:[ Iteration 6 ] Training loss: 0.9625
DEBUG:root:[ Iteration 9 ] Training loss: 0.9725
DEBUG:root:[ Iteration 12 ] Training loss: 0.9725
DEBUG:root:[ Iteration 15 ] Training loss: 0.9675
DEBUG:root:[ Iteration 18 ] Training loss: 0.9725
DEBUG:root:[ Iteration 20 ] Test loss: 0.875
DEBUG:root:[ Iteration 21 ] Training loss: 0.9625
DEBUG:root:[ Iteration 24 ] Training loss: 0.95
DEBUG:root:[ Iteration 27 ] Training loss: 0.965
DEBUG:root:[ Iteration 30 ] Training loss: 0.95
DEBUG:root:[ Iteration 33 ] Training loss: 0.9725
DEBUG:root:[ Iteration 36 ] Training loss: 0.9775
DEBUG:root:[ Iteration 39 ] Training loss: 0.955
DEBUG:root:[ Iteration 40 ] Test loss: 0.8725
DEBUG:root:[ Iteration 42 ] Training loss: 0.94
DEBUG:root:[ Iteration 45 ] Training loss: 0.9525
DEBUG:root:[ Iteration 48 ] Training loss: 0.9525
DEBUG:root:[ Iteration 51 ] Training loss: 0.9475
DEBUG:root:[ Iteration 54 ] Training loss: 0.9675
DEBUG:root:[ Iteration 57 ] Training loss: 0.975
DEBUG:root:[ Iteration 60 ] Training loss: 0.9525
DEBUG:root:[ Iteration 60 ] Test loss: 0.9075
DEBUG:root:[ Iteration 63 ] Training loss: 0.9625
DEBUG:root:[ Iteration 66 ] Training loss: 0.955
DEBUG:root:[ Iteration 69 ] Training loss: 0.98
DEBUG:root:[ Iteration 72 ] Training loss: 0.9675
DEBUG:root:[ Iteration 75 ] Training loss: 0.9775
DEBUG:root:[ Iteration 78 ] Training loss: 0.9775
DEBUG:root:[ Iteration 80 ] Test loss: 0.935
DEBUG:root:[ Iteration 81 ] Training loss: 0.9575
DEBUG:root:[ Iteration 84 ] Training loss: 0.9575
DEBUG:root:[ Iteration 87 ] Training loss: 0.9725
DEBUG:root:[ Iteration 90 ] Training loss: 0.9775
DEBUG:root:[ Iteration 93 ] Training loss: 0.9775
DEBUG:root:[ Iteration 96 ] Training loss: 0.9825
DEBUG:root:[ Iteration 99 ] Training loss: 0.96
DEBUG:root:[ Iteration 100 ] Test loss: 0.8925
DEBUG:root:[ Iteration 102 ] Training loss: 0.9825
DEBUG:root:[ Iteration 105 ] Training loss: 0.985
DEBUG:root:[ Iteration 108 ] Training loss: 0.9825
DEBUG:root:[ Iteration 111 ] Training loss: 0.9625
DEBUG:root:[ Iteration 114 ] Training loss: 0.965
DEBUG:root:[ Iteration 117 ] Training loss: 0.9625
DEBUG:root:[ Iteration 120 ] Training loss: 0.975
DEBUG:root:[ Iteration 120 ] Test loss: 0.8925
DEBUG:root:[ Iteration 123 ] Training loss: 0.97
DEBUG:root:[ Iteration 126 ] Training loss: 0.975
DEBUG:root:[ Iteration 129 ] Training loss: 0.955
DEBUG:root:[ Iteration 132 ] Training loss: 0.9825
DEBUG:root:[ Iteration 135 ] Training loss: 0.965
DEBUG:root:[ Iteration 138 ] Training loss: 0.975
DEBUG:root:[ Iteration 140 ] Test loss: 0.895
DEBUG:root:[ Iteration 141 ] Training loss: 0.9725
DEBUG:root:[ Iteration 144 ] Training loss: 0.99
DEBUG:root:[ Iteration 147 ] Training loss: 0.97
DEBUG:root:[ Iteration 150 ] Training loss: 0.97
DEBUG:root:[ Iteration 153 ] Training loss: 0.985
DEBUG:root:[ Iteration 156 ] Training loss: 0.985
DEBUG:root:[ Iteration 159 ] Training loss: 0.9875
DEBUG:root:[ Iteration 160 ] Test loss: 0.89
DEBUG:root:[ Iteration 162 ] Training loss: 0.965
DEBUG:root:[ Iteration 165 ] Training loss: 0.9575
DEBUG:root:[ Iteration 168 ] Training loss: 0.9775
DEBUG:root:[ Iteration 171 ] Training loss: 0.9925
DEBUG:root:[ Iteration 174 ] Training loss: 0.96
DEBUG:root:[ Iteration 177 ] Training loss: 0.985
DEBUG:root:[ Iteration 180 ] Training loss: 0.9775
DEBUG:root:[ Iteration 180 ] Test loss: 0.8925
DEBUG:root:[ Iteration 183 ] Training loss: 0.9775
DEBUG:root:[ Iteration 186 ] Training loss: 0.9725
DEBUG:root:[ Iteration 189 ] Training loss: 0.9775
DEBUG:root:[ Iteration 192 ] Training loss: 0.98
DEBUG:root:[ Iteration 195 ] Training loss: 0.9825
DEBUG:root:[ Iteration 198 ] Training loss: 0.99
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-21-2016_12h13m51s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training loss: 0.215
DEBUG:root:[ Iteration 0 ] Test loss: 0.215
DEBUG:root:[ Iteration 3 ] Training loss: 0.5575
DEBUG:root:[ Iteration 6 ] Training loss: 0.5825
DEBUG:root:[ Iteration 9 ] Training loss: 0.5675
DEBUG:root:[ Iteration 12 ] Training loss: 0.58
DEBUG:root:[ Iteration 15 ] Training loss: 0.58
DEBUG:root:[ Iteration 18 ] Training loss: 0.5625
DEBUG:root:[ Iteration 20 ] Test loss: 0.6075
DEBUG:root:[ Iteration 21 ] Training loss: 0.5575
DEBUG:root:[ Iteration 24 ] Training loss: 0.62
DEBUG:root:[ Iteration 27 ] Training loss: 0.6375
DEBUG:root:[ Iteration 30 ] Training loss: 0.7025
DEBUG:root:[ Iteration 33 ] Training loss: 0.68
DEBUG:root:[ Iteration 36 ] Training loss: 0.6375
DEBUG:root:[ Iteration 39 ] Training loss: 0.6325
DEBUG:root:[ Iteration 40 ] Test loss: 0.655
DEBUG:root:[ Iteration 42 ] Training loss: 0.5925
DEBUG:root:[ Iteration 45 ] Training loss: 0.6625
DEBUG:root:[ Iteration 48 ] Training loss: 0.68
DEBUG:root:[ Iteration 51 ] Training loss: 0.6675
DEBUG:root:[ Iteration 54 ] Training loss: 0.6775
DEBUG:root:[ Iteration 57 ] Training loss: 0.6825
DEBUG:root:[ Iteration 60 ] Training loss: 0.7025
DEBUG:root:[ Iteration 60 ] Test loss: 0.7275
DEBUG:root:[ Iteration 63 ] Training loss: 0.6775
DEBUG:root:[ Iteration 66 ] Training loss: 0.6725
DEBUG:root:[ Iteration 69 ] Training loss: 0.68
DEBUG:root:[ Iteration 72 ] Training loss: 0.71
DEBUG:root:[ Iteration 75 ] Training loss: 0.7275
DEBUG:root:[ Iteration 78 ] Training loss: 0.725
DEBUG:root:[ Iteration 80 ] Test loss: 0.7225
DEBUG:root:[ Iteration 81 ] Training loss: 0.75
DEBUG:root:[ Iteration 84 ] Training loss: 0.735
DEBUG:root:[ Iteration 87 ] Training loss: 0.7425
DEBUG:root:[ Iteration 90 ] Training loss: 0.74
DEBUG:root:[ Iteration 93 ] Training loss: 0.7125
DEBUG:root:[ Iteration 96 ] Training loss: 0.7575
DEBUG:root:[ Iteration 99 ] Training loss: 0.7625
DEBUG:root:[ Iteration 100 ] Test loss: 0.73
DEBUG:root:[ Iteration 102 ] Training loss: 0.7475
DEBUG:root:[ Iteration 105 ] Training loss: 0.74
DEBUG:root:[ Iteration 108 ] Training loss: 0.76
DEBUG:root:[ Iteration 111 ] Training loss: 0.7675
DEBUG:root:[ Iteration 114 ] Training loss: 0.79
DEBUG:root:[ Iteration 117 ] Training loss: 0.735
DEBUG:root:[ Iteration 120 ] Training loss: 0.775
DEBUG:root:[ Iteration 120 ] Test loss: 0.7375
DEBUG:root:[ Iteration 123 ] Training loss: 0.79
DEBUG:root:[ Iteration 126 ] Training loss: 0.7725
DEBUG:root:[ Iteration 129 ] Training loss: 0.8025
DEBUG:root:[ Iteration 132 ] Training loss: 0.805
DEBUG:root:[ Iteration 135 ] Training loss: 0.805
DEBUG:root:[ Iteration 138 ] Training loss: 0.8175
DEBUG:root:[ Iteration 140 ] Test loss: 0.7775
DEBUG:root:[ Iteration 141 ] Training loss: 0.8075
DEBUG:root:[ Iteration 144 ] Training loss: 0.84
DEBUG:root:[ Iteration 147 ] Training loss: 0.8075
DEBUG:root:[ Iteration 150 ] Training loss: 0.8275
DEBUG:root:[ Iteration 153 ] Training loss: 0.8275
DEBUG:root:[ Iteration 156 ] Training loss: 0.835
DEBUG:root:[ Iteration 159 ] Training loss: 0.84
DEBUG:root:[ Iteration 160 ] Test loss: 0.8025
DEBUG:root:[ Iteration 162 ] Training loss: 0.8125
DEBUG:root:[ Iteration 165 ] Training loss: 0.8575
DEBUG:root:[ Iteration 168 ] Training loss: 0.845
DEBUG:root:[ Iteration 171 ] Training loss: 0.84
DEBUG:root:[ Iteration 174 ] Training loss: 0.825
DEBUG:root:[ Iteration 177 ] Training loss: 0.8275
DEBUG:root:[ Iteration 180 ] Training loss: 0.8525
DEBUG:root:[ Iteration 180 ] Test loss: 0.8225
DEBUG:root:[ Iteration 183 ] Training loss: 0.8825
DEBUG:root:[ Iteration 186 ] Training loss: 0.8575
DEBUG:root:[ Iteration 189 ] Training loss: 0.85
DEBUG:root:[ Iteration 192 ] Training loss: 0.885
DEBUG:root:[ Iteration 195 ] Training loss: 0.8675
DEBUG:root:[ Iteration 198 ] Training loss: 0.8325
DEBUG:root:[ Iteration 200 ] Test loss: 0.805
DEBUG:root:[ Iteration 201 ] Training loss: 0.89
DEBUG:root:[ Iteration 204 ] Training loss: 0.8575
DEBUG:root:[ Iteration 207 ] Training loss: 0.86
DEBUG:root:[ Iteration 210 ] Training loss: 0.845
DEBUG:root:[ Iteration 213 ] Training loss: 0.91
DEBUG:root:[ Iteration 216 ] Training loss: 0.865
DEBUG:root:[ Iteration 219 ] Training loss: 0.8675
DEBUG:root:[ Iteration 220 ] Test loss: 0.825
DEBUG:root:[ Iteration 222 ] Training loss: 0.8925
DEBUG:root:[ Iteration 225 ] Training loss: 0.9025
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_c_06-21-2016_12h24m19s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 1328.44  Training rotation loss: 22507.2
DEBUG:root:[ Iteration 0 ] Training rotation loss: 1351.7  Training rotation loss: 21630.9
DEBUG:root:[ Iteration 3 ] Training rotation loss: 1304.84  Training rotation loss: 21551.2
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-21-2016_12h55m42s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.000221905  Training rotation loss: 3.10815e-05
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.000212298  Training rotation loss: 3.14832e-05
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.000214546  Training rotation loss: 3.15661e-05
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.000214744  Training rotation loss: 3.00667e-05
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.000243955  Training rotation loss: 2.93772e-05
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.000209449  Training rotation loss: 2.96408e-05
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.000208738  Training rotation loss: 2.77018e-05
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.000235624  Training rotation loss: 2.70377e-05
DEBUG:root:[ Iteration 20 ] Training rotation loss: 0.000222162  Training rotation loss: 2.47768e-05
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.000204536  Training rotation loss: 2.51998e-05
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.000209846  Training rotation loss: 2.39702e-05
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.000186378  Training rotation loss: 2.23244e-05
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.00019988  Training rotation loss: 2.02644e-05
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.000223425  Training rotation loss: 1.70539e-05
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.000187873  Training rotation loss: 1.21379e-05
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.000161813  Training rotation loss: 6.36504e-06
DEBUG:root:[ Iteration 40 ] Training rotation loss: 0.000147274  Training rotation loss: 5.11876e-06
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.000122761  Training rotation loss: 3.02709e-06
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.000162613  Training rotation loss: 2.07547e-06
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.000170949  Training rotation loss: 2.31909e-06
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.000118065  Training rotation loss: 2.90632e-06
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.000137504  Training rotation loss: 2.86024e-06
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.000145982  Training rotation loss: 2.7397e-06
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.000116148  Training rotation loss: 2.39824e-06
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00013361  Training rotation loss: 2.79892e-06
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.000133874  Training rotation loss: 2.63167e-06
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00011734  Training rotation loss: 3.04777e-06
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.000125338  Training rotation loss: 3.15861e-06
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.000117623  Training rotation loss: 2.77498e-06
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.000131554  Training rotation loss: 2.1402e-06
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.000113372  Training rotation loss: 2.55849e-06
DEBUG:root:[ Iteration 80 ] Training rotation loss: 0.000106305  Training rotation loss: 2.6486e-06
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.000104028  Training rotation loss: 3.10595e-06
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.000133176  Training rotation loss: 2.81527e-06
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.0001043  Training rotation loss: 2.51251e-06
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.000107672  Training rotation loss: 2.16212e-06
DEBUG:root:[ Iteration 93 ] Training rotation loss: 9.98405e-05  Training rotation loss: 2.52742e-06
DEBUG:root:[ Iteration 96 ] Training rotation loss: 9.74924e-05  Training rotation loss: 3.02301e-06
DEBUG:root:[ Iteration 99 ] Training rotation loss: 9.25082e-05  Training rotation loss: 2.32642e-06
DEBUG:root:[ Iteration 100 ] Training rotation loss: 8.76594e-05  Training rotation loss: 2.70649e-06
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.000100906  Training rotation loss: 2.15417e-06
DEBUG:root:[ Iteration 105 ] Training rotation loss: 8.52481e-05  Training rotation loss: 2.58712e-06
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00011106  Training rotation loss: 2.61938e-06
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.000101422  Training rotation loss: 3.30055e-06
DEBUG:root:[ Iteration 114 ] Training rotation loss: 7.77292e-05  Training rotation loss: 3.16483e-06
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.000117664  Training rotation loss: 2.66768e-06
DEBUG:root:[ Iteration 120 ] Training rotation loss: 8.92149e-05  Training rotation loss: 2.12296e-06
DEBUG:root:[ Iteration 120 ] Training rotation loss: 9.97477e-05  Training rotation loss: 1.92802e-06
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00010385  Training rotation loss: 2.73655e-06
DEBUG:root:[ Iteration 126 ] Training rotation loss: 7.12044e-05  Training rotation loss: 3.89383e-06
DEBUG:root:[ Iteration 129 ] Training rotation loss: 9.55318e-05  Training rotation loss: 2.47522e-06
DEBUG:root:[ Iteration 132 ] Training rotation loss: 8.9653e-05  Training rotation loss: 3.23647e-06
DEBUG:root:[ Iteration 135 ] Training rotation loss: 7.95196e-05  Training rotation loss: 2.75415e-06
DEBUG:root:[ Iteration 138 ] Training rotation loss: 7.81468e-05  Training rotation loss: 2.25978e-06
DEBUG:root:[ Iteration 140 ] Training rotation loss: 8.63536e-05  Training rotation loss: 2.60558e-06
DEBUG:root:[ Iteration 141 ] Training rotation loss: 6.82468e-05  Training rotation loss: 3.05838e-06
DEBUG:root:[ Iteration 144 ] Training rotation loss: 8.17253e-05  Training rotation loss: 2.41698e-06
DEBUG:root:[ Iteration 147 ] Training rotation loss: 8.55815e-05  Training rotation loss: 3.13624e-06
DEBUG:root:[ Iteration 150 ] Training rotation loss: 7.63095e-05  Training rotation loss: 2.45134e-06
DEBUG:root:[ Iteration 153 ] Training rotation loss: 7.42075e-05  Training rotation loss: 2.46565e-06
DEBUG:root:[ Iteration 156 ] Training rotation loss: 8.56026e-05  Training rotation loss: 2.40416e-06
DEBUG:root:[ Iteration 159 ] Training rotation loss: 6.74307e-05  Training rotation loss: 3.5979e-06
DEBUG:root:[ Iteration 160 ] Training rotation loss: 8.99045e-05  Training rotation loss: 2.71048e-06
DEBUG:root:[ Iteration 162 ] Training rotation loss: 6.60034e-05  Training rotation loss: 2.91553e-06
DEBUG:root:[ Iteration 165 ] Training rotation loss: 6.5656e-05  Training rotation loss: 2.44288e-06
DEBUG:root:[ Iteration 168 ] Training rotation loss: 6.77697e-05  Training rotation loss: 1.69364e-06
DEBUG:root:[ Iteration 171 ] Training rotation loss: 8.4499e-05  Training rotation loss: 2.72961e-06
DEBUG:root:[ Iteration 174 ] Training rotation loss: 7.2678e-05  Training rotation loss: 2.60415e-06
DEBUG:root:[ Iteration 177 ] Training rotation loss: 7.69604e-05  Training rotation loss: 2.62067e-06
DEBUG:root:[ Iteration 180 ] Training rotation loss: 6.24022e-05  Training rotation loss: 2.89611e-06
DEBUG:root:[ Iteration 180 ] Training rotation loss: 7.2458e-05  Training rotation loss: 2.42763e-06
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-21-2016_13h02m22s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: [ -4.24730359e-04   9.84456837e-02   4.92171459e-02  -4.92739342e-02
   9.80271846e-02   4.91307341e-02   9.85004082e-02  -7.34709538e-05
  -2.84401176e-04  -4.93105277e-02   9.81675982e-02   4.86310050e-02
  -9.86350849e-02  -8.53260062e-05   9.81280804e-02  -1.69812251e-04
   4.90315184e-02   9.82878730e-02  -2.84662587e-04   4.89321612e-02
   4.92420979e-02  -1.99701462e-04   9.84337628e-02  -4.94251668e-04
  -9.86340046e-02  -6.99228549e-05  -4.88094578e-04   9.81073305e-02
   9.81920734e-02   9.81200412e-02   9.80759412e-02  -4.94229645e-02
   9.79019776e-02  -3.38137877e-04  -2.16480155e-04  -1.04956998e-04
  -9.84157175e-02  -9.84542444e-02  -9.87573788e-02   9.83372554e-02
   9.81217101e-02  -1.47796090e-04   9.82360095e-02   9.83414501e-02
  -9.85723138e-02  -8.07780889e-05   4.88536842e-02  -3.30804934e-04
  -1.30193468e-04   4.92712259e-02   3.64414996e-06   4.90568392e-02
   9.83300656e-02  -3.12388205e-04   4.92375791e-02   9.79190171e-02
  -9.86183062e-02   9.79925320e-02   4.87008356e-02  -1.42528341e-04
  -4.91564050e-02  -9.85423774e-02  -4.29079402e-04   4.88985516e-02
  -9.85352322e-02   2.00773648e-04  -4.95143197e-02   9.82846320e-02
   9.78552848e-02   4.91580293e-02  -2.71436933e-04  -4.23316582e-04
  -4.12728987e-04  -9.88642275e-02   5.75012891e-05   9.81035307e-02
   4.86646332e-02  -4.94248196e-02   4.87957634e-02   9.81755033e-02
   9.82504711e-02  -3.39978898e-04   9.84126255e-02  -9.87273306e-02
   9.83754247e-02   9.82310250e-02  -9.87440869e-02  -6.92739850e-05
  -2.96611979e-04   9.83246565e-02   4.90396358e-02  -3.27810667e-05
  -9.87096429e-02  -1.43077166e-04   9.83632952e-02   9.80728120e-02
  -4.94096726e-02   9.80153009e-02   4.90069948e-02   1.34652088e-04
  -5.52683603e-04   3.60702506e-06   9.83414277e-02   9.81559232e-02
  -2.78146123e-04   4.92956601e-02   9.79777947e-02   9.82901752e-02
  -1.32513145e-04  -6.04506815e-04   4.86107580e-02   6.23276865e-05
   9.84580442e-02  -1.65160891e-04  -2.77197163e-04   9.80669037e-02
   9.80775729e-02  -4.95066680e-02   9.80837569e-02  -9.86949801e-02
   9.83423889e-02  -9.88796726e-02  -2.81449291e-04  -9.88347903e-02
  -3.87768028e-04  -9.86523107e-02   4.91229333e-02  -5.70122829e-05
  -1.47258907e-04   4.88324352e-02   9.81148630e-02   9.82512981e-02
  -1.30674613e-04   9.85509455e-02   9.82679650e-02   9.86326337e-02
   9.79849622e-02  -4.95883599e-02  -2.55402294e-04   9.79985744e-02
  -1.85105600e-04   9.78177413e-02  -3.91355075e-04  -8.35326064e-05
   4.88152727e-02   9.81919467e-02   4.90384549e-02   4.89529595e-02
   9.79464427e-02   9.82584655e-02  -7.16040813e-05  -1.92391635e-05
   9.81293544e-02  -2.37136075e-04  -1.91516869e-04  -9.88077447e-02
  -9.85426307e-02   9.79260653e-02   9.82717946e-02   9.83237177e-02
  -7.11356770e-05   9.84065160e-02  -1.48277628e-04   9.83498245e-02
   9.78424326e-02   9.82509777e-02   8.24008675e-05   4.87020425e-02
   4.87281345e-02   9.83158201e-02   9.80957374e-02   9.83859673e-02
   9.85130221e-02  -2.68461969e-04   4.91038598e-02  -4.93202358e-02
   9.83492509e-02  -9.87419188e-02  -3.52946750e-04  -1.00863617e-04
  -7.10433160e-05   4.88712005e-02  -9.84669328e-02   9.77694467e-02
   9.78964791e-02  -4.93345149e-02   9.79798660e-02  -5.03058953e-04
  -6.03154418e-04  -3.32904921e-04   9.81594399e-02   4.90312725e-02
  -4.94688898e-02   9.83890668e-02   9.86571684e-02   9.85794365e-02
   4.87537272e-02  -2.85666116e-04   9.83280614e-02  -1.99084869e-04]  Training rotation loss: [ -7.70026296e-02  -7.69919828e-02  -7.69158304e-02  -7.71764740e-02
  -5.81912473e-02  -7.71960467e-02  -7.70919994e-02  -7.69717097e-02
  -7.72412196e-02  -7.70597309e-02  -7.71225691e-02  -7.70313367e-02
  -7.70229474e-02  -7.70298839e-02  -7.71426260e-02  -7.69053325e-02
  -7.69302100e-02  -3.84792350e-02  -7.71159828e-02  -7.71589950e-02
  -7.71079808e-02  -7.70813078e-02  -7.71165863e-02  -7.70405978e-02
  -7.69819394e-02  -7.71003217e-02  -7.70912990e-02  -1.92655455e-02
  -3.84763964e-02  -7.71822333e-02  -7.70780072e-02  -7.70180672e-02
  -7.71075264e-02  -7.70759583e-02  -7.70629197e-02  -7.69728348e-02
  -7.70214498e-02  -7.70536736e-02  -7.71673620e-02  -7.69205391e-02
  -5.78161553e-02  -7.69443288e-02  -7.71680102e-02  -5.77956624e-02
  -9.69862085e-05  -7.70282894e-02  -7.71460310e-02  -7.68577382e-02
  -7.70690516e-02  -7.73576647e-02  -7.72522837e-02  -7.69886002e-02
  -5.80201522e-02  -7.72228613e-02  -7.72237256e-02  -7.70646110e-02
  -1.43566736e-04  -2.14746600e-04  -7.71074370e-02  -7.69890100e-02
  -7.70159885e-02  -1.68037950e-04  -7.70929083e-02  -7.72375017e-02
  -7.71057457e-02  -7.70115331e-02  -7.72629604e-02  -5.82221672e-02
  -7.71511719e-02  -7.72382915e-02  -7.68489391e-02  -7.72822574e-02
  -7.72313252e-02  -3.86267416e-02  -7.70416781e-02  -3.85105051e-02
  -7.70495757e-02  -7.71793053e-02  -7.71267042e-02  -3.87600809e-02
  -7.72062168e-02  -7.71538392e-02  -3.86838019e-02  -7.69977197e-02
  -7.71034434e-02  -7.72428364e-02  -5.77218346e-02  -7.71492943e-02
  -7.71959797e-02  -3.85976247e-02  -7.71069527e-02  -7.71586597e-02
  -7.70528242e-02  -5.77454381e-02  -7.72572011e-02  -5.78570999e-02
  -7.70358741e-02  -3.89039777e-02  -7.72351772e-02  -7.70226568e-02
  -7.71044791e-02  -7.72547871e-02  -5.80992512e-02  -5.80924377e-02
  -7.69622847e-02  -7.71265402e-02  -2.43757386e-04  -7.71574080e-02
  -7.69487247e-02  -7.71921054e-02  -7.71461502e-02  -7.70415440e-02
  -5.77076375e-02  -7.69360512e-02  -7.71033391e-02  -7.74450675e-02
  -7.70893618e-02  -7.70413429e-02  -3.88378017e-02  -7.70751461e-02
  -3.84885781e-02  -5.80633059e-02  -7.71666095e-02  -7.73355216e-02
  -7.72442669e-02  -7.70670325e-02  -7.70586878e-02  -7.70866498e-02
  -7.71255866e-02  -7.71113187e-02  -5.80361262e-02  -5.80714643e-02
  -7.70782158e-02  -3.88142355e-02  -7.70888627e-02  -7.71449208e-02
  -5.78795858e-02  -7.70298466e-02  -7.69911557e-02  -5.79173937e-02
  -7.72142485e-02  -5.77981658e-02  -7.69929811e-02  -7.70070180e-02
  -7.72074312e-02  -7.73957595e-02  -7.71926790e-02  -7.70930424e-02
  -5.78366518e-02  -1.97358783e-02  -7.71250203e-02  -7.69185275e-02
  -3.86723094e-02  -7.70729557e-02  -7.70125464e-02  -7.72188976e-02
  -1.37570838e-04  -1.92921907e-02  -5.77330776e-02  -7.70490468e-02
  -7.71155283e-02  -7.73896575e-02  -7.69844130e-02  -3.85361835e-02
  -1.92642994e-02  -7.73449615e-02  -7.68501386e-02  -7.75321722e-02
  -7.73487091e-02  -7.73176998e-02  -7.72214457e-02  -3.85479853e-02
  -7.71463960e-02  -7.69726485e-02  -7.73392394e-02  -7.72632509e-02
  -3.83025222e-02  -5.79852201e-02  -7.70672858e-02  -7.70194679e-02
  -7.71513730e-02  -7.72784725e-02  -7.70519599e-02  -5.77650331e-02
  -1.92925427e-02  -7.69880861e-02  -3.85895967e-02  -7.71098435e-02
  -7.71155953e-02  -7.71520808e-02  -7.70851448e-02  -7.72306472e-02
  -7.71903917e-02  -7.68986866e-02  -7.69859701e-02  -7.70206973e-02
  -7.71083534e-02  -7.69746751e-02  -7.72754401e-02  -7.71996230e-02]
DEBUG:root:[ Iteration 0 ] Training rotation loss: [  9.54312459e-02   9.52003300e-02   9.53532234e-02  -4.79900129e-02
   9.57908779e-02   9.59097221e-02   4.76396941e-02   9.55776125e-02
   4.76053804e-02  -4.82924245e-02   4.54268229e-06  -2.59685359e-04
   4.72899489e-02   9.52882767e-02   9.55981761e-02  -1.95385801e-04
  -9.60796997e-02  -2.77450978e-04  -1.16882853e-04   9.56899747e-02
  -3.08906601e-04  -2.23291776e-04   9.55638438e-02  -3.41944164e-04
  -1.62246833e-05  -8.51129298e-05   4.79542688e-02  -9.60669443e-02
   9.54467431e-02   4.72962074e-02   9.55582857e-02   6.49193971e-05
   9.59535614e-02   9.56017524e-02  -2.23762821e-04   9.57615152e-02
  -9.61241275e-02  -9.60146114e-02   9.57022458e-02  -1.62352051e-04
   4.74382378e-02   4.77706157e-02   9.54964310e-02  -3.25590139e-04
  -4.83209005e-04   9.56198499e-02   9.53397155e-02  -6.38112178e-05
   9.56220850e-02  -4.80884276e-02   4.77169640e-02   9.54953581e-02
  -9.60539430e-02   9.60538834e-02   9.53997895e-02   9.56522897e-02
  -4.66227066e-04   4.75440063e-02   4.79873456e-02  -9.58897173e-02
  -3.09300347e-04   9.59468707e-02   9.54009965e-02  -1.00838392e-04
   9.56197903e-02   9.56817716e-02  -4.13553120e-04   9.54719409e-02
   9.53236222e-02   4.74156663e-02   9.57531258e-02   9.53088552e-02
   9.55966190e-02   9.55524743e-02   9.58771631e-02  -9.62059051e-02
  -1.96777008e-04  -4.57226037e-04   9.56302211e-02  -9.60498154e-02
  -4.76849120e-04   9.52957273e-02   4.78857830e-02   4.78235148e-02
  -1.85526034e-04   9.57458764e-02  -2.04530108e-04   9.57693979e-02
   2.12296745e-05   9.56813097e-02   9.56480652e-02  -2.87363044e-04
  -4.59033734e-04  -3.58502526e-04   9.59872082e-02  -2.25787400e-04
  -2.85481510e-04  -1.16569325e-04  -3.05995985e-04  -2.77474552e-04
   4.80260104e-02  -1.46757666e-04  -3.86751744e-05  -4.85366508e-02
  -4.49275511e-04   9.57117826e-02   4.71095294e-02  -3.16963764e-04
   9.58641395e-02  -9.58648548e-02  -2.95779231e-04  -2.18138157e-04
   9.56762880e-02  -9.09978990e-05  -4.06310864e-04   9.56486538e-02
  -2.81712943e-04   9.54902992e-02  -1.27532039e-04   9.54820514e-02
  -5.64865113e-05   4.77203578e-02   9.58351120e-02   9.53665897e-02
  -2.67563533e-04   4.77572270e-02   9.54765007e-02   9.57649052e-02
  -9.59909186e-02  -1.45424463e-04  -1.90255480e-04  -9.59330946e-02
   1.34977425e-04   9.59559232e-02   9.51336548e-02  -4.81499843e-02
   4.75413911e-02   9.57174599e-02  -9.60501060e-02   9.57358927e-02
   9.58745554e-02   4.81606796e-02   4.76752594e-02   9.58281532e-02
  -2.77025712e-04   9.56376046e-02   9.56992805e-02   9.59149748e-02
  -5.74033693e-05  -4.81816344e-02   9.53482166e-02   9.53876153e-02
  -9.55710262e-02  -1.86132092e-05   9.57755819e-02  -5.93999517e-04
   9.57555398e-02  -5.22933085e-04  -2.94282159e-04   9.57551748e-02
   4.75956164e-02  -2.88530631e-04   9.56633762e-02   4.79822755e-02
   4.76478189e-02  -9.60725769e-02   4.78278063e-02   9.57796797e-02
   9.54227522e-02   4.73944880e-02   9.57787409e-02   9.52331349e-02
   9.54086185e-02  -1.76172063e-04   9.58075598e-02  -9.60471407e-02
  -9.61524695e-02  -3.97970231e-04  -1.94073582e-04   9.58915278e-02
   9.53932554e-02   9.55249295e-02   9.54900831e-02   9.53473970e-02
   8.25339885e-06  -9.59691033e-02  -2.33224113e-04   9.59070697e-02
  -1.66003869e-04  -2.84771930e-04   4.78128977e-02   9.57330167e-02
   9.53428298e-02  -1.99515096e-04  -2.91438482e-04  -9.59405452e-02
  -4.41898446e-05  -9.57441628e-02   9.53219160e-02   4.73323762e-02]  Training rotation loss: [ -7.82955438e-02  -5.88145219e-02  -7.86726549e-02  -7.83481151e-02
  -5.89035451e-02  -7.83846602e-02  -7.85781369e-02  -7.84399286e-02
  -7.85567984e-02  -7.83715099e-02  -7.82068968e-02  -7.84580633e-02
  -7.88410753e-02  -7.85701051e-02  -7.85159394e-02  -7.82423690e-02
  -7.85059705e-02  -7.84439519e-02  -7.82884657e-02  -7.88509399e-02
  -7.83777907e-02  -7.83706903e-02  -3.92084122e-02  -7.85885602e-02
  -7.83074796e-02  -7.85100907e-02  -7.83938244e-02  -7.85994530e-02
  -8.31982834e-05  -7.85664171e-02  -7.85078853e-02  -7.85107911e-02
  -7.83543736e-02  -3.92020680e-02  -7.83213004e-02  -7.84502104e-02
  -7.84418806e-02  -1.28759872e-04  -7.85357431e-02  -7.85171613e-02
  -7.87650123e-02  -7.85720944e-02  -5.87852187e-02  -7.83919096e-02
  -7.85416365e-02  -3.92609835e-02  -3.93130034e-02  -7.84403831e-02
  -7.83061683e-02  -7.83336684e-02  -7.82907531e-02  -3.92848812e-02
  -7.82489255e-02  -7.82963932e-02  -5.91735579e-02  -1.97464544e-02
  -7.83865675e-02  -7.85781443e-02  -7.83336610e-02  -5.86823151e-02
  -7.84729272e-02  -7.84217939e-02  -3.96145917e-02  -7.84431472e-02
  -1.96306910e-02  -3.93216535e-02  -7.84307420e-02  -7.84574449e-02
  -2.17966139e-04  -7.86173791e-02  -2.00576931e-02  -7.84266144e-02
  -7.84102380e-02  -5.88895157e-02  -5.87039739e-02  -7.85779655e-02
  -7.83728808e-02  -7.85025656e-02  -3.92362140e-02  -7.84264430e-02
  -7.86103681e-02  -7.87127763e-02  -7.86681548e-02  -7.86177292e-02
  -7.84284770e-02  -7.84022361e-02  -7.83429518e-02  -1.98734943e-02
  -7.86174387e-02  -7.88099021e-02  -5.87366447e-02  -7.83445090e-02
  -7.83959702e-02  -7.85168335e-02  -5.87652959e-02  -7.83078447e-02
  -7.83840865e-02  -7.84219727e-02  -7.82776400e-02  -7.87175894e-02
  -7.83823803e-02  -7.83703178e-02  -7.84263089e-02  -7.84328729e-02
  -7.86053762e-02  -1.95163433e-02  -7.88610429e-02  -7.84014016e-02
  -3.92197371e-02  -7.82887191e-02  -7.84965381e-02  -7.83783346e-02
  -5.89248016e-02  -7.83406347e-02  -7.86268041e-02  -5.91048375e-02
  -7.83171654e-02  -7.83834755e-02  -7.84541816e-02  -1.96337979e-02
  -7.83162788e-02  -7.83674791e-02  -7.84946978e-02  -7.87507519e-02
  -7.86135122e-02  -7.84636587e-02  -5.87616116e-02  -5.87812662e-02
  -7.83694983e-02  -7.85261095e-02  -7.83981830e-02   4.05493593e-05
  -7.85224512e-02  -7.83703923e-02  -8.72721139e-05  -7.84495324e-02
  -7.88525790e-02  -3.92793417e-02  -3.91774066e-02  -5.88791482e-02
  -7.83217922e-02  -7.83491358e-02  -7.85149410e-02  -3.93641330e-02
  -7.84848332e-02  -7.87348151e-02  -5.88974282e-02  -7.84272328e-02
  -7.83849955e-02  -7.82942027e-02  -5.88523783e-02  -3.94866876e-02
  -7.86049590e-02  -7.84454346e-02  -1.95440054e-02  -7.84229562e-02
  -3.91415395e-02  -7.85502270e-02  -7.84424394e-02  -7.85755515e-02
  -7.88107589e-02  -7.82500654e-02  -5.90122193e-02  -7.84891024e-02
  -7.84488469e-02  -7.84411132e-02  -7.85465539e-02  -3.94061171e-02
  -5.91361746e-02  -7.87085742e-02  -3.94070745e-02  -5.89688234e-02
  -3.90741974e-02  -7.85932466e-02  -1.95922349e-02  -7.84098431e-02
  -7.86284581e-02  -7.83059373e-02  -7.83827603e-02  -7.84702003e-02
  -1.96667369e-02  -3.92353609e-02  -3.92222181e-02  -3.93697470e-02
  -7.84397051e-02  -5.88874258e-02  -7.83634037e-02  -7.85019770e-02
  -7.85654858e-02  -7.84033015e-02  -7.84571394e-02  -7.82663822e-02
  -7.84994885e-02  -7.83911645e-02  -7.83184543e-02  -5.87112047e-02
  -7.83996806e-02  -7.86439627e-02  -7.86030367e-02  -7.84140751e-02]
DEBUG:root:[ Iteration 3 ] Training rotation loss: [ -3.94139963e-04   4.70349938e-02  -1.69844425e-04  -9.58497301e-02
  -3.83885665e-04  -2.39716741e-04  -3.57792247e-04   9.54173803e-02
   9.53443721e-02  -9.58445594e-02  -5.04144817e-04   4.73820083e-02
   9.52289477e-02   9.52678844e-02   9.53069031e-02  -1.44135120e-04
  -9.61358175e-02   9.51172188e-02  -9.56480056e-02  -9.59861726e-02
  -5.70476579e-04  -2.45108735e-04  -1.58626077e-04  -4.78128291e-04
  -4.16185881e-04  -4.80153896e-02   9.54009220e-02  -3.91986978e-04
  -4.08711960e-04  -5.14282845e-04  -9.58408043e-02   9.54487994e-02
   9.50830653e-02   9.53127667e-02   9.52253044e-02   4.75051925e-02
  -4.63815231e-04   9.48339030e-02  -1.38160598e-04  -4.81424555e-02
   9.52715203e-02   9.48534310e-02  -3.98133125e-04  -2.98645202e-04
   9.55681950e-02   9.51949582e-02  -5.76274833e-05   9.54528674e-02
   9.51478258e-02   9.51397270e-02   9.54023972e-02  -2.32344639e-04
   9.50672403e-02   4.74821739e-02  -4.60475596e-04  -3.71669506e-04
   9.52665508e-02  -3.64494364e-04  -4.78741527e-02  -1.20090208e-05
   9.52736884e-02   9.51436684e-02  -4.82270904e-02   9.50443968e-02
  -9.60940719e-02   9.49836895e-02   4.73243855e-02  -4.11340588e-04
   9.51329991e-02  -4.79415283e-02   9.51957554e-02   9.54406485e-02
   9.49637070e-02  -3.80516576e-04   9.55019519e-02  -4.79330681e-02
   9.54323858e-02   9.52158272e-02   9.53232199e-02  -3.39106948e-04
  -2.20170652e-04  -4.80935052e-02   9.51128304e-02   4.78045754e-02
   9.49596763e-02  -1.01294472e-05   9.52648893e-02  -2.42289869e-04
  -1.71647771e-04   9.49412510e-02   9.50995609e-02   9.52789336e-02
  -3.77381832e-04   9.50687975e-02  -3.91357433e-04  -2.62282265e-04
  -3.99280863e-04  -9.59930569e-02  -5.88959316e-04  -4.09008586e-04
  -4.80426624e-02   4.74158116e-02  -2.85420392e-04   9.52332690e-02
   9.49475467e-02  -5.22742106e-04   9.56202298e-02  -9.60255265e-02
   4.72113043e-02   9.50358659e-02  -4.81629595e-02   9.49232429e-02
   9.49662924e-02  -5.01125527e-04   9.49414521e-02   4.74108681e-02
  -9.58628431e-02  -9.62145701e-02   9.54256132e-02   9.49669927e-02
   9.50145572e-02   9.54073370e-02  -4.80398946e-02  -1.87845697e-04
  -9.60232466e-02  -4.79588322e-02  -4.81751189e-02  -4.85643424e-04
   4.74276282e-02  -9.62120295e-02  -5.42795518e-04   9.52929258e-02
   9.48714465e-02   9.48468670e-02  -4.81862947e-02   9.49756876e-02
   4.72208783e-02  -9.61337164e-02   9.53776687e-02  -3.18653940e-04
   9.51155350e-02  -3.83507722e-04   9.54866707e-02   9.51065496e-02
   9.48790312e-02   4.71545160e-02  -2.99802516e-04   9.49425101e-02
   9.53011587e-02  -5.66479168e-04  -4.84815016e-02  -4.79444563e-02
   9.51294899e-02   9.53578874e-02   4.71668020e-02  -1.55445334e-04
  -4.17208299e-04  -4.65100602e-04   9.54371318e-02   9.54343900e-02
  -4.46732040e-04   9.55460221e-02   4.75319177e-02   4.72778678e-02
  -6.41570194e-04  -9.57886130e-02  -9.59952027e-02  -4.79813255e-02
  -4.70480096e-04  -6.06027374e-04   4.73459214e-02   9.48898196e-02
  -9.59625319e-02  -4.81471680e-02   9.53157395e-02   9.50113311e-02
  -9.61686522e-02   9.52296704e-02   9.52271968e-02   7.82278130e-06
  -3.92721529e-04   4.72160578e-02   9.52773541e-02   9.51277241e-02
   9.49876904e-02   9.47356522e-02   4.75628264e-02  -3.45618813e-04
   9.50514227e-02   4.72929515e-02  -1.75600027e-04  -9.57921743e-02
   9.54746455e-02   9.54974145e-02  -3.36569909e-04   9.52814594e-02
  -4.80406843e-02   4.74675894e-02   4.76618521e-02   4.76147570e-02]  Training rotation loss: [-0.07870172 -0.07868648 -0.07863069  0.00012401 -0.07860523 -0.07876368
 -0.07873783 -0.03908137 -0.01946572 -0.07859302 -0.07860012 -0.07899723
 -0.0593159  -0.07856189 -0.05913966 -0.07878973 -0.01951223 -0.07864013
 -0.07856886 -0.07867343 -0.07866694 -0.07871463 -0.07848056 -0.0788383
 -0.07861009 -0.0786393  -0.07895535 -0.07881529 -0.07880937 -0.07862007
  0.00043565  0.000362   -0.01942527 -0.05893452 -0.07879549 -0.07868244
 -0.07875193 -0.05885739 -0.07860252 -0.07871272 -0.07870985 -0.05895481
 -0.07878021 -0.07874078 -0.07869712 -0.07860135 -0.07873387 -0.07872848
 -0.07873958 -0.03918109 -0.01949569 -0.0786539  -0.03925045 -0.07875853
 -0.07873021 -0.07853257 -0.07879356 -0.07855746 -0.07863192 -0.07860888
 -0.03936508 -0.07878634 -0.0786116  -0.07880226 -0.05916209 -0.01947255
 -0.07870043 -0.07858787 -0.03925504 -0.07855545 -0.07875717 -0.03911775
 -0.07916926 -0.07876444 -0.07875385 -0.07876658 -0.03925439 -0.01957143
 -0.07862407 -0.07865702 -0.07867026 -0.07874322 -0.07856167 -0.078558
 -0.05937807 -0.07852859 -0.03909527 -0.07874952 -0.07861067 -0.05922555
 -0.07881417 -0.07863237 -0.07863854 -0.03932161 -0.07865338 -0.07875505
 -0.07861012 -0.07858921 -0.07863767 -0.07861885 -0.07869865 -0.07878789
 -0.07882816 -0.0392706   0.00012892 -0.078649   -0.07862706 -0.07884982
 -0.07869659 -0.05880232 -0.07865423 -0.07861399 -0.07874861 -0.07854089
 -0.03947445 -0.07910395 -0.07865918 -0.07887727 -0.05880574 -0.01946284
 -0.07908069 -0.07858319 -0.07859888 -0.07855351 -0.07869792 -0.07854901
 -0.078714   -0.07858358 -0.07864473 -0.07874709 -0.07907426 -0.0194673
 -0.01936625 -0.03917203 -0.07884511 -0.01947526 -0.07871066 -0.07877298
 -0.07858093 -0.07867796 -0.07875756 -0.07861505 -0.07858046 -0.05897225
 -0.05883291 -0.07892571 -0.0787139  -0.07881161 -0.07904003 -0.0788184
 -0.0785735  -0.07871087 -0.01952485 -0.07875136 -0.07887338 -0.07873577
 -0.07855477 -0.03919518 -0.07872293 -0.0789599  -0.07891239 -0.03931779
 -0.07887475 -0.07869586 -0.07878362 -0.03928732 -0.01943802 -0.07877787
 -0.07862268 -0.07871701 -0.07886813 -0.05900742 -0.0786093  -0.07859236
 -0.0789598   0.00010019 -0.05906442 -0.03928676 -0.03924933 -0.07874711
 -0.0787918  -0.07881599 -0.05920321 -0.07870483 -0.059058   -0.03914676
 -0.07882919 -0.07875528 -0.01951857 -0.07873856 -0.0785993  -0.07856203
 -0.0589139  -0.07888111 -0.07865678 -0.05891626 -0.07864266 -0.07869466
 -0.07877076 -0.07876124]
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-21-2016_13h03m13s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0370309  Training rotation loss: -0.0686445
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0390267  Training rotation loss: -0.0667266
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0271225  Training rotation loss: -0.0670782
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0278648  Training rotation loss: -0.0678455
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0290183  Training rotation loss: -0.0673984
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0386473  Training rotation loss: -0.0674786
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-21-2016_13h05m01s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0119229  Training rotation loss: 0.00530717
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0120359  Training rotation loss: 0.00499889
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0122802  Training rotation loss: 0.00516712
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0121673  Training rotation loss: 0.00528588
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0119289  Training rotation loss: 0.00506686
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0119444  Training rotation loss: 0.00497414
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0132436  Training rotation loss: 0.0045178
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0119055  Training rotation loss: 0.00415375
DEBUG:root:[ Iteration 20 ] Training rotation loss: 0.0112708  Training rotation loss: 0.00357985
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0104759  Training rotation loss: 0.00328846
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0105309  Training rotation loss: 0.00189005
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.00985141  Training rotation loss: 0.00115038
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0108507  Training rotation loss: 0.00108932
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.00955966  Training rotation loss: 0.000861487
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.00856892  Training rotation loss: 0.00093035
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0105018  Training rotation loss: 0.000912238
DEBUG:root:[ Iteration 40 ] Training rotation loss: 0.00853381  Training rotation loss: 0.000832675
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.00904273  Training rotation loss: 0.000830538
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-21-2016_13h08m09s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0115544  Training extension loss: 0.00519798
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.012103  Training extension loss: 0.00485437
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0116939  Training extension loss: 0.00518857
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.011764  Training extension loss: 0.00499563
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0124516  Training extension loss: 0.0048224
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-21-2016_13h09m33s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0126686  Training extension loss: 0.00550972
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.012542  Testing extension loss: 0.00530845
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0119866  Training extension loss: 0.0052603
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0121838  Training extension loss: 0.00533446
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.012521  Training extension loss: 0.00514628
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0128858  Training extension loss: 0.00494315
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0125108  Training extension loss: 0.00501975
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0126093  Training extension loss: 0.00468634
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0116177  Testing extension loss: 0.00459851
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0115765  Training extension loss: 0.00470219
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0107628  Training extension loss: 0.00441699
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.0120947  Training extension loss: 0.00367966
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.010183  Training extension loss: 0.00265008
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0096377  Training extension loss: 0.00172943
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.00864746  Training extension loss: 0.00113832
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0100735  Training extension loss: 0.00101582
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.0091368  Testing extension loss: 0.00104252
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.00912306  Training extension loss: 0.000878487
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00964069  Training extension loss: 0.00104537
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00796801  Training extension loss: 0.000692711
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00914589  Training extension loss: 0.000843772
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00884059  Training extension loss: 0.000728317
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00804765  Training extension loss: 0.000973548
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00859308  Training extension loss: 0.000852158
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00866483  Testing extension loss: 0.00100142
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00857559  Training extension loss: 0.00105592
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00805616  Training extension loss: 0.000947631
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.0077203  Training extension loss: 0.000960368
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00817935  Training extension loss: 0.000834431
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00728908  Training extension loss: 0.000925352
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.008154  Training extension loss: 0.00105818
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00742241  Testing extension loss: 0.000985929
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00736256  Training extension loss: 0.000979874
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00752432  Training extension loss: 0.000868034
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00754229  Training extension loss: 0.000900661
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.0072118  Training extension loss: 0.000885674
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00692346  Training extension loss: 0.0007959
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00691396  Training extension loss: 0.000904549
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00712028  Training extension loss: 0.000914589
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00762036  Testing extension loss: 0.000976665
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00689974  Training extension loss: 0.000742017
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00622672  Training extension loss: 0.0006988
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00784309  Training extension loss: 0.000989473
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00732249  Training extension loss: 0.000872216
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00739448  Training extension loss: 0.00106327
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00724773  Training extension loss: 0.000778168
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00744096  Training extension loss: 0.000842119
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00644391  Testing extension loss: 0.00081429
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00700556  Training extension loss: 0.000858955
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00720487  Training extension loss: 0.000958653
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00719152  Training extension loss: 0.000830554
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.0068486  Training extension loss: 0.00122032
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00675316  Training extension loss: 0.000854737
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00694919  Training extension loss: 0.000970779
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00714537  Testing extension loss: 0.000819934
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00674801  Training extension loss: 0.000937948
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00643871  Training extension loss: 0.0008484
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00712972  Training extension loss: 0.000880741
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00734462  Training extension loss: 0.000875657
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00662425  Training extension loss: 0.000915696
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00638912  Training extension loss: 0.000889419
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00719959  Training extension loss: 0.000967451
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00626555  Testing extension loss: 0.000855953
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00683611  Training extension loss: 0.000850112
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00662031  Training extension loss: 0.000855081
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00626023  Training extension loss: 0.000940324
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00663879  Training extension loss: 0.000933244
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00647709  Training extension loss: 0.000935798
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00655454  Training extension loss: 0.000852116
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00588433  Training extension loss: 0.000882962
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00591334  Testing extension loss: 0.000826433
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.0063714  Training extension loss: 0.00100215
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00618732  Training extension loss: 0.00115175
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00694433  Training extension loss: 0.000877466
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.00664333  Training extension loss: 0.00084586
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00638212  Training extension loss: 0.000801826
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00641544  Training extension loss: 0.0010265
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00654896  Testing extension loss: 0.00102208
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00657511  Training extension loss: 0.00107914
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.0068007  Training extension loss: 0.00093154
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00609051  Training extension loss: 0.000738524
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00561946  Training extension loss: 0.000704636
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00628973  Training extension loss: 0.00107083
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.0061371  Training extension loss: 0.000822435
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00620018  Training extension loss: 0.000981327
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00635852  Testing extension loss: 0.000839006
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00645285  Training extension loss: 0.00104912
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00563768  Training extension loss: 0.000900782
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00625588  Training extension loss: 0.00077944
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00630087  Training extension loss: 0.000772473
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.00635495  Training extension loss: 0.000918533
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.00637648  Training extension loss: 0.00105419
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00654267  Training extension loss: 0.00102221
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00614954  Testing extension loss: 0.000908929
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.00646226  Training extension loss: 0.000834481
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.006638  Training extension loss: 0.000872828
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00561711  Training extension loss: 0.000770473
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00613185  Training extension loss: 0.000944693
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.00595106  Training extension loss: 0.000975771
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00578268  Training extension loss: 0.00093876
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00658738  Testing extension loss: 0.00104051
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00576322  Training extension loss: 0.000713743
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.00638628  Training extension loss: 0.00088347
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00600105  Training extension loss: 0.00117239
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00591671  Training extension loss: 0.000811758
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00638666  Training extension loss: 0.000995763
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.00614044  Training extension loss: 0.00101438
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.00607717  Training extension loss: 0.000971862
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.00624503  Testing extension loss: 0.000776667
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.0058408  Training extension loss: 0.000875729
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.00550515  Training extension loss: 0.00103197
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.00610481  Training extension loss: 0.000830219
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.00584234  Training extension loss: 0.000840317
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.00528063  Training extension loss: 0.000961413
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.00606107  Training extension loss: 0.000993838
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-21-2016_13h18m30s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.012443  Training extension loss: 0.00524628
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0123501  Testing extension loss: 0.00524453
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0131543  Training extension loss: 0.00492074
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0120669  Training extension loss: 0.00488005
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0128949  Training extension loss: 0.00474413
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0139937  Training extension loss: 0.00446315
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0137148  Training extension loss: 0.00454423
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0121009  Training extension loss: 0.00467136
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0116244  Testing extension loss: 0.00482365
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.01275  Training extension loss: 0.0044774
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0126663  Training extension loss: 0.00428793
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.012025  Training extension loss: 0.00433079
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0124777  Training extension loss: 0.00397764
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0122935  Training extension loss: 0.00349652
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.0121772  Training extension loss: 0.00298716
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0111503  Training extension loss: 0.00228375
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.0102936  Testing extension loss: 0.00203346
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.00982495  Training extension loss: 0.00153214
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.0104397  Training extension loss: 0.00146086
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00966623  Training extension loss: 0.00132781
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00954473  Training extension loss: 0.00144125
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.0102962  Training extension loss: 0.00130225
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00998781  Training extension loss: 0.00125534
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00942543  Training extension loss: 0.00116224
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00860767  Testing extension loss: 0.00107976
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00919009  Training extension loss: 0.0014243
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00877966  Training extension loss: 0.00140013
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00887929  Training extension loss: 0.00128303
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00922373  Training extension loss: 0.00143307
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00859256  Training extension loss: 0.00140076
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00850917  Training extension loss: 0.00122508
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00803167  Testing extension loss: 0.00113868
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00827214  Training extension loss: 0.00118369
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00833771  Training extension loss: 0.00130646
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00797485  Training extension loss: 0.00120429
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00798491  Training extension loss: 0.00130683
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00866032  Training extension loss: 0.00135138
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00857748  Training extension loss: 0.00127372
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00847673  Training extension loss: 0.00127275
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00762383  Testing extension loss: 0.00110168
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00809439  Training extension loss: 0.00139373
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00787067  Training extension loss: 0.00134292
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.0080806  Training extension loss: 0.00137174
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.007492  Training extension loss: 0.00125146
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00740558  Training extension loss: 0.00138491
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00816857  Training extension loss: 0.00135749
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00741139  Training extension loss: 0.00137562
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00676044  Testing extension loss: 0.0010897
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00774907  Training extension loss: 0.00133469
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00774113  Training extension loss: 0.00141615
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00745885  Training extension loss: 0.00119391
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00725064  Training extension loss: 0.00114294
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.0065863  Training extension loss: 0.00125543
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00694047  Training extension loss: 0.00125752
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00699173  Testing extension loss: 0.00107197
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.0073741  Training extension loss: 0.00115368
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00695683  Training extension loss: 0.00111734
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00736073  Training extension loss: 0.00117632
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00676967  Training extension loss: 0.00121913
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00677566  Training extension loss: 0.00139044
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00691901  Training extension loss: 0.00134232
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00654866  Training extension loss: 0.00131174
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00675116  Testing extension loss: 0.00109611
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00651422  Training extension loss: 0.00125827
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00646769  Training extension loss: 0.00117501
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.0064057  Training extension loss: 0.00119963
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00592476  Training extension loss: 0.00122927
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00645898  Training extension loss: 0.0012356
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00615026  Training extension loss: 0.00115898
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00638367  Training extension loss: 0.00120044
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00648654  Testing extension loss: 0.00108033
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00609066  Training extension loss: 0.00118072
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00594664  Training extension loss: 0.00122713
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00582183  Training extension loss: 0.0012126
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.00608646  Training extension loss: 0.00120089
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00648092  Training extension loss: 0.00113989
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00524911  Training extension loss: 0.00105512
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00592172  Testing extension loss: 0.00106672
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00583779  Training extension loss: 0.00135673
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00557743  Training extension loss: 0.0010859
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00573524  Training extension loss: 0.00121708
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00545468  Training extension loss: 0.0012664
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00559257  Training extension loss: 0.00129885
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.00579533  Training extension loss: 0.00105135
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00521047  Training extension loss: 0.0011804
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00565218  Testing extension loss: 0.00110613
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00503458  Training extension loss: 0.00109204
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.0050931  Training extension loss: 0.001126
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00473132  Training extension loss: 0.00115824
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00546515  Training extension loss: 0.00119858
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.00506533  Training extension loss: 0.00123309
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.00508167  Training extension loss: 0.00113606
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00507756  Training extension loss: 0.0011106
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00520023  Testing extension loss: 0.00108364
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.00457668  Training extension loss: 0.00114264
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.00468698  Training extension loss: 0.00120667
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00484371  Training extension loss: 0.0011625
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00505142  Training extension loss: 0.00109351
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.00481177  Training extension loss: 0.00124455
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00457319  Training extension loss: 0.00115867
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00549183  Testing extension loss: 0.00110253
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00494892  Training extension loss: 0.00112262
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.00488073  Training extension loss: 0.00123097
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00513814  Training extension loss: 0.00123126
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00450563  Training extension loss: 0.0011408
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00441495  Training extension loss: 0.00102181
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-21-2016_15h05m59s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.013073  Training extension loss: 0.00516491
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0134204  Testing extension loss: 0.00506698
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0127156  Training extension loss: 0.00529404
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0142881  Training extension loss: 0.00494776
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0128436  Training extension loss: 0.00488051
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0131866  Training extension loss: 0.00475998
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0133241  Training extension loss: 0.00464379
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0137981  Training extension loss: 0.00425701
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.012035  Testing extension loss: 0.00375344
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0118852  Training extension loss: 0.00370074
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.01187  Training extension loss: 0.00240082
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.0108393  Training extension loss: 0.00151025
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.00998976  Training extension loss: 0.00115992
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0108903  Training extension loss: 0.000980217
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.0110092  Training extension loss: 0.000885371
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0100884  Training extension loss: 0.00105298
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.0105169  Testing extension loss: 0.000840323
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.0091812  Training extension loss: 0.000753537
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.0100547  Training extension loss: 0.000867797
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00989062  Training extension loss: 0.000892379
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00965909  Training extension loss: 0.000874138
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00990859  Training extension loss: 0.000755788
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.0102133  Training extension loss: 0.000841279
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00860538  Training extension loss: 0.00083641
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00833291  Testing extension loss: 0.00103502
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00912312  Training extension loss: 0.000963638
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00860091  Training extension loss: 0.000959526
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00876484  Training extension loss: 0.000954655
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00974526  Training extension loss: 0.00108725
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00810652  Training extension loss: 0.000818882
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00909984  Training extension loss: 0.00104037
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00814078  Testing extension loss: 0.00117786
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00765299  Training extension loss: 0.000765381
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00911218  Training extension loss: 0.00100182
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00814943  Training extension loss: 0.000829531
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00807853  Training extension loss: 0.000902256
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00742195  Training extension loss: 0.000996218
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00881287  Training extension loss: 0.000846386
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.0084638  Training extension loss: 0.00101982
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00792153  Testing extension loss: 0.00104093
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00835673  Training extension loss: 0.000981948
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.0086517  Training extension loss: 0.000965049
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00748996  Training extension loss: 0.00085558
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00788229  Training extension loss: 0.000928431
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00878285  Training extension loss: 0.00106706
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00747858  Training extension loss: 0.000700666
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00757396  Training extension loss: 0.000988879
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00741492  Testing extension loss: 0.0011254
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00787111  Training extension loss: 0.000867853
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00718029  Training extension loss: 0.000848315
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00771873  Training extension loss: 0.000852072
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00804543  Training extension loss: 0.000955775
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00751428  Training extension loss: 0.000845653
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00795665  Training extension loss: 0.000846013
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00715624  Testing extension loss: 0.00107244
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00795114  Training extension loss: 0.0010878
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00744037  Training extension loss: 0.000908826
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00749643  Training extension loss: 0.00100998
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00766567  Training extension loss: 0.000967081
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00731058  Training extension loss: 0.000896976
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00803931  Training extension loss: 0.000962158
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00716543  Training extension loss: 0.000866838
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00653135  Testing extension loss: 0.00109237
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00768706  Training extension loss: 0.00102362
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00675281  Training extension loss: 0.00101685
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00739857  Training extension loss: 0.000865715
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00708994  Training extension loss: 0.000914778
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00725179  Training extension loss: 0.00100037
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00674645  Training extension loss: 0.000956132
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00715749  Training extension loss: 0.000930368
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00736691  Testing extension loss: 0.00117557
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00680381  Training extension loss: 0.000873656
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00652262  Training extension loss: 0.000870702
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.006811  Training extension loss: 0.000953129
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.00644234  Training extension loss: 0.000920297
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00664288  Training extension loss: 0.000982566
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00683127  Training extension loss: 0.000909212
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00695424  Testing extension loss: 0.00124101
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00718057  Training extension loss: 0.000791395
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00699998  Training extension loss: 0.00105654
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.0065944  Training extension loss: 0.000984446
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00629415  Training extension loss: 0.00106646
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00694473  Training extension loss: 0.000981104
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.00670409  Training extension loss: 0.000851564
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00689373  Training extension loss: 0.000807065
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00682823  Testing extension loss: 0.00109635
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00683064  Training extension loss: 0.0010194
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00624101  Training extension loss: 0.000943032
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00689262  Training extension loss: 0.000990003
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00555126  Training extension loss: 0.00105635
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.00663167  Training extension loss: 0.000912856
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.00648396  Training extension loss: 0.000826612
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00647555  Training extension loss: 0.000929066
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00661577  Testing extension loss: 0.0010975
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.00550081  Training extension loss: 0.00088768
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.0067503  Training extension loss: 0.00111869
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00644687  Training extension loss: 0.000948367
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00618518  Training extension loss: 0.00109174
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.00655853  Training extension loss: 0.00111522
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00663685  Training extension loss: 0.000966695
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.0061241  Testing extension loss: 0.00120809
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00590061  Training extension loss: 0.00106679
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.00557198  Training extension loss: 0.000940639
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00618902  Training extension loss: 0.0010647
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00570444  Training extension loss: 0.000952859
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00602446  Training extension loss: 0.000987998
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.00642732  Training extension loss: 0.000945723
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.0058601  Training extension loss: 0.00102669
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.00605969  Testing extension loss: 0.000991462
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.00563148  Training extension loss: 0.000972876
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.006302  Training extension loss: 0.00113583
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.00646617  Training extension loss: 0.00101915
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.00568943  Training extension loss: 0.00110413
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.00611565  Training extension loss: 0.000909136
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.00566992  Training extension loss: 0.00110848
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-21-2016_16h43m38s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0132275  Training extension loss: 0.00530622
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0121138  Testing extension loss: 0.00532042
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0127142  Training extension loss: 0.0052455
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0123248  Training extension loss: 0.00506839
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.011861  Training extension loss: 0.00518155
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0112689  Training extension loss: 0.00504951
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0120359  Training extension loss: 0.00482429
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0122345  Training extension loss: 0.00435393
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0108532  Testing extension loss: 0.00438845
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0126789  Training extension loss: 0.00387808
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0105262  Training extension loss: 0.00274173
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.00976856  Training extension loss: 0.00161727
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.00938153  Training extension loss: 0.00107726
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0103449  Training extension loss: 0.00103293
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.00968658  Training extension loss: 0.000790594
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.00785387  Training extension loss: 0.000644972
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.0081124  Testing extension loss: 0.000667733
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.009405  Training extension loss: 0.000857073
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00835624  Training extension loss: 0.000768988
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00966944  Training extension loss: 0.000869713
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00886277  Training extension loss: 0.000912096
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00820683  Training extension loss: 0.00106001
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00891096  Training extension loss: 0.000963602
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00875149  Training extension loss: 0.000790078
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00845321  Testing extension loss: 0.000774611
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00740254  Training extension loss: 0.000807145
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00834578  Training extension loss: 0.00076367
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00780816  Training extension loss: 0.000824295
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00819406  Training extension loss: 0.000942744
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00712712  Training extension loss: 0.000784716
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00722892  Training extension loss: 0.00102509
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00788269  Testing extension loss: 0.000770788
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00859216  Training extension loss: 0.000789108
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00804454  Training extension loss: 0.000920728
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00799748  Training extension loss: 0.000830097
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00832419  Training extension loss: 0.00114909
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00739376  Training extension loss: 0.00097791
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00718476  Training extension loss: 0.00086355
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00664979  Training extension loss: 0.000794914
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00669471  Testing extension loss: 0.000814187
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00763392  Training extension loss: 0.00105041
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00729758  Training extension loss: 0.000842059
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00731005  Training extension loss: 0.000863031
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00704538  Training extension loss: 0.00100411
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00714355  Training extension loss: 0.00081483
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00829761  Training extension loss: 0.000877176
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00712967  Training extension loss: 0.00101957
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00626836  Testing extension loss: 0.000673472
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00720608  Training extension loss: 0.000985623
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00641287  Training extension loss: 0.000779759
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.0066142  Training extension loss: 0.000863573
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.006881  Training extension loss: 0.000875594
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00660707  Training extension loss: 0.000977312
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00702901  Training extension loss: 0.00106696
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00680632  Testing extension loss: 0.000869074
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00680767  Training extension loss: 0.000883317
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00768892  Training extension loss: 0.00103591
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00670221  Training extension loss: 0.00101146
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00724261  Training extension loss: 0.000907455
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00610508  Training extension loss: 0.000841188
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00688734  Training extension loss: 0.00102256
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00642068  Training extension loss: 0.000848964
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.0062228  Testing extension loss: 0.000740841
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00657452  Training extension loss: 0.00101376
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00671945  Training extension loss: 0.00108365
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00685564  Training extension loss: 0.00103386
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00657235  Training extension loss: 0.000917568
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00608776  Training extension loss: 0.00100989
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00703962  Training extension loss: 0.000952979
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00640404  Training extension loss: 0.00101461
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00660394  Testing extension loss: 0.000766582
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00613786  Training extension loss: 0.00095106
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00711614  Training extension loss: 0.000883218
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00707037  Training extension loss: 0.000916337
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.0064558  Training extension loss: 0.00102575
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00620588  Training extension loss: 0.00114432
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00637308  Training extension loss: 0.000941751
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00669233  Testing extension loss: 0.000906498
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00646115  Training extension loss: 0.000959801
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00684643  Training extension loss: 0.00102697
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00622266  Training extension loss: 0.00101046
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00624663  Training extension loss: 0.000845014
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00602117  Training extension loss: 0.000970554
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.0060526  Training extension loss: 0.00109578
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00630125  Training extension loss: 0.00106096
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00615929  Testing extension loss: 0.000841105
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00632283  Training extension loss: 0.000995038
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00593458  Training extension loss: 0.000911034
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00677223  Training extension loss: 0.000972432
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00592955  Training extension loss: 0.00101265
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.00668001  Training extension loss: 0.00112525
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.00647995  Training extension loss: 0.000958247
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00686792  Training extension loss: 0.00100165
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00625007  Testing extension loss: 0.000944087
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.0059952  Training extension loss: 0.000955222
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.00605889  Training extension loss: 0.000885916
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00598337  Training extension loss: 0.00110178
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00634438  Training extension loss: 0.000970566
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.0058596  Training extension loss: 0.000941386
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00618055  Training extension loss: 0.000859419
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00573254  Testing extension loss: 0.00075758
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00612188  Training extension loss: 0.00105886
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.00586531  Training extension loss: 0.000942855
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00644759  Training extension loss: 0.000972226
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00580078  Training extension loss: 0.000899861
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00589434  Training extension loss: 0.00100311
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.00589679  Training extension loss: 0.000819647
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.00587144  Training extension loss: 0.00121731
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.0055913  Testing extension loss: 0.000843768
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.00589735  Training extension loss: 0.000952784
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.00606566  Training extension loss: 0.000885538
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.00600064  Training extension loss: 0.00106386
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.00592698  Training extension loss: 0.00110894
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.00550639  Training extension loss: 0.00105558
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.00574438  Training extension loss: 0.000986442
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-21-2016_17h33m35s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.012383  Training extension loss: 0.00518112
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0116623  Testing extension loss: 0.00540659
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-21-2016_17h35m07s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0115298  Training extension loss: 0.0051917
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0127306  Testing extension loss: 0.00509505
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0120106  Training extension loss: 0.00491757
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0124877  Training extension loss: 0.00504719
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0116779  Training extension loss: 0.00502221
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0104942  Training extension loss: 0.00488662
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0120424  Training extension loss: 0.00469864
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0121817  Training extension loss: 0.00396094
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0115005  Testing extension loss: 0.00344844
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.011504  Training extension loss: 0.00316269
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0105796  Training extension loss: 0.00186893
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.00916564  Training extension loss: 0.00126921
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0100749  Training extension loss: 0.000925966
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0094657  Training extension loss: 0.000864115
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.00865855  Training extension loss: 0.000735283
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0091479  Training extension loss: 0.000769937
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.00882415  Testing extension loss: 0.000704852
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.00892626  Training extension loss: 0.000909313
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00967389  Training extension loss: 0.000948317
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00873927  Training extension loss: 0.000978551
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00872515  Training extension loss: 0.000713607
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00834253  Training extension loss: 0.000724006
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00872135  Training extension loss: 0.000751434
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00761616  Training extension loss: 0.00083656
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00968696  Testing extension loss: 0.000957248
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00813679  Training extension loss: 0.000742488
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00778963  Training extension loss: 0.000729513
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00822078  Training extension loss: 0.000786903
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00783024  Training extension loss: 0.000775269
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.008809  Training extension loss: 0.000931473
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.0083147  Training extension loss: 0.00099784
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00791732  Testing extension loss: 0.000793372
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00748567  Training extension loss: 0.00103897
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00708604  Training extension loss: 0.00083748
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00792988  Training extension loss: 0.00103618
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.0078963  Training extension loss: 0.0011077
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00747838  Training extension loss: 0.000804917
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00764371  Training extension loss: 0.000777398
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00739329  Training extension loss: 0.000752034
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00787225  Testing extension loss: 0.000949894
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00724755  Training extension loss: 0.000738851
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00687956  Training extension loss: 0.000921704
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00750213  Training extension loss: 0.000852734
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00717913  Training extension loss: 0.000871371
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00700051  Training extension loss: 0.00103112
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00687787  Training extension loss: 0.000797389
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00727809  Training extension loss: 0.000943386
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00717345  Testing extension loss: 0.000864905
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00670079  Training extension loss: 0.000697365
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00736492  Training extension loss: 0.000991591
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.0077931  Training extension loss: 0.000961748
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00680701  Training extension loss: 0.000829956
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00671954  Training extension loss: 0.000930622
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00754949  Training extension loss: 0.000974063
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00680209  Testing extension loss: 0.000899611
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.0069014  Training extension loss: 0.000902796
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00688714  Training extension loss: 0.000748767
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.0064911  Training extension loss: 0.00101585
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00650905  Training extension loss: 0.000805607
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00691746  Training extension loss: 0.000915338
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00691037  Training extension loss: 0.00109321
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00687377  Training extension loss: 0.000816413
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00664369  Testing extension loss: 0.0010133
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00661742  Training extension loss: 0.000979614
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00700091  Training extension loss: 0.000946089
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.0066356  Training extension loss: 0.000985772
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00606598  Training extension loss: 0.00109376
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00659347  Training extension loss: 0.000921304
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00685101  Training extension loss: 0.00102026
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00651744  Training extension loss: 0.000899734
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00673638  Testing extension loss: 0.000933713
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.0067751  Training extension loss: 0.000985752
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00603958  Training extension loss: 0.000915186
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00652393  Training extension loss: 0.000907629
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.00680068  Training extension loss: 0.00106181
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00692313  Training extension loss: 0.000930142
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00661766  Training extension loss: 0.000854896
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00653958  Testing extension loss: 0.000975102
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.0059011  Training extension loss: 0.000908574
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00667067  Training extension loss: 0.000952504
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00648518  Training extension loss: 0.000800871
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00616845  Training extension loss: 0.000994636
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00634088  Training extension loss: 0.00101402
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.00606469  Training extension loss: 0.000973234
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00582923  Training extension loss: 0.000925972
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.0057597  Testing extension loss: 0.00104162
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.0063872  Training extension loss: 0.00089538
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00641725  Training extension loss: 0.00100109
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00615187  Training extension loss: 0.00102293
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00607895  Training extension loss: 0.000949005
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.00662293  Training extension loss: 0.000912177
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.00633247  Training extension loss: 0.0010224
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00595108  Training extension loss: 0.000946598
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00634841  Testing extension loss: 0.000994069
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.00618529  Training extension loss: 0.000915477
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.00649496  Training extension loss: 0.00124738
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00597594  Training extension loss: 0.000875147
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00593914  Training extension loss: 0.000928778
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.0060503  Training extension loss: 0.00108905
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00618471  Training extension loss: 0.000877966
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00608243  Testing extension loss: 0.000899772
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00591763  Training extension loss: 0.00100641
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.00663766  Training extension loss: 0.00107754
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00636757  Training extension loss: 0.00100075
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00586674  Training extension loss: 0.000974819
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00589717  Training extension loss: 0.000951479
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.00605023  Training extension loss: 0.000961425
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.00636483  Training extension loss: 0.00117078
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.00584934  Testing extension loss: 0.00105336
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.00612052  Training extension loss: 0.00113191
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.00608857  Training extension loss: 0.000990408
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.00634764  Training extension loss: 0.00118902
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.0058252  Training extension loss: 0.000975802
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.00624876  Training extension loss: 0.00104094
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.00609879  Training extension loss: 0.00109326
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-22-2016_10h27m21s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.00564356  Training extension loss: 0.0010926
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.00611475  Testing extension loss: 0.00107268
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.00568706  Training extension loss: 0.00106523
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.00571386  Training extension loss: 0.00106521
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.00555347  Training extension loss: 0.00109748
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.00613784  Training extension loss: 0.00102072
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.00586761  Training extension loss: 0.000915566
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.00641935  Training extension loss: 0.00114478
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.00665874  Testing extension loss: 0.00108861
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0059529  Training extension loss: 0.00108555
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.00602792  Training extension loss: 0.00118429
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.00532678  Training extension loss: 0.00104845
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.00569789  Training extension loss: 0.00105931
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.00631068  Training extension loss: 0.00121176
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.00559962  Training extension loss: 0.00105961
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.00586001  Training extension loss: 0.00111928
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.00601258  Testing extension loss: 0.00106313
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.00565037  Training extension loss: 0.00104158
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00525331  Training extension loss: 0.00104792
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00628866  Training extension loss: 0.00103329
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00578051  Training extension loss: 0.00115304
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00567416  Training extension loss: 0.00107485
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00512179  Training extension loss: 0.00114289
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00616873  Training extension loss: 0.00103086
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00654694  Testing extension loss: 0.00104861
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-22-2016_10h30m35s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0127097  Training extension loss: 0.00528897
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0127604  Testing extension loss: 0.00531687
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0116867  Training extension loss: 0.00502553
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.010647  Training extension loss: 0.00517863
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0107181  Training extension loss: 0.00516097
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0118537  Training extension loss: 0.00483767
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.011416  Training extension loss: 0.00467244
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0110343  Training extension loss: 0.00470454
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0120498  Testing extension loss: 0.0041345
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0114135  Training extension loss: 0.00408606
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.011137  Training extension loss: 0.00311644
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.00985424  Training extension loss: 0.00183828
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.00975481  Training extension loss: 0.00130341
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0103341  Training extension loss: 0.000989802
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.0095112  Training extension loss: 0.000924943
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.00993705  Training extension loss: 0.000858342
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.0101573  Testing extension loss: 0.000935962
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.009344  Training extension loss: 0.000907494
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00873877  Training extension loss: 0.000759486
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00935105  Training extension loss: 0.00109465
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00850716  Training extension loss: 0.000645747
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.0087285  Training extension loss: 0.000777773
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00890125  Training extension loss: 0.00110083
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00835924  Training extension loss: 0.00087862
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00849331  Testing extension loss: 0.000737941
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.0088458  Training extension loss: 0.000943933
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00877887  Training extension loss: 0.000731539
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00774043  Training extension loss: 0.000848841
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00765089  Training extension loss: 0.000984022
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00816332  Training extension loss: 0.00100817
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00781035  Training extension loss: 0.000841771
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00838482  Testing extension loss: 0.00106549
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00832313  Training extension loss: 0.000988743
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00915622  Training extension loss: 0.000946766
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00781635  Training extension loss: 0.00111803
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00792236  Training extension loss: 0.00102562
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00755635  Training extension loss: 0.00100231
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00756059  Training extension loss: 0.000993348
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00734461  Training extension loss: 0.000844579
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.0078859  Testing extension loss: 0.00105713
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00738295  Training extension loss: 0.000983597
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00740514  Training extension loss: 0.000894238
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00758577  Training extension loss: 0.000979693
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00774366  Training extension loss: 0.00099433
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00673075  Training extension loss: 0.000928614
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00708792  Training extension loss: 0.000756478
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00664709  Training extension loss: 0.000839932
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00746877  Testing extension loss: 0.000891639
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00762868  Training extension loss: 0.00102485
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00724912  Training extension loss: 0.000820607
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00700678  Training extension loss: 0.000991855
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00670381  Training extension loss: 0.000879669
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00744426  Training extension loss: 0.00084782
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00662084  Training extension loss: 0.000894856
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00699047  Testing extension loss: 0.00111663
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00648766  Training extension loss: 0.000881489
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00681078  Training extension loss: 0.000999994
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00698926  Training extension loss: 0.000977629
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00705512  Training extension loss: 0.000913247
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00749295  Training extension loss: 0.000954785
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00630489  Training extension loss: 0.000773496
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00678691  Training extension loss: 0.00092972
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00634255  Testing extension loss: 0.00103509
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00649387  Training extension loss: 0.000905383
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00702345  Training extension loss: 0.000892211
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00719971  Training extension loss: 0.000932056
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00585176  Training extension loss: 0.000952606
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00654201  Training extension loss: 0.000761951
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00575528  Training extension loss: 0.000763004
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00721611  Training extension loss: 0.000879234
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00622002  Testing extension loss: 0.000989588
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00669514  Training extension loss: 0.000949098
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00701923  Training extension loss: 0.00109552
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.0063281  Training extension loss: 0.000936875
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.0058217  Training extension loss: 0.00103692
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00625655  Training extension loss: 0.00095421
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00664939  Training extension loss: 0.000799704
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00702571  Testing extension loss: 0.000888484
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00656868  Training extension loss: 0.00117584
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00621446  Training extension loss: 0.000909758
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00605384  Training extension loss: 0.00108593
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00578926  Training extension loss: 0.0010253
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00655931  Training extension loss: 0.000964622
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.00646904  Training extension loss: 0.000896089
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00596564  Training extension loss: 0.00111659
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00665139  Testing extension loss: 0.00110293
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00641517  Training extension loss: 0.000847188
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00637605  Training extension loss: 0.000921908
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00611384  Training extension loss: 0.000889179
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00629312  Training extension loss: 0.000864264
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.0056011  Training extension loss: 0.000923566
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.0058421  Training extension loss: 0.000966818
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00643949  Training extension loss: 0.00113088
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00608202  Testing extension loss: 0.000954167
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.0057382  Training extension loss: 0.000848271
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.00635324  Training extension loss: 0.000905204
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00599991  Training extension loss: 0.000985521
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00576052  Training extension loss: 0.00099427
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.00658946  Training extension loss: 0.0010451
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00636069  Training extension loss: 0.00108332
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00666785  Testing extension loss: 0.00095266
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00615937  Training extension loss: 0.00109194
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.00569437  Training extension loss: 0.000704745
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00616338  Training extension loss: 0.000842282
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00621568  Training extension loss: 0.000821935
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00596779  Training extension loss: 0.000923069
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.00592889  Training extension loss: 0.000958241
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.00607795  Training extension loss: 0.00089529
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.00638738  Testing extension loss: 0.000974645
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.00609899  Training extension loss: 0.000909965
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.00612469  Training extension loss: 0.000826262
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.00611432  Training extension loss: 0.00106054
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.00581482  Training extension loss: 0.00111869
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.00571384  Training extension loss: 0.00100102
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.00654319  Training extension loss: 0.00108965
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-22-2016_11h24m54s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.00578546  Training extension loss: 0.00101253
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.00630037  Testing extension loss: 0.00114985
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.00626923  Training extension loss: 0.00114594
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.00597017  Training extension loss: 0.000866882
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.00567888  Training extension loss: 0.000966622
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.00628894  Training extension loss: 0.00101812
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.00541445  Training extension loss: 0.0010351
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.00569027  Training extension loss: 0.000984683
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.00549069  Testing extension loss: 0.000927432
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.00629444  Training extension loss: 0.00109619
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0060215  Training extension loss: 0.00101216
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.00623723  Training extension loss: 0.000967205
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.00584128  Training extension loss: 0.000950863
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.00602685  Training extension loss: 0.00110913
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.00579532  Training extension loss: 0.000705136
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.00634402  Training extension loss: 0.000939047
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.00636131  Testing extension loss: 0.00116545
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.0059422  Training extension loss: 0.000884486
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00565748  Training extension loss: 0.00104862
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00613927  Training extension loss: 0.000943329
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.0062984  Training extension loss: 0.00116441
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00557547  Training extension loss: 0.000925628
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00644849  Training extension loss: 0.00090937
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00603247  Training extension loss: 0.00110615
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00635329  Testing extension loss: 0.00102259
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00577219  Training extension loss: 0.00105935
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00614366  Training extension loss: 0.000993055
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00597631  Training extension loss: 0.000857089
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00506419  Training extension loss: 0.000929589
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.0055911  Training extension loss: 0.00101249
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00583895  Training extension loss: 0.000998441
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00563833  Testing extension loss: 0.00101356
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00617016  Training extension loss: 0.00103356
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00563743  Training extension loss: 0.00092503
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00707385  Training extension loss: 0.00101158
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00609071  Training extension loss: 0.000875793
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.0057226  Training extension loss: 0.00110175
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00557217  Training extension loss: 0.00101787
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00570646  Training extension loss: 0.00100975
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00582454  Testing extension loss: 0.000944756
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00634638  Training extension loss: 0.0010509
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00523747  Training extension loss: 0.0010996
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00592846  Training extension loss: 0.00106594
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00593642  Training extension loss: 0.00106523
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00564695  Training extension loss: 0.00101071
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00568841  Training extension loss: 0.00116018
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00563259  Training extension loss: 0.000856871
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00610653  Testing extension loss: 0.000820417
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.0058172  Training extension loss: 0.000998809
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00589502  Training extension loss: 0.00103104
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00556832  Training extension loss: 0.00100085
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00530527  Training extension loss: 0.0010249
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00579907  Training extension loss: 0.0011405
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00541873  Training extension loss: 0.00100965
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00562847  Testing extension loss: 0.00104733
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00623756  Training extension loss: 0.0010471
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00555216  Training extension loss: 0.00100394
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00570325  Training extension loss: 0.0010521
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00594987  Training extension loss: 0.00108116
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.0052026  Training extension loss: 0.00102678
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00594113  Training extension loss: 0.00108211
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00616704  Training extension loss: 0.00103305
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00576197  Testing extension loss: 0.00115486
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00544563  Training extension loss: 0.00109449
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00591375  Training extension loss: 0.00109781
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00589107  Training extension loss: 0.00115705
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00544034  Training extension loss: 0.00108627
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00540729  Training extension loss: 0.00109208
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00551536  Training extension loss: 0.00124188
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00580434  Training extension loss: 0.00121873
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00618724  Testing extension loss: 0.0010818
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00570489  Training extension loss: 0.00113757
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00526355  Training extension loss: 0.00108515
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00567415  Training extension loss: 0.00114663
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.0053189  Training extension loss: 0.000988106
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00536473  Training extension loss: 0.0011106
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00534187  Training extension loss: 0.00117528
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-22-2016_11h32m47s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0150148  Training extension loss: 0.00561961
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0149619  Testing extension loss: 0.00551825
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0144821  Training extension loss: 0.00546465
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0149781  Training extension loss: 0.0054953
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0145772  Training extension loss: 0.00540463
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0148951  Training extension loss: 0.00521022
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0150677  Training extension loss: 0.00518462
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0145722  Training extension loss: 0.00508992
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.014678  Testing extension loss: 0.00482138
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0145823  Training extension loss: 0.00466796
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0146006  Training extension loss: 0.00413062
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.0145473  Training extension loss: 0.00314016
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0137146  Training extension loss: 0.00204194
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0133907  Training extension loss: 0.00155682
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.0123845  Training extension loss: 0.001225
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0128269  Training extension loss: 0.000879474
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.0134051  Testing extension loss: 0.00116174
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.0136986  Training extension loss: 0.00104361
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.0125195  Training extension loss: 0.000783644
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.0132918  Training extension loss: 0.000882154
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.0124937  Training extension loss: 0.00071002
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.0116769  Training extension loss: 0.000919868
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.0120866  Training extension loss: 0.00118636
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.0113064  Training extension loss: 0.000970827
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.0113953  Testing extension loss: 0.000969397
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.0111853  Training extension loss: 0.000859332
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.0100962  Training extension loss: 0.000997157
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00972935  Training extension loss: 0.00091375
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00930335  Training extension loss: 0.000831841
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00860064  Training extension loss: 0.000828273
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00854834  Training extension loss: 0.000914167
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00878593  Testing extension loss: 0.000930611
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00809507  Training extension loss: 0.0010448
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00808171  Training extension loss: 0.00103323
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00718311  Training extension loss: 0.000736988
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00676194  Training extension loss: 0.000965711
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00681115  Training extension loss: 0.000906058
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00613246  Training extension loss: 0.000758295
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00618822  Training extension loss: 0.000633367
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00640185  Testing extension loss: 0.0009278
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00556193  Training extension loss: 0.00087335
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00517561  Training extension loss: 0.000836269
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00503366  Training extension loss: 0.000838282
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00518339  Training extension loss: 0.000780344
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00500607  Training extension loss: 0.000760629
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00427696  Training extension loss: 0.000906771
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00457941  Training extension loss: 0.000569952
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.0053556  Testing extension loss: 0.000950026
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00461928  Training extension loss: 0.000778248
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00418779  Training extension loss: 0.000594573
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00398199  Training extension loss: 0.000766152
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00410831  Training extension loss: 0.00044931
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00431522  Training extension loss: 0.000640193
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00348212  Training extension loss: 0.000692818
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00475337  Testing extension loss: 0.000688647
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00385153  Training extension loss: 0.000574811
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00376144  Training extension loss: 0.000512951
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-22-2016_14h38m02s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0158297  Training extension loss: 0.00523457
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.015127  Testing extension loss: 0.00535492
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0146343  Training extension loss: 0.00531842
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0147405  Training extension loss: 0.00524184
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.015243  Training extension loss: 0.00527413
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0155297  Training extension loss: 0.00507987
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0151476  Training extension loss: 0.00506313
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0149507  Training extension loss: 0.00464985
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0152946  Testing extension loss: 0.00432096
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0141354  Training extension loss: 0.00428083
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0152507  Training extension loss: 0.00341063
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.0148354  Training extension loss: 0.0022297
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0138606  Training extension loss: 0.00156708
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0144417  Training extension loss: 0.00152101
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.0138256  Training extension loss: 0.00131867
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0133231  Training extension loss: 0.00120128
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.0142881  Testing extension loss: 0.00166476
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.0127286  Training extension loss: 0.00152307
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.0139008  Training extension loss: 0.000980781
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.0122108  Training extension loss: 0.00126064
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.0125839  Training extension loss: 0.00113759
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.0127106  Training extension loss: 0.00104626
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.0124314  Training extension loss: 0.00124782
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.0117178  Training extension loss: 0.00109819
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.0115875  Testing extension loss: 0.00132094
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.0104066  Training extension loss: 0.0014974
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.0103973  Training extension loss: 0.00133404
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00933022  Training extension loss: 0.00132436
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.0104165  Training extension loss: 0.00128723
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00943407  Training extension loss: 0.00145332
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00874095  Training extension loss: 0.00139814
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00939781  Testing extension loss: 0.00139871
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00854947  Training extension loss: 0.00159319
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.0082854  Training extension loss: 0.00116428
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00853979  Training extension loss: 0.00153859
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00787542  Training extension loss: 0.00112905
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.0082423  Training extension loss: 0.00139688
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00753964  Training extension loss: 0.00120904
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00744674  Training extension loss: 0.00146893
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00706023  Testing extension loss: 0.00148875
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00687793  Training extension loss: 0.00137665
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00650781  Training extension loss: 0.0016765
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00558315  Training extension loss: 0.00146622
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00673695  Training extension loss: 0.00147886
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00660715  Training extension loss: 0.0013002
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00590319  Training extension loss: 0.00128753
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00664557  Training extension loss: 0.00133307
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00622956  Testing extension loss: 0.00141933
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00559674  Training extension loss: 0.00147315
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00583754  Training extension loss: 0.001374
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00562361  Training extension loss: 0.00137301
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00614162  Training extension loss: 0.00115293
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00571398  Training extension loss: 0.00124353
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00559749  Training extension loss: 0.00115797
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00523383  Testing extension loss: 0.00125378
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00508293  Training extension loss: 0.00121699
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00534211  Training extension loss: 0.00107515
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00518245  Training extension loss: 0.00113902
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00519712  Training extension loss: 0.00118401
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.0046365  Training extension loss: 0.00111298
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00481588  Training extension loss: 0.00100801
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00517096  Training extension loss: 0.00123525
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00501233  Testing extension loss: 0.00121706
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00499055  Training extension loss: 0.00107771
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00430714  Training extension loss: 0.00126962
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00436416  Training extension loss: 0.00120419
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00416256  Training extension loss: 0.00103666
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00479367  Training extension loss: 0.000992293
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00483729  Training extension loss: 0.00122499
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00436634  Training extension loss: 0.000981454
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00471444  Testing extension loss: 0.000994468
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00424773  Training extension loss: 0.00116448
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00416659  Training extension loss: 0.00113456
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00503121  Training extension loss: 0.00125689
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.0036217  Training extension loss: 0.001124
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00425564  Training extension loss: 0.0010652
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00413501  Training extension loss: 0.00100034
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00416886  Testing extension loss: 0.00104923
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00387962  Training extension loss: 0.000957645
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00425279  Training extension loss: 0.000837351
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00407534  Training extension loss: 0.000895212
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00433127  Training extension loss: 0.00122047
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00415229  Training extension loss: 0.00107794
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.00415067  Training extension loss: 0.000961533
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00359919  Training extension loss: 0.00105605
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00506403  Testing extension loss: 0.00101497
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00417887  Training extension loss: 0.00107336
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00388471  Training extension loss: 0.000988803
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00382504  Training extension loss: 0.000851215
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00377253  Training extension loss: 0.00100801
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.0037151  Training extension loss: 0.00101312
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.0033833  Training extension loss: 0.000912833
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00350712  Training extension loss: 0.000842263
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00392315  Testing extension loss: 0.000974606
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.00337653  Training extension loss: 0.000850317
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.00352852  Training extension loss: 0.000903629
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00309782  Training extension loss: 0.00105639
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00367228  Training extension loss: 0.00106371
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.00356246  Training extension loss: 0.00120172
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00366329  Training extension loss: 0.000798656
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00359222  Testing extension loss: 0.000946275
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00347562  Training extension loss: 0.000911799
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.00390163  Training extension loss: 0.00111712
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00366483  Training extension loss: 0.0010097
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00334776  Training extension loss: 0.000862047
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00334655  Training extension loss: 0.000973391
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.00320618  Training extension loss: 0.00084587
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.00345934  Training extension loss: 0.00115339
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.00358818  Testing extension loss: 0.000902365
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.00333245  Training extension loss: 0.000871236
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.00359722  Training extension loss: 0.000872768
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.00362167  Training extension loss: 0.000734245
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.00330122  Training extension loss: 0.00111489
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.00306168  Training extension loss: 0.00107708
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.00321167  Training extension loss: 0.00088522
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-22-2016_16h41m20s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0147659  Training extension loss: 0.00540631
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0151975  Testing extension loss: 0.00558562
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0154496  Training extension loss: 0.00546831
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0148015  Training extension loss: 0.00531843
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0157744  Training extension loss: 0.00527454
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0162615  Training extension loss: 0.00517734
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0155767  Training extension loss: 0.00519391
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0149503  Training extension loss: 0.00482983
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0157596  Testing extension loss: 0.00472447
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0156138  Training extension loss: 0.00436206
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0158704  Training extension loss: 0.00373062
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.0151602  Training extension loss: 0.0028346
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0151129  Training extension loss: 0.00194355
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0147855  Training extension loss: 0.00160093
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.0150342  Training extension loss: 0.00114368
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0143582  Training extension loss: 0.000976045
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.014428  Testing extension loss: 0.00107524
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.0137961  Training extension loss: 0.00111199
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.0147152  Training extension loss: 0.00140398
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.0136176  Training extension loss: 0.000961398
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.0134499  Training extension loss: 0.000982206
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.0132024  Training extension loss: 0.00172294
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.0135734  Training extension loss: 0.00152129
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.0126283  Training extension loss: 0.00100505
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.0133841  Testing extension loss: 0.00139027
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.0123435  Training extension loss: 0.00145854
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.0121239  Training extension loss: 0.00152738
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.0118557  Training extension loss: 0.00158345
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.0114534  Training extension loss: 0.00182545
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.0108744  Training extension loss: 0.00136112
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.0099557  Training extension loss: 0.00143763
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.0106529  Testing extension loss: 0.00122229
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.0095824  Training extension loss: 0.00113111
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.0102693  Training extension loss: 0.00118749
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00918746  Training extension loss: 0.00145818
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00893236  Training extension loss: 0.00142993
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00869165  Training extension loss: 0.00144229
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00778278  Training extension loss: 0.00161965
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00821659  Training extension loss: 0.00137721
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00838183  Testing extension loss: 0.00150282
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00820199  Training extension loss: 0.00155337
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00755435  Training extension loss: 0.00143339
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00744351  Training extension loss: 0.00143807
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00767789  Training extension loss: 0.00136351
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.0065523  Training extension loss: 0.00121673
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00628368  Training extension loss: 0.0015137
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00735162  Training extension loss: 0.00139201
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00686701  Testing extension loss: 0.0010104
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.006683  Training extension loss: 0.00129723
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00622264  Training extension loss: 0.00149696
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00653316  Training extension loss: 0.00132883
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00595854  Training extension loss: 0.00143777
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00594533  Training extension loss: 0.0011347
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00553316  Training extension loss: 0.00137057
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00605319  Testing extension loss: 0.00125886
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00596896  Training extension loss: 0.00118536
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00575134  Training extension loss: 0.00135527
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00612731  Training extension loss: 0.0013602
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.0055131  Training extension loss: 0.00142284
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.0056233  Training extension loss: 0.00154544
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00551673  Training extension loss: 0.00115125
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00579941  Training extension loss: 0.00104767
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00534329  Testing extension loss: 0.00138629
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00549683  Training extension loss: 0.00129572
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00510195  Training extension loss: 0.00136655
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00502967  Training extension loss: 0.00117439
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00555865  Training extension loss: 0.00152671
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00548198  Training extension loss: 0.00130825
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00500766  Training extension loss: 0.00129258
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00473173  Training extension loss: 0.00151768
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00450277  Testing extension loss: 0.00118398
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00440888  Training extension loss: 0.00144424
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00486047  Training extension loss: 0.0012347
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00450618  Training extension loss: 0.00103751
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.00487689  Training extension loss: 0.00145599
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00408597  Training extension loss: 0.00137619
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.004469  Training extension loss: 0.00133908
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00444846  Testing extension loss: 0.00114267
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00511759  Training extension loss: 0.00148591
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00464383  Training extension loss: 0.00105502
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00440387  Training extension loss: 0.00127047
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00387214  Training extension loss: 0.00114893
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00456897  Training extension loss: 0.00125436
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.00407096  Training extension loss: 0.00140922
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00441403  Training extension loss: 0.00125953
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00415407  Testing extension loss: 0.00121589
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00444143  Training extension loss: 0.000952719
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00465523  Training extension loss: 0.00113571
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00435196  Training extension loss: 0.0011351
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00422591  Training extension loss: 0.00108175
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.00441648  Training extension loss: 0.000925075
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.00428279  Training extension loss: 0.000965679
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00447584  Training extension loss: 0.000956536
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00456094  Testing extension loss: 0.00103416
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.00440959  Training extension loss: 0.00131575
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.00422778  Training extension loss: 0.00119718
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00431364  Training extension loss: 0.00118518
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00378924  Training extension loss: 0.00123562
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.00387561  Training extension loss: 0.00110632
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00395758  Training extension loss: 0.00121988
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00419265  Testing extension loss: 0.00112583
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00438842  Training extension loss: 0.00097893
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.003173  Training extension loss: 0.000969638
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00344199  Training extension loss: 0.00109052
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00344669  Training extension loss: 0.000961831
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00406418  Training extension loss: 0.00127635
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.00382479  Training extension loss: 0.00105128
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.00422477  Training extension loss: 0.00121314
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.00374134  Testing extension loss: 0.00105964
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.00340045  Training extension loss: 0.00109999
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.00371149  Training extension loss: 0.00119619
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.003759  Training extension loss: 0.000870066
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.00413848  Training extension loss: 0.00116646
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.00345518  Training extension loss: 0.000913937
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.00363425  Training extension loss: 0.000917426
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-22-2016_18h10m13s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0146092  Training extension loss: 0.00560255
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0160711  Testing extension loss: 0.0055932
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0162824  Training extension loss: 0.00558478
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0149266  Training extension loss: 0.00551704
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0157797  Training extension loss: 0.00548765
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0159319  Training extension loss: 0.00522719
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0153558  Training extension loss: 0.0052757
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0154307  Training extension loss: 0.00516874
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0156349  Testing extension loss: 0.00515605
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0152421  Training extension loss: 0.00534528
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0148044  Training extension loss: 0.00502015
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.0145705  Training extension loss: 0.00485366
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0153703  Training extension loss: 0.00473795
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0152499  Training extension loss: 0.004079
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.0149949  Training extension loss: 0.00345141
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0141162  Training extension loss: 0.00242919
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.0155171  Testing extension loss: 0.00230218
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.0153087  Training extension loss: 0.00219086
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.0152517  Training extension loss: 0.00178592
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.0150348  Training extension loss: 0.0014013
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.013928  Training extension loss: 0.00136852
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.0148429  Training extension loss: 0.00131509
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.014167  Training extension loss: 0.00115232
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.0139571  Training extension loss: 0.00132603
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.015025  Testing extension loss: 0.00156877
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.0131213  Training extension loss: 0.0014126
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.013359  Training extension loss: 0.0014419
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.0135089  Training extension loss: 0.00122701
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.0137197  Training extension loss: 0.0017728
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.0128933  Training extension loss: 0.00156309
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.0116614  Training extension loss: 0.00157653
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.0121542  Testing extension loss: 0.00157116
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.0126225  Training extension loss: 0.00149258
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.0121192  Training extension loss: 0.00138032
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.0112724  Training extension loss: 0.001274
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.0102811  Training extension loss: 0.0018697
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.01037  Training extension loss: 0.00143612
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00992138  Training extension loss: 0.00164599
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.0102867  Training extension loss: 0.00149439
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00894078  Testing extension loss: 0.00169325
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00920798  Training extension loss: 0.00157762
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00970978  Training extension loss: 0.00139248
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00958504  Training extension loss: 0.00131316
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.009228  Training extension loss: 0.00159371
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00876778  Training extension loss: 0.00155821
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00942592  Training extension loss: 0.00146468
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00837934  Training extension loss: 0.00149238
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00834047  Testing extension loss: 0.00155476
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.0086838  Training extension loss: 0.00169214
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00836807  Training extension loss: 0.00147808
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00737219  Training extension loss: 0.00144976
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00734455  Training extension loss: 0.00153952
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00680902  Training extension loss: 0.00148211
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.0074502  Training extension loss: 0.00164384
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00723052  Testing extension loss: 0.00151495
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00761828  Training extension loss: 0.00126071
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00720945  Training extension loss: 0.00162483
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00734327  Training extension loss: 0.00136866
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00639147  Training extension loss: 0.00134034
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00626079  Training extension loss: 0.00147776
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00644165  Training extension loss: 0.00136734
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00617648  Training extension loss: 0.00145753
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.0060128  Testing extension loss: 0.00160311
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00563796  Training extension loss: 0.00153698
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00645795  Training extension loss: 0.00142911
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00618099  Training extension loss: 0.00141339
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00595065  Training extension loss: 0.00168291
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00515779  Training extension loss: 0.00139841
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00561649  Training extension loss: 0.00168885
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00542869  Training extension loss: 0.00158866
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00538967  Testing extension loss: 0.00139972
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00555227  Training extension loss: 0.00146559
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00541905  Training extension loss: 0.0015744
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00522016  Training extension loss: 0.00133185
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.00521607  Training extension loss: 0.00123307
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00539195  Training extension loss: 0.00145759
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00449373  Training extension loss: 0.00168146
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00463096  Testing extension loss: 0.00148377
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00572777  Training extension loss: 0.00112013
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00535039  Training extension loss: 0.00134524
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00427472  Training extension loss: 0.00114862
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00475377  Training extension loss: 0.0012634
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00515204  Training extension loss: 0.00130779
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.00464238  Training extension loss: 0.00143292
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00538674  Training extension loss: 0.00109455
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00467482  Testing extension loss: 0.00113614
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00500699  Training extension loss: 0.00123126
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00389661  Training extension loss: 0.00114993
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00412342  Training extension loss: 0.00135431
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00456386  Training extension loss: 0.00125185
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.00433649  Training extension loss: 0.00140701
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.00450419  Training extension loss: 0.0013358
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00359126  Training extension loss: 0.00105784
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00436047  Testing extension loss: 0.00123452
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.00320509  Training extension loss: 0.00130503
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.00409835  Training extension loss: 0.00121993
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00408104  Training extension loss: 0.00126313
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00406538  Training extension loss: 0.00142858
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.00530875  Training extension loss: 0.000932095
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00415051  Training extension loss: 0.000969907
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00424145  Testing extension loss: 0.00111914
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00421571  Training extension loss: 0.00117823
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.00417885  Training extension loss: 0.000983998
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00397728  Training extension loss: 0.000864465
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.0042631  Training extension loss: 0.00106354
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00425287  Training extension loss: 0.00114294
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.0040876  Training extension loss: 0.000991981
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.00350436  Training extension loss: 0.00130272
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.00373242  Testing extension loss: 0.00127126
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.00394023  Training extension loss: 0.00107702
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.00413891  Training extension loss: 0.00101517
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.00362027  Training extension loss: 0.000858177
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.00418939  Training extension loss: 0.000968809
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.00330258  Training extension loss: 0.00106733
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.00323612  Training extension loss: 0.000994198
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-22-2016_19h37m30s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.00321952  Training extension loss: 0.00107597
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.00356083  Testing extension loss: 0.000988347
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.00369134  Training extension loss: 0.00100942
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.00347833  Training extension loss: 0.00097129
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.00341515  Training extension loss: 0.00102556
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.00341376  Training extension loss: 0.00119704
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.00397709  Training extension loss: 0.000984967
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.00328588  Training extension loss: 0.000988666
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.00406057  Testing extension loss: 0.000887973
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.00311496  Training extension loss: 0.00100845
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.00339991  Training extension loss: 0.00102092
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.00372636  Training extension loss: 0.000777179
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0034041  Training extension loss: 0.000923726
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.00333681  Training extension loss: 0.000985491
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.00324852  Training extension loss: 0.000743653
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.00351459  Training extension loss: 0.000745906
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.0036622  Testing extension loss: 0.000740062
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.00360184  Training extension loss: 0.0011562
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00364045  Training extension loss: 0.000940366
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00345353  Training extension loss: 0.000954219
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00331199  Training extension loss: 0.000908014
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00305248  Training extension loss: 0.000845774
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00329086  Training extension loss: 0.00123107
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00312611  Training extension loss: 0.000746797
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00379816  Testing extension loss: 0.000839366
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00312251  Training extension loss: 0.000961516
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.0038614  Training extension loss: 0.00103929
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00327882  Training extension loss: 0.00092402
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.0034223  Training extension loss: 0.000840156
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00356261  Training extension loss: 0.00122309
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00344301  Training extension loss: 0.000836263
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00366057  Testing extension loss: 0.000835378
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00324955  Training extension loss: 0.000879735
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00312033  Training extension loss: 0.00121146
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.0034095  Training extension loss: 0.000808108
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00295592  Training extension loss: 0.00108955
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00326556  Training extension loss: 0.000935223
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00296234  Training extension loss: 0.000633334
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00336201  Training extension loss: 0.000961666
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00329958  Testing extension loss: 0.000904415
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.0032558  Training extension loss: 0.000756751
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00332862  Training extension loss: 0.0010676
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00309109  Training extension loss: 0.000824222
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00354074  Training extension loss: 0.00108163
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00310847  Training extension loss: 0.00100692
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00304244  Training extension loss: 0.0010669
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00290848  Training extension loss: 0.000794169
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00333512  Testing extension loss: 0.00107784
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00312967  Training extension loss: 0.000789718
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00247533  Training extension loss: 0.00087607
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00308772  Training extension loss: 0.00084937
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00335456  Training extension loss: 0.000980118
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00304182  Training extension loss: 0.000892391
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00285014  Training extension loss: 0.000778302
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00317384  Testing extension loss: 0.00076436
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00279102  Training extension loss: 0.000952478
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00362566  Training extension loss: 0.00102533
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00358937  Training extension loss: 0.000949532
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.002615  Training extension loss: 0.000860528
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00252826  Training extension loss: 0.00104482
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00264577  Training extension loss: 0.000766682
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00257213  Training extension loss: 0.000700108
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00283403  Testing extension loss: 0.00120497
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00295581  Training extension loss: 0.000863777
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00284833  Training extension loss: 0.000923378
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00301005  Training extension loss: 0.000858058
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00344951  Training extension loss: 0.000860033
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00267284  Training extension loss: 0.000647196
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00280166  Training extension loss: 0.000997878
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00266073  Training extension loss: 0.000770416
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.0028652  Testing extension loss: 0.00103472
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00299976  Training extension loss: 0.000812403
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.0024838  Training extension loss: 0.00103841
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00270604  Training extension loss: 0.000867252
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.00292707  Training extension loss: 0.000682387
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00283346  Training extension loss: 0.000686494
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00276612  Training extension loss: 0.000830738
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_06-23-2016_09h27m56s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0141562  Training extension loss: 0.00423139
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0143326  Testing extension loss: 0.00412717
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0137013  Training extension loss: 0.00423652
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0126553  Training extension loss: 0.00442437
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0137746  Training extension loss: 0.00419477
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0134343  Training extension loss: 0.00441208
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.013544  Training extension loss: 0.0041363
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.013388  Training extension loss: 0.00409239
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0144838  Testing extension loss: 0.00368985
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0125452  Training extension loss: 0.00395572
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0131892  Training extension loss: 0.00390378
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.0126253  Training extension loss: 0.00362669
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0133625  Training extension loss: 0.0034514
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0126439  Training extension loss: 0.00307974
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.0120728  Training extension loss: 0.00284648
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0118928  Training extension loss: 0.00219364
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.0125359  Testing extension loss: 0.00209526
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.0112781  Training extension loss: 0.00188911
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.0111503  Training extension loss: 0.00169543
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.0109018  Training extension loss: 0.00164431
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.0110962  Training extension loss: 0.00166224
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00984949  Training extension loss: 0.00160359
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.0109353  Training extension loss: 0.00174374
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.0105796  Training extension loss: 0.00154886
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.0108676  Testing extension loss: 0.00181331
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00989471  Training extension loss: 0.00178637
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00921442  Training extension loss: 0.00166439
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00963704  Training extension loss: 0.00166676
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00976296  Training extension loss: 0.00175626
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00860551  Training extension loss: 0.00158103
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00905224  Training extension loss: 0.00186781
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00942094  Testing extension loss: 0.00180244
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00880765  Training extension loss: 0.00173711
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00842293  Training extension loss: 0.00182888
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00699988  Training extension loss: 0.00154088
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00696702  Training extension loss: 0.00173919
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00760251  Training extension loss: 0.00154477
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00725389  Training extension loss: 0.00185029
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00769417  Training extension loss: 0.00164878
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00725393  Testing extension loss: 0.00172871
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00784902  Training extension loss: 0.00181735
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00742139  Training extension loss: 0.00168833
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.0065342  Training extension loss: 0.00157546
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00730785  Training extension loss: 0.00170761
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00714551  Training extension loss: 0.00176865
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00690854  Training extension loss: 0.0017007
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00658427  Training extension loss: 0.0015401
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00788824  Testing extension loss: 0.00160249
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00663408  Training extension loss: 0.00165857
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00733676  Training extension loss: 0.0017432
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.0075155  Training extension loss: 0.00167857
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00701952  Training extension loss: 0.00158089
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00663371  Training extension loss: 0.00168902
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00750243  Training extension loss: 0.0017448
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00664324  Testing extension loss: 0.00166544
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00733701  Training extension loss: 0.00160093
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00635948  Training extension loss: 0.00156969
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00666132  Training extension loss: 0.00169836
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00610319  Training extension loss: 0.00156364
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00679764  Training extension loss: 0.0016107
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00629246  Training extension loss: 0.00164837
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00623294  Training extension loss: 0.00162389
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00627375  Testing extension loss: 0.00176337
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00595959  Training extension loss: 0.00157342
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.0065005  Training extension loss: 0.0014887
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00611207  Training extension loss: 0.00152362
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00648679  Training extension loss: 0.00146951
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00662798  Training extension loss: 0.00153229
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00563041  Training extension loss: 0.00142107
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00582387  Training extension loss: 0.00144877
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00580704  Testing extension loss: 0.00163604
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00550571  Training extension loss: 0.00138592
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00549103  Training extension loss: 0.00163917
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00612567  Training extension loss: 0.00154355
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.00593876  Training extension loss: 0.00156506
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00669986  Training extension loss: 0.00151192
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00520647  Training extension loss: 0.00143301
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00640529  Testing extension loss: 0.00158931
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00606991  Training extension loss: 0.00143151
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00512914  Training extension loss: 0.00134953
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00581736  Training extension loss: 0.00151141
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00545626  Training extension loss: 0.00135852
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00611326  Training extension loss: 0.00153345
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.00632517  Training extension loss: 0.00137701
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00587389  Training extension loss: 0.00141699
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00606635  Testing extension loss: 0.00153131
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00516262  Training extension loss: 0.00140726
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00521832  Training extension loss: 0.00136032
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00550309  Training extension loss: 0.0014286
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00561765  Training extension loss: 0.00143161
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.00528188  Training extension loss: 0.00144609
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.00570736  Training extension loss: 0.00142163
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00589705  Training extension loss: 0.00148475
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00543832  Testing extension loss: 0.00130924
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.00467309  Training extension loss: 0.00128533
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.00580072  Training extension loss: 0.00145323
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00541259  Training extension loss: 0.00139234
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00570956  Training extension loss: 0.00120868
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.00538969  Training extension loss: 0.00128319
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00528687  Training extension loss: 0.00130487
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00556031  Testing extension loss: 0.0013668
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00511894  Training extension loss: 0.0013544
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.00490642  Training extension loss: 0.00129169
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00514479  Training extension loss: 0.00132439
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00506546  Training extension loss: 0.0014607
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00536918  Training extension loss: 0.00144815
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.0050227  Training extension loss: 0.0012463
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.0043486  Training extension loss: 0.00136368
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.00584808  Testing extension loss: 0.00148896
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.00474323  Training extension loss: 0.00126137
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.00534992  Training extension loss: 0.0012342
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.00475937  Training extension loss: 0.00132779
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.00532056  Training extension loss: 0.0012938
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.00538337  Training extension loss: 0.00129137
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.00547867  Training extension loss: 0.00123991
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_07-01-2016_14h23m05s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.00529237  Training extension loss: 0.00126687
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.00512296  Testing extension loss: 0.00129309
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.00479788  Training extension loss: 0.00130254
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.00486793  Training extension loss: 0.00126391
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0046164  Training extension loss: 0.00123082
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.00473111  Training extension loss: 0.00122512
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0050622  Training extension loss: 0.00135077
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.00453599  Training extension loss: 0.001169
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.00530923  Testing extension loss: 0.00129927
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.00483447  Training extension loss: 0.0012934
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.00486244  Training extension loss: 0.00135409
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.00409647  Training extension loss: 0.00119885
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.00389758  Training extension loss: 0.00119657
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.00494599  Training extension loss: 0.00115414
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.00492506  Training extension loss: 0.00133959
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.00526655  Training extension loss: 0.00135919
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.00446003  Testing extension loss: 0.00131124
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.00429966  Training extension loss: 0.00121625
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00439749  Training extension loss: 0.00113604
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00516521  Training extension loss: 0.00118692
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00472481  Training extension loss: 0.00105801
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00473642  Training extension loss: 0.00115431
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00451274  Training extension loss: 0.00107104
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00420467  Training extension loss: 0.00114856
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00488963  Testing extension loss: 0.00130843
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00454509  Training extension loss: 0.00121643
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00449924  Training extension loss: 0.0012724
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00411723  Training extension loss: 0.00122695
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00488472  Training extension loss: 0.00129819
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00490147  Training extension loss: 0.00117573
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00428612  Training extension loss: 0.00104986
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00537873  Testing extension loss: 0.0012264
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00472478  Training extension loss: 0.00114325
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.0046069  Training extension loss: 0.00121851
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.0043651  Training extension loss: 0.00126217
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.0044203  Training extension loss: 0.00116587
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00450695  Training extension loss: 0.00116283
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00401858  Training extension loss: 0.000998179
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00419072  Training extension loss: 0.00104154
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00452464  Testing extension loss: 0.0012434
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00414919  Training extension loss: 0.00125854
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00396478  Training extension loss: 0.00108301
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00396168  Training extension loss: 0.00113719
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00417385  Training extension loss: 0.0011232
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00415637  Training extension loss: 0.00104654
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00438907  Training extension loss: 0.0010171
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00421133  Training extension loss: 0.00108705
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00467835  Testing extension loss: 0.00123807
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00424725  Training extension loss: 0.00115078
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00418729  Training extension loss: 0.00110819
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00431694  Training extension loss: 0.00111705
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.0041722  Training extension loss: 0.00115396
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00422383  Training extension loss: 0.0011591
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00386637  Training extension loss: 0.000985837
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00478391  Testing extension loss: 0.00110046
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00388692  Training extension loss: 0.00102422
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00415657  Training extension loss: 0.000976811
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00472334  Training extension loss: 0.000993454
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00413428  Training extension loss: 0.000975564
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00427259  Training extension loss: 0.00105529
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00380824  Training extension loss: 0.00112975
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.0043596  Training extension loss: 0.00105697
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00432673  Testing extension loss: 0.00113899
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00374385  Training extension loss: 0.000920064
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00434596  Training extension loss: 0.00115689
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00386608  Training extension loss: 0.00108376
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00383334  Training extension loss: 0.00116071
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.0044939  Training extension loss: 0.00116732
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00394877  Training extension loss: 0.000992239
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00407542  Training extension loss: 0.00105119
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00412237  Testing extension loss: 0.00108589
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00362226  Training extension loss: 0.00101576
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00362951  Training extension loss: 0.00104707
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00403556  Training extension loss: 0.00109385
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.0037601  Training extension loss: 0.00106256
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00370606  Training extension loss: 0.00103326
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.0040109  Training extension loss: 0.000991198
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_07-01-2016_14h31m28s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0135464  Training extension loss: 0.00437245
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0142152  Testing extension loss: 0.00414263
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0127069  Training extension loss: 0.00442066
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0130145  Training extension loss: 0.00431697
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0130542  Training extension loss: 0.00437306
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0131877  Training extension loss: 0.00403352
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0132565  Training extension loss: 0.00391718
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.013251  Training extension loss: 0.00361649
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0132923  Testing extension loss: 0.00330802
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0124556  Training extension loss: 0.00321461
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0127846  Training extension loss: 0.00244982
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.0117995  Training extension loss: 0.00184413
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0117815  Training extension loss: 0.00175558
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0106522  Training extension loss: 0.00180923
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.0108527  Training extension loss: 0.00160639
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0108099  Training extension loss: 0.0016934
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.010491  Testing extension loss: 0.00180151
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.00892965  Training extension loss: 0.00161544
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00979963  Training extension loss: 0.00163199
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00884544  Training extension loss: 0.00162159
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00950998  Training extension loss: 0.00175088
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.0082455  Training extension loss: 0.00175145
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00850451  Training extension loss: 0.0017327
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00816861  Training extension loss: 0.00158501
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00877863  Testing extension loss: 0.00182495
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00791442  Training extension loss: 0.00186132
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00741057  Training extension loss: 0.00175444
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00748985  Training extension loss: 0.00169517
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00722626  Training extension loss: 0.00178476
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00670002  Training extension loss: 0.00185093
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00608218  Training extension loss: 0.00178408
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00709657  Testing extension loss: 0.00169322
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.0065627  Training extension loss: 0.00167407
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00693065  Training extension loss: 0.00170853
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00653632  Training extension loss: 0.00168934
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00666776  Training extension loss: 0.00173805
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00593837  Training extension loss: 0.00159014
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00696876  Training extension loss: 0.00164251
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00648658  Training extension loss: 0.00160587
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00621779  Testing extension loss: 0.00151136
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00573425  Training extension loss: 0.00160439
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00582521  Training extension loss: 0.0016225
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00575773  Training extension loss: 0.00162252
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00587616  Training extension loss: 0.00159433
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00578229  Training extension loss: 0.00144127
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00529109  Training extension loss: 0.00152257
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00526364  Training extension loss: 0.0015194
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00645909  Testing extension loss: 0.00139157
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00566551  Training extension loss: 0.00145684
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00567139  Training extension loss: 0.00143221
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00573417  Training extension loss: 0.00157091
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00583372  Training extension loss: 0.00154741
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00516481  Training extension loss: 0.00154773
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00555674  Training extension loss: 0.00137232
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00518736  Testing extension loss: 0.00138659
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00540743  Training extension loss: 0.00142486
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00509045  Training extension loss: 0.0014841
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00509682  Training extension loss: 0.00129782
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00539099  Training extension loss: 0.00143641
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00505728  Training extension loss: 0.00125894
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00526166  Training extension loss: 0.00140553
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00496871  Training extension loss: 0.00131575
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00563004  Testing extension loss: 0.00126154
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00460771  Training extension loss: 0.00128718
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.0051159  Training extension loss: 0.00131477
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00522887  Training extension loss: 0.00127086
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00445171  Training extension loss: 0.00140993
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00473356  Training extension loss: 0.00144377
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00517608  Training extension loss: 0.0013619
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.0047508  Training extension loss: 0.0012787
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00510612  Testing extension loss: 0.00128878
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00479087  Training extension loss: 0.00138652
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.0046741  Training extension loss: 0.00130096
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00468379  Training extension loss: 0.00129268
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.00510486  Training extension loss: 0.00117281
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00442501  Training extension loss: 0.00118841
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00569125  Training extension loss: 0.00127705
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00460335  Testing extension loss: 0.00128468
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00448852  Training extension loss: 0.00120068
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00477678  Training extension loss: 0.00121644
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00477328  Training extension loss: 0.00121376
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00438347  Training extension loss: 0.00124324
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00490764  Training extension loss: 0.00124832
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.00512297  Training extension loss: 0.00111898
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00421104  Training extension loss: 0.00121643
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00499315  Testing extension loss: 0.00127273
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00503887  Training extension loss: 0.00105312
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.0042866  Training extension loss: 0.00114991
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00458246  Training extension loss: 0.00118592
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00449993  Training extension loss: 0.0010927
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.00426504  Training extension loss: 0.00121298
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.00416183  Training extension loss: 0.00116401
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00455557  Training extension loss: 0.00105151
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00455799  Testing extension loss: 0.00107932
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.00449943  Training extension loss: 0.00111248
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.00423384  Training extension loss: 0.00106596
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00432507  Training extension loss: 0.00105503
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00418329  Training extension loss: 0.00106369
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.00385936  Training extension loss: 0.001033
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00363512  Training extension loss: 0.000991532
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00449726  Testing extension loss: 0.00102223
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00390657  Training extension loss: 0.00107412
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.00436798  Training extension loss: 0.00105555
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.0045355  Training extension loss: 0.00116201
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00417188  Training extension loss: 0.00111278
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00433222  Training extension loss: 0.00111435
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.00449794  Training extension loss: 0.00102109
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.0039127  Training extension loss: 0.00104494
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.00467879  Testing extension loss: 0.000983779
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.00380835  Training extension loss: 0.00101538
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.00438519  Training extension loss: 0.00106508
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.00415349  Training extension loss: 0.000993142
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.00385561  Training extension loss: 0.000976007
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.0037267  Training extension loss: 0.000985196
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.00484162  Training extension loss: 0.000922409
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_07-01-2016_16h15m32s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.00356795  Training extension loss: 0.000999572
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.00387263  Testing extension loss: 0.000956469
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.00398325  Training extension loss: 0.00097203
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.00473987  Training extension loss: 0.000904182
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.00444394  Training extension loss: 0.00103117
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.00418931  Training extension loss: 0.00102505
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.00398242  Training extension loss: 0.000880422
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.00404163  Training extension loss: 0.000886963
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.00386228  Testing extension loss: 0.0009803
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.00413042  Training extension loss: 0.00104456
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.00391558  Training extension loss: 0.000832797
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.00366295  Training extension loss: 0.000860647
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.00392941  Training extension loss: 0.000961066
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.00370453  Training extension loss: 0.000948108
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.00363617  Training extension loss: 0.00086895
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.00371843  Training extension loss: 0.000918467
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.00373002  Testing extension loss: 0.000929752
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.00357621  Training extension loss: 0.00098991
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00408403  Training extension loss: 0.000904175
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00395026  Training extension loss: 0.000933746
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.0037108  Training extension loss: 0.000982595
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00422465  Training extension loss: 0.00092946
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00403539  Training extension loss: 0.000818675
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00356949  Training extension loss: 0.000939491
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00435286  Testing extension loss: 0.000934805
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00350704  Training extension loss: 0.00085838
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00421697  Training extension loss: 0.000823948
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00400825  Training extension loss: 0.000892024
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00358123  Training extension loss: 0.00090273
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00313458  Training extension loss: 0.000970978
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00351872  Training extension loss: 0.00093053
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00385198  Testing extension loss: 0.000850505
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.0031259  Training extension loss: 0.000928616
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00366301  Training extension loss: 0.000922938
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00339955  Training extension loss: 0.000867817
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00328655  Training extension loss: 0.000925433
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00361917  Training extension loss: 0.000861235
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00332052  Training extension loss: 0.000893959
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.0039187  Training extension loss: 0.000881121
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00388523  Testing extension loss: 0.000853114
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00365426  Training extension loss: 0.00085684
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00348597  Training extension loss: 0.000818373
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00291876  Training extension loss: 0.000834782
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00369711  Training extension loss: 0.000842933
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00340151  Training extension loss: 0.00071144
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00364081  Training extension loss: 0.000846992
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00348847  Training extension loss: 0.000738329
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00355813  Testing extension loss: 0.000858002
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00355979  Training extension loss: 0.000810787
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00283473  Training extension loss: 0.000793214
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.0031855  Training extension loss: 0.000682181
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00314866  Training extension loss: 0.000826066
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00357406  Training extension loss: 0.000691271
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00339917  Training extension loss: 0.00075095
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00333836  Testing extension loss: 0.000789146
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00292077  Training extension loss: 0.000778213
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00323777  Training extension loss: 0.000854892
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.0035948  Training extension loss: 0.000764254
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00336542  Training extension loss: 0.000738854
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00305703  Training extension loss: 0.000843967
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00304704  Training extension loss: 0.000641263
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00307425  Training extension loss: 0.000740734
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00348475  Testing extension loss: 0.000877491
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00321744  Training extension loss: 0.000742562
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00329276  Training extension loss: 0.000727478
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00259331  Training extension loss: 0.000679325
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.0032093  Training extension loss: 0.000750484
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00325523  Training extension loss: 0.000799635
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00283279  Training extension loss: 0.000636251
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00365432  Training extension loss: 0.000693705
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00352615  Testing extension loss: 0.000770892
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00328646  Training extension loss: 0.000832471
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00353873  Training extension loss: 0.000673801
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00329906  Training extension loss: 0.000744201
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.0028961  Training extension loss: 0.00076397
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00349904  Training extension loss: 0.00077818
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00333063  Training extension loss: 0.000797811
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_07-01-2016_16h24m28s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0131928  Training extension loss: 0.0043341
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0133919  Testing extension loss: 0.00440355
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0124874  Training extension loss: 0.00435052
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0141995  Training extension loss: 0.00410693
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0128164  Training extension loss: 0.0042621
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0129249  Training extension loss: 0.00402543
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0135497  Training extension loss: 0.00352492
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0128859  Training extension loss: 0.00309367
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.011459  Testing extension loss: 0.00234327
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0118716  Training extension loss: 0.00211094
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0120347  Training extension loss: 0.00172923
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.0102487  Training extension loss: 0.00151931
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0106299  Training extension loss: 0.00167649
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0102858  Training extension loss: 0.00143095
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.00965941  Training extension loss: 0.00166048
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.00823517  Training extension loss: 0.00145598
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.00940001  Testing extension loss: 0.00152196
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.00945337  Training extension loss: 0.00154105
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00914444  Training extension loss: 0.00173914
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00891062  Training extension loss: 0.00156503
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00892009  Training extension loss: 0.00158933
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00826841  Training extension loss: 0.00165285
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00737597  Training extension loss: 0.00156685
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00769934  Training extension loss: 0.0016836
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00719021  Testing extension loss: 0.00156048
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00668473  Training extension loss: 0.00161842
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00675649  Training extension loss: 0.00167077
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00680061  Training extension loss: 0.00174642
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00712969  Training extension loss: 0.0014474
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.0061284  Training extension loss: 0.00154619
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00639908  Training extension loss: 0.00167749
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00612827  Testing extension loss: 0.00147413
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00577123  Training extension loss: 0.00145727
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00632545  Training extension loss: 0.00157841
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00691927  Training extension loss: 0.0016236
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00653645  Training extension loss: 0.00156078
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00617957  Training extension loss: 0.00163694
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00544308  Training extension loss: 0.00146378
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00650655  Training extension loss: 0.0016057
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00567853  Testing extension loss: 0.00141633
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00557911  Training extension loss: 0.00149204
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00543695  Training extension loss: 0.00143139
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.0061018  Training extension loss: 0.00146157
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00588111  Training extension loss: 0.00160159
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00592954  Training extension loss: 0.00138121
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00530995  Training extension loss: 0.00140595
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00561034  Training extension loss: 0.001376
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00575299  Testing extension loss: 0.00146733
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00500722  Training extension loss: 0.00134986
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00621217  Training extension loss: 0.00146201
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00557294  Training extension loss: 0.00140063
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00555727  Training extension loss: 0.00143057
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.0051989  Training extension loss: 0.00132163
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00591161  Training extension loss: 0.00135281
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00517911  Testing extension loss: 0.00140857
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00536629  Training extension loss: 0.00141426
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00523471  Training extension loss: 0.00128441
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00556276  Training extension loss: 0.00138221
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00620667  Training extension loss: 0.00141412
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00560749  Training extension loss: 0.00118344
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00453829  Training extension loss: 0.00122738
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00541878  Training extension loss: 0.00132866
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00477157  Testing extension loss: 0.00131296
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00525794  Training extension loss: 0.00131913
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00481015  Training extension loss: 0.00125081
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00466199  Training extension loss: 0.00126649
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00455432  Training extension loss: 0.00129462
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00569522  Training extension loss: 0.0013397
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00454635  Training extension loss: 0.00118386
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00480901  Training extension loss: 0.00125047
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00569622  Testing extension loss: 0.00120475
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00537889  Training extension loss: 0.00137821
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00501182  Training extension loss: 0.00124614
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00491264  Training extension loss: 0.00114625
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.00501815  Training extension loss: 0.00128596
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.0040701  Training extension loss: 0.00120386
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00562017  Training extension loss: 0.00126524
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00489807  Testing extension loss: 0.00116806
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00455566  Training extension loss: 0.0011439
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00453825  Training extension loss: 0.00113801
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00429892  Training extension loss: 0.00113616
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00426634  Training extension loss: 0.00120034
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00460156  Training extension loss: 0.00103444
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.00394668  Training extension loss: 0.00112609
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00467807  Training extension loss: 0.00105614
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00514759  Testing extension loss: 0.00101499
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00431066  Training extension loss: 0.00104379
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00467995  Training extension loss: 0.00108171
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00437006  Training extension loss: 0.00117563
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00461129  Training extension loss: 0.00109838
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.00443706  Training extension loss: 0.00108826
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.00414012  Training extension loss: 0.00110569
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00389574  Training extension loss: 0.00097789
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00460633  Testing extension loss: 0.00109192
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.00418633  Training extension loss: 0.00110169
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.00387181  Training extension loss: 0.000981609
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00354351  Training extension loss: 0.000944774
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00411347  Training extension loss: 0.001109
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.00392869  Training extension loss: 0.000957127
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00452733  Training extension loss: 0.00108181
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00500144  Testing extension loss: 0.000983075
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00459195  Training extension loss: 0.000957801
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.00367487  Training extension loss: 0.00102984
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00372922  Training extension loss: 0.00105351
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00402537  Training extension loss: 0.00106622
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.0038941  Training extension loss: 0.000985648
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.00467505  Training extension loss: 0.000943676
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.00412377  Training extension loss: 0.000999198
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.00418429  Testing extension loss: 0.00103032
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.00354845  Training extension loss: 0.000969474
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.00369001  Training extension loss: 0.000959271
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.00370695  Training extension loss: 0.000978533
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.00377271  Training extension loss: 0.000938717
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.00423216  Training extension loss: 0.000979899
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.00403008  Training extension loss: 0.00094846
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_07-05-2016_14h17m40s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0124115  Training extension loss: 0.00454472
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0134475  Testing extension loss: 0.00455592
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0138798  Training extension loss: 0.00439244
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0131818  Training extension loss: 0.00429215
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0131942  Training extension loss: 0.00428047
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0144213  Training extension loss: 0.00410289
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0130173  Training extension loss: 0.00407258
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0127191  Training extension loss: 0.00392745
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0133083  Testing extension loss: 0.00387284
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0132958  Training extension loss: 0.00366094
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0135848  Training extension loss: 0.00366909
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.0126289  Training extension loss: 0.00336868
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0131832  Training extension loss: 0.00292732
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0121543  Training extension loss: 0.00269575
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.0123982  Training extension loss: 0.00196454
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0118503  Training extension loss: 0.00166175
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.0116727  Testing extension loss: 0.00166842
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.0111066  Training extension loss: 0.00163815
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00983787  Training extension loss: 0.00147102
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.0109112  Training extension loss: 0.00171433
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.0101912  Training extension loss: 0.00156678
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00987739  Training extension loss: 0.00165852
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.0100752  Training extension loss: 0.00171014
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00913241  Training extension loss: 0.00162914
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00913342  Testing extension loss: 0.00167193
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00867215  Training extension loss: 0.00177042
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00873809  Training extension loss: 0.00161456
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00865934  Training extension loss: 0.0017377
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00793597  Training extension loss: 0.0016784
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00715744  Training extension loss: 0.00160545
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00671213  Training extension loss: 0.00153158
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00747699  Testing extension loss: 0.00152637
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00640696  Training extension loss: 0.00152937
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00712133  Training extension loss: 0.00172203
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00678583  Training extension loss: 0.00157286
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00647307  Training extension loss: 0.00173062
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00608651  Training extension loss: 0.00161556
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00652012  Training extension loss: 0.00151882
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.006102  Training extension loss: 0.00151828
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00619748  Testing extension loss: 0.00153692
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00567517  Training extension loss: 0.00150433
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00640368  Training extension loss: 0.00149823
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00577279  Training extension loss: 0.00142827
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00565031  Training extension loss: 0.0015276
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00575435  Training extension loss: 0.00146371
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00571369  Training extension loss: 0.00140665
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.0053318  Training extension loss: 0.00143156
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00595365  Testing extension loss: 0.00142314
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00587895  Training extension loss: 0.00135045
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00478066  Training extension loss: 0.00146
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.0049633  Training extension loss: 0.00131449
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00498188  Training extension loss: 0.00143637
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00466297  Training extension loss: 0.00133997
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00532143  Training extension loss: 0.00145109
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00586674  Testing extension loss: 0.00144949
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00525129  Training extension loss: 0.00124201
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00464071  Training extension loss: 0.00129261
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00541741  Training extension loss: 0.0013971
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00519702  Training extension loss: 0.00131179
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00455843  Training extension loss: 0.00120692
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00474855  Training extension loss: 0.00119731
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00514914  Training extension loss: 0.00133529
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00506053  Testing extension loss: 0.00122747
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00464593  Training extension loss: 0.00115035
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00507966  Training extension loss: 0.00133969
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00426295  Training extension loss: 0.00116872
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00469173  Training extension loss: 0.0012287
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00495541  Training extension loss: 0.00119519
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00431527  Training extension loss: 0.00114718
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00391656  Training extension loss: 0.00115526
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.0049108  Testing extension loss: 0.00120919
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.0043341  Training extension loss: 0.00116352
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00478011  Training extension loss: 0.00119436
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00422976  Training extension loss: 0.00111792
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.00420618  Training extension loss: 0.00135842
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00410406  Training extension loss: 0.00115052
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00433619  Training extension loss: 0.00124404
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00497966  Testing extension loss: 0.00112581
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00442773  Training extension loss: 0.00117534
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00441669  Training extension loss: 0.00111474
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00427962  Training extension loss: 0.0011547
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00393772  Training extension loss: 0.00106825
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00421025  Training extension loss: 0.00108594
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.00404247  Training extension loss: 0.00103327
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.0035751  Training extension loss: 0.00108451
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00363151  Testing extension loss: 0.00110407
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00375657  Training extension loss: 0.000996413
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00404293  Training extension loss: 0.00113089
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00385801  Training extension loss: 0.00102728
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00346003  Training extension loss: 0.000978962
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.00371892  Training extension loss: 0.00103527
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.00405008  Training extension loss: 0.00101756
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00391712  Training extension loss: 0.000849642
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00437774  Testing extension loss: 0.000991887
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.00393326  Training extension loss: 0.00109978
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.00303524  Training extension loss: 0.000967826
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00340337  Training extension loss: 0.000937999
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00356831  Training extension loss: 0.00100863
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.00378627  Training extension loss: 0.000987475
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00398118  Training extension loss: 0.00103717
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00388991  Testing extension loss: 0.00100141
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.0036524  Training extension loss: 0.000905372
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.00337083  Training extension loss: 0.000902411
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00329501  Training extension loss: 0.000857151
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00343208  Training extension loss: 0.000964273
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00290297  Training extension loss: 0.000844154
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.00356339  Training extension loss: 0.00094772
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.00407308  Training extension loss: 0.000913861
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.00387384  Testing extension loss: 0.00090868
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.00404378  Training extension loss: 0.000902173
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.00371451  Training extension loss: 0.000918253
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.0029876  Training extension loss: 0.000839014
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.00342563  Training extension loss: 0.000902964
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.00302967  Training extension loss: 0.000832865
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.00333886  Training extension loss: 0.000867449
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_07-05-2016_15h26m12s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0136359  Training extension loss: 0.00449843
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.014208  Testing extension loss: 0.00430943
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0131222  Training extension loss: 0.00442443
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.0133313  Training extension loss: 0.00428205
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0128216  Training extension loss: 0.00456321
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0135699  Training extension loss: 0.00410527
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0127577  Training extension loss: 0.004017
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.013949  Training extension loss: 0.00387388
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0139869  Testing extension loss: 0.00365529
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0133895  Training extension loss: 0.00344714
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0130057  Training extension loss: 0.00337278
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.0131458  Training extension loss: 0.00245799
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0112381  Training extension loss: 0.00189475
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.0113298  Training extension loss: 0.00155265
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.010777  Training extension loss: 0.0015686
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.0105815  Training extension loss: 0.00162671
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.0111009  Testing extension loss: 0.00151609
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.0098427  Training extension loss: 0.00175707
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00848251  Training extension loss: 0.00143527
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00822075  Training extension loss: 0.00162418
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00787607  Training extension loss: 0.00144201
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00787834  Training extension loss: 0.00160953
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00781125  Training extension loss: 0.00164426
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00718899  Training extension loss: 0.00173335
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00620843  Testing extension loss: 0.00163767
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00660798  Training extension loss: 0.00166979
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00649897  Training extension loss: 0.00156397
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00620793  Training extension loss: 0.00157123
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00691545  Training extension loss: 0.0015197
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00577987  Training extension loss: 0.00168026
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00552602  Training extension loss: 0.00151388
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00549189  Testing extension loss: 0.00153769
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00571984  Training extension loss: 0.00148948
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00539306  Training extension loss: 0.00147944
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00483639  Training extension loss: 0.00150387
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00529131  Training extension loss: 0.00148322
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.0052123  Training extension loss: 0.00139201
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00522393  Training extension loss: 0.00136471
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00513604  Training extension loss: 0.00143674
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00527873  Testing extension loss: 0.001485
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.00452399  Training extension loss: 0.0014342
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00545904  Training extension loss: 0.00139127
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00475007  Training extension loss: 0.00137411
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00500873  Training extension loss: 0.00134015
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00486721  Training extension loss: 0.00127154
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00479492  Training extension loss: 0.0012188
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00446662  Training extension loss: 0.00130609
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00492769  Testing extension loss: 0.00135416
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00409341  Training extension loss: 0.00128072
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00435592  Training extension loss: 0.00117754
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00518337  Training extension loss: 0.00110801
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00437469  Training extension loss: 0.00127826
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00400782  Training extension loss: 0.00110545
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.0040066  Training extension loss: 0.00120652
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00443396  Testing extension loss: 0.00119959
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00379364  Training extension loss: 0.00115382
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.0043482  Training extension loss: 0.00117701
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00387472  Training extension loss: 0.00105406
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00413248  Training extension loss: 0.00108795
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00392308  Training extension loss: 0.00101543
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00353708  Training extension loss: 0.00103075
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00390548  Training extension loss: 0.00101838
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00384547  Testing extension loss: 0.00102339
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.00354472  Training extension loss: 0.000958918
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00380061  Training extension loss: 0.000932987
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00339973  Training extension loss: 0.000935058
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00399143  Training extension loss: 0.00101475
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00382731  Training extension loss: 0.000914113
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00336746  Training extension loss: 0.000869386
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00358833  Training extension loss: 0.000841934
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00411556  Testing extension loss: 0.000934167
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00342314  Training extension loss: 0.000926311
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00349385  Training extension loss: 0.000857024
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00319748  Training extension loss: 0.000941294
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.0038059  Training extension loss: 0.000881103
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00346241  Training extension loss: 0.000887208
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00342581  Training extension loss: 0.000868745
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00359988  Testing extension loss: 0.00099942
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.00314623  Training extension loss: 0.000826075
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00355454  Training extension loss: 0.000879314
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00312059  Training extension loss: 0.000882214
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00352195  Training extension loss: 0.000768795
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.00334479  Training extension loss: 0.000810199
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.00315701  Training extension loss: 0.000827395
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00322252  Training extension loss: 0.000842724
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00331229  Testing extension loss: 0.000799805
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00296888  Training extension loss: 0.000654547
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00280453  Training extension loss: 0.000818649
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00290958  Training extension loss: 0.000746155
DEBUG:root:[ Iteration 231 ] Training rotation loss: 0.00315614  Training extension loss: 0.000709412
DEBUG:root:[ Iteration 234 ] Training rotation loss: 0.00279846  Training extension loss: 0.000666658
DEBUG:root:[ Iteration 237 ] Training rotation loss: 0.00325531  Training extension loss: 0.000787501
DEBUG:root:[ Iteration 240 ] Training rotation loss: 0.00291748  Training extension loss: 0.000745062
DEBUG:root:[ Iteration 240 ] Testing rotation loss: 0.00313354  Testing extension loss: 0.000808779
DEBUG:root:[ Iteration 243 ] Training rotation loss: 0.00252503  Training extension loss: 0.000685412
DEBUG:root:[ Iteration 246 ] Training rotation loss: 0.00299008  Training extension loss: 0.000711363
DEBUG:root:[ Iteration 249 ] Training rotation loss: 0.00298024  Training extension loss: 0.000778808
DEBUG:root:[ Iteration 252 ] Training rotation loss: 0.00305568  Training extension loss: 0.000741245
DEBUG:root:[ Iteration 255 ] Training rotation loss: 0.00266204  Training extension loss: 0.000650256
DEBUG:root:[ Iteration 258 ] Training rotation loss: 0.00298955  Training extension loss: 0.000634311
DEBUG:root:[ Iteration 260 ] Testing rotation loss: 0.00337708  Testing extension loss: 0.000796362
DEBUG:root:[ Iteration 261 ] Training rotation loss: 0.00271879  Training extension loss: 0.000691012
DEBUG:root:[ Iteration 264 ] Training rotation loss: 0.0027626  Training extension loss: 0.000641189
DEBUG:root:[ Iteration 267 ] Training rotation loss: 0.00245281  Training extension loss: 0.000684734
DEBUG:root:[ Iteration 270 ] Training rotation loss: 0.00266671  Training extension loss: 0.000592479
DEBUG:root:[ Iteration 273 ] Training rotation loss: 0.00261021  Training extension loss: 0.000660195
DEBUG:root:[ Iteration 276 ] Training rotation loss: 0.0025323  Training extension loss: 0.000696713
DEBUG:root:[ Iteration 279 ] Training rotation loss: 0.00252424  Training extension loss: 0.000678049
DEBUG:root:[ Iteration 280 ] Testing rotation loss: 0.00314807  Testing extension loss: 0.000796738
DEBUG:root:[ Iteration 282 ] Training rotation loss: 0.00257568  Training extension loss: 0.00068515
DEBUG:root:[ Iteration 285 ] Training rotation loss: 0.00262493  Training extension loss: 0.00061476
DEBUG:root:[ Iteration 288 ] Training rotation loss: 0.00273746  Training extension loss: 0.00059587
DEBUG:root:[ Iteration 291 ] Training rotation loss: 0.00276259  Training extension loss: 0.000628395
DEBUG:root:[ Iteration 294 ] Training rotation loss: 0.00254148  Training extension loss: 0.000534958
DEBUG:root:[ Iteration 297 ] Training rotation loss: 0.0024329  Training extension loss: 0.000672862
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_07-05-2016_16h11m28s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0129103  Training extension loss: 0.00466778
DEBUG:root:[ Iteration 0 ] Testing rotation loss: 0.0125342  Testing extension loss: 0.00468136
DEBUG:root:[ Iteration 3 ] Training rotation loss: 0.0124407  Training extension loss: 0.00472673
DEBUG:root:[ Iteration 6 ] Training rotation loss: 0.011835  Training extension loss: 0.00465283
DEBUG:root:[ Iteration 9 ] Training rotation loss: 0.0124952  Training extension loss: 0.00445612
DEBUG:root:[ Iteration 12 ] Training rotation loss: 0.0115551  Training extension loss: 0.00443035
DEBUG:root:[ Iteration 15 ] Training rotation loss: 0.0136511  Training extension loss: 0.00381904
DEBUG:root:[ Iteration 18 ] Training rotation loss: 0.0119995  Training extension loss: 0.00321795
DEBUG:root:[ Iteration 20 ] Testing rotation loss: 0.0114184  Testing extension loss: 0.00224546
DEBUG:root:[ Iteration 21 ] Training rotation loss: 0.0108811  Training extension loss: 0.00188312
DEBUG:root:[ Iteration 24 ] Training rotation loss: 0.0109446  Training extension loss: 0.00142708
DEBUG:root:[ Iteration 27 ] Training rotation loss: 0.00998317  Training extension loss: 0.0013086
DEBUG:root:[ Iteration 30 ] Training rotation loss: 0.0099974  Training extension loss: 0.00133451
DEBUG:root:[ Iteration 33 ] Training rotation loss: 0.00938782  Training extension loss: 0.0012522
DEBUG:root:[ Iteration 36 ] Training rotation loss: 0.009152  Training extension loss: 0.00128681
DEBUG:root:[ Iteration 39 ] Training rotation loss: 0.00846967  Training extension loss: 0.00121447
DEBUG:root:[ Iteration 40 ] Testing rotation loss: 0.00803214  Testing extension loss: 0.00126233
DEBUG:root:[ Iteration 42 ] Training rotation loss: 0.00832889  Training extension loss: 0.00113163
DEBUG:root:[ Iteration 45 ] Training rotation loss: 0.00772553  Training extension loss: 0.0012533
DEBUG:root:[ Iteration 48 ] Training rotation loss: 0.00739914  Training extension loss: 0.00134274
DEBUG:root:[ Iteration 51 ] Training rotation loss: 0.00679555  Training extension loss: 0.00122328
DEBUG:root:[ Iteration 54 ] Training rotation loss: 0.00659379  Training extension loss: 0.00121849
DEBUG:root:[ Iteration 57 ] Training rotation loss: 0.00579017  Training extension loss: 0.00108575
DEBUG:root:[ Iteration 60 ] Training rotation loss: 0.00620798  Training extension loss: 0.0011308
DEBUG:root:[ Iteration 60 ] Testing rotation loss: 0.00593319  Testing extension loss: 0.00121445
DEBUG:root:[ Iteration 63 ] Training rotation loss: 0.00555366  Training extension loss: 0.00137445
DEBUG:root:[ Iteration 66 ] Training rotation loss: 0.00531677  Training extension loss: 0.00128191
DEBUG:root:[ Iteration 69 ] Training rotation loss: 0.00544022  Training extension loss: 0.00131103
DEBUG:root:[ Iteration 72 ] Training rotation loss: 0.00519883  Training extension loss: 0.0013272
DEBUG:root:[ Iteration 75 ] Training rotation loss: 0.00462166  Training extension loss: 0.00136022
DEBUG:root:[ Iteration 78 ] Training rotation loss: 0.00503484  Training extension loss: 0.00124622
DEBUG:root:[ Iteration 80 ] Testing rotation loss: 0.00484737  Testing extension loss: 0.00119505
DEBUG:root:[ Iteration 81 ] Training rotation loss: 0.00487982  Training extension loss: 0.00113054
DEBUG:root:[ Iteration 84 ] Training rotation loss: 0.00477118  Training extension loss: 0.00119967
DEBUG:root:[ Iteration 87 ] Training rotation loss: 0.00450266  Training extension loss: 0.00124027
DEBUG:root:[ Iteration 90 ] Training rotation loss: 0.00405706  Training extension loss: 0.00125198
DEBUG:root:[ Iteration 93 ] Training rotation loss: 0.00417316  Training extension loss: 0.00114618
DEBUG:root:[ Iteration 96 ] Training rotation loss: 0.00405336  Training extension loss: 0.00122169
DEBUG:root:[ Iteration 99 ] Training rotation loss: 0.00446463  Training extension loss: 0.00122342
DEBUG:root:[ Iteration 100 ] Testing rotation loss: 0.00440155  Testing extension loss: 0.00110555
DEBUG:root:[ Iteration 102 ] Training rotation loss: 0.0042112  Training extension loss: 0.00128958
DEBUG:root:[ Iteration 105 ] Training rotation loss: 0.00359946  Training extension loss: 0.00106439
DEBUG:root:[ Iteration 108 ] Training rotation loss: 0.00380181  Training extension loss: 0.00112442
DEBUG:root:[ Iteration 111 ] Training rotation loss: 0.00382769  Training extension loss: 0.00123771
DEBUG:root:[ Iteration 114 ] Training rotation loss: 0.00408559  Training extension loss: 0.00106357
DEBUG:root:[ Iteration 117 ] Training rotation loss: 0.00361123  Training extension loss: 0.00112109
DEBUG:root:[ Iteration 120 ] Training rotation loss: 0.00392347  Training extension loss: 0.00107889
DEBUG:root:[ Iteration 120 ] Testing rotation loss: 0.00408215  Testing extension loss: 0.00108851
DEBUG:root:[ Iteration 123 ] Training rotation loss: 0.00376241  Training extension loss: 0.00103632
DEBUG:root:[ Iteration 126 ] Training rotation loss: 0.00348893  Training extension loss: 0.00108237
DEBUG:root:[ Iteration 129 ] Training rotation loss: 0.00331731  Training extension loss: 0.00103235
DEBUG:root:[ Iteration 132 ] Training rotation loss: 0.00317867  Training extension loss: 0.000916961
DEBUG:root:[ Iteration 135 ] Training rotation loss: 0.00322719  Training extension loss: 0.00100756
DEBUG:root:[ Iteration 138 ] Training rotation loss: 0.00306219  Training extension loss: 0.000966481
DEBUG:root:[ Iteration 140 ] Testing rotation loss: 0.00380046  Testing extension loss: 0.000940536
DEBUG:root:[ Iteration 141 ] Training rotation loss: 0.00338733  Training extension loss: 0.000951939
DEBUG:root:[ Iteration 144 ] Training rotation loss: 0.00329872  Training extension loss: 0.000896652
DEBUG:root:[ Iteration 147 ] Training rotation loss: 0.00302684  Training extension loss: 0.000916424
DEBUG:root:[ Iteration 150 ] Training rotation loss: 0.00315753  Training extension loss: 0.000849033
DEBUG:root:[ Iteration 153 ] Training rotation loss: 0.00296258  Training extension loss: 0.000896167
DEBUG:root:[ Iteration 156 ] Training rotation loss: 0.00283289  Training extension loss: 0.000847086
DEBUG:root:[ Iteration 159 ] Training rotation loss: 0.00282337  Training extension loss: 0.000799541
DEBUG:root:[ Iteration 160 ] Testing rotation loss: 0.00327762  Testing extension loss: 0.00084124
DEBUG:root:[ Iteration 162 ] Training rotation loss: 0.0025068  Training extension loss: 0.000797626
DEBUG:root:[ Iteration 165 ] Training rotation loss: 0.00250525  Training extension loss: 0.000720359
DEBUG:root:[ Iteration 168 ] Training rotation loss: 0.00299222  Training extension loss: 0.000782041
DEBUG:root:[ Iteration 171 ] Training rotation loss: 0.00263014  Training extension loss: 0.000801923
DEBUG:root:[ Iteration 174 ] Training rotation loss: 0.00289177  Training extension loss: 0.000812697
DEBUG:root:[ Iteration 177 ] Training rotation loss: 0.00257581  Training extension loss: 0.000783193
DEBUG:root:[ Iteration 180 ] Training rotation loss: 0.00296411  Training extension loss: 0.00070714
DEBUG:root:[ Iteration 180 ] Testing rotation loss: 0.00315857  Testing extension loss: 0.000756779
DEBUG:root:[ Iteration 183 ] Training rotation loss: 0.00241781  Training extension loss: 0.000741485
DEBUG:root:[ Iteration 186 ] Training rotation loss: 0.00241557  Training extension loss: 0.00068979
DEBUG:root:[ Iteration 189 ] Training rotation loss: 0.00221689  Training extension loss: 0.000664215
DEBUG:root:[ Iteration 192 ] Training rotation loss: 0.00247319  Training extension loss: 0.000702586
DEBUG:root:[ Iteration 195 ] Training rotation loss: 0.00262024  Training extension loss: 0.000648948
DEBUG:root:[ Iteration 198 ] Training rotation loss: 0.00234584  Training extension loss: 0.000707487
DEBUG:root:[ Iteration 200 ] Testing rotation loss: 0.00312707  Testing extension loss: 0.000719545
DEBUG:root:[ Iteration 201 ] Training rotation loss: 0.0020787  Training extension loss: 0.000595471
DEBUG:root:[ Iteration 204 ] Training rotation loss: 0.00215885  Training extension loss: 0.000600753
DEBUG:root:[ Iteration 207 ] Training rotation loss: 0.00222632  Training extension loss: 0.00060401
DEBUG:root:[ Iteration 210 ] Training rotation loss: 0.00221381  Training extension loss: 0.000589036
DEBUG:root:[ Iteration 213 ] Training rotation loss: 0.0025455  Training extension loss: 0.000602233
DEBUG:root:[ Iteration 216 ] Training rotation loss: 0.002286  Training extension loss: 0.000630774
DEBUG:root:[ Iteration 219 ] Training rotation loss: 0.00243922  Training extension loss: 0.000644032
DEBUG:root:[ Iteration 220 ] Testing rotation loss: 0.00298943  Testing extension loss: 0.000667948
DEBUG:root:[ Iteration 222 ] Training rotation loss: 0.00229357  Training extension loss: 0.000561162
DEBUG:root:[ Iteration 225 ] Training rotation loss: 0.00218628  Training extension loss: 0.000654737
DEBUG:root:[ Iteration 228 ] Training rotation loss: 0.00217482  Training extension loss: 0.000603037
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_07-06-2016_11h32m01s.ckpt
DEBUG:root:Optimization done.
DEBUG:root:[ Iteration 0 ] Training rotation loss: 0.0126446  Training extension loss: 0.00469662
DEBUG:root:Saving...
DEBUG:root:Saved model to /media/1tb/Izzy/nets/net6_07-06-2016_16h09m01s.ckpt
DEBUG:root:Optimization done.
